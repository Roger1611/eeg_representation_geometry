{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c11fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import joblib\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bcbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Correct root (notebook is 2 levels deep)\n",
    "# -------------------------------------------------\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[2]\n",
    "\n",
    "DATASETS_DIR = PROJECT_ROOT / \"datasets\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Tables location (NEW STRUCTURE)\n",
    "# -------------------------------------------------\n",
    "TABLES_DIR = RESULTS_DIR / \"tables\" / \"backbone_benchmarking\"\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Models location (NEW STRUCTURE)\n",
    "# -------------------------------------------------\n",
    "MODELS_ROOT = PROJECT_ROOT / \"models\" / \"backbone_benchmark_models\"\n",
    "EEGNET_DIR = MODELS_ROOT / \"EEGNet\"\n",
    "EEGNET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project Root:\", PROJECT_ROOT)\n",
    "print(\"Tables ->\", TABLES_DIR)\n",
    "print(\"Models ->\", MODELS_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4e7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    import os, random\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddec6861",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = DATASETS_DIR / \"physionet_dataset\" / \"processed\" / \"preprocessed.npz\"\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Preprocessed file not found at {DATA_PATH}\"\n",
    "    )\n",
    "\n",
    "d = np.load(DATA_PATH, allow_pickle=True)\n",
    "\n",
    "X = d[\"X\"]\n",
    "y = d[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e1d5462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (30, 64, 561)\n",
      "y.shape = (30,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X.shape =\", X.shape)\n",
    "print(\"y.shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8537027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank=None\n",
      "    Using tolerance 12 (2.2e-16 eps * 64 dim * 8.3e+14  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computed CSP features: (30, 8)\n"
     ]
    }
   ],
   "source": [
    "USE_CSP = True\n",
    "CSP_COMPONENTS = 8\n",
    "if USE_CSP:\n",
    "    from mne.decoding import CSP\n",
    "    csp = CSP(n_components=CSP_COMPONENTS, log=True, norm_trace=False)\n",
    "    X_csp = csp.fit_transform(X, y)  \n",
    "    print(\"Computed CSP features:\", X_csp.shape)\n",
    "else:\n",
    "    X_csp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af5670c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CSP] LDA: acc=1.000 ± 0.000, f1=1.000\n",
      "[CSP] SVM-rbf: acc=1.000 ± 0.000, f1=1.000\n",
      "[CSP] RandomForest: acc=0.967 ± 0.067, f1=0.966\n",
      "[CSP] MLP: acc=1.000 ± 0.000, f1=1.000\n"
     ]
    }
   ],
   "source": [
    "classical_models = {\n",
    "    'LDA': Pipeline([('sc', StandardScaler()), ('clf', LinearDiscriminantAnalysis())]),\n",
    "    'SVM-rbf': Pipeline([('sc', StandardScaler()), ('clf', SVC(kernel='rbf', C=1, probability=True))]),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    'MLP': Pipeline([('sc', StandardScaler()), ('clf', MLPClassifier(hidden_layer_sizes=(100,), max_iter=400))])\n",
    "}\n",
    "\n",
    "CV_SPLITS = 5\n",
    "results_classical = {}\n",
    "if X_csp is not None:\n",
    "    skf = StratifiedKFold(n_splits=CV_SPLITS, shuffle=True, random_state=42)\n",
    "    for name, clf in classical_models.items():\n",
    "        accs=[]; f1s=[]\n",
    "        for tr, te in skf.split(X_csp, y):\n",
    "            clf.fit(X_csp[tr], y[tr])\n",
    "            p = clf.predict(X_csp[te])\n",
    "            accs.append(accuracy_score(y[te], p))\n",
    "            f1s.append(f1_score(y[te], p, average='weighted'))\n",
    "        results_classical[name] = {'acc_mean': float(np.mean(accs)), 'acc_std': float(np.std(accs)),\n",
    "                                   'f1_mean': float(np.mean(f1s)), 'f1_std': float(np.std(f1s))}\n",
    "        print(f\"[CSP] {name}: acc={results_classical[name]['acc_mean']:.3f} ± {results_classical[name]['acc_std']:.3f}, f1={results_classical[name]['f1_mean']:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1e6c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classical ML results.\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_classical).T\n",
    "results_df.to_csv(TABLES_DIR / \"classical_ml_kfold_results.csv\")\n",
    "\n",
    "print(\"Saved classical ML results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1df63fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self,X,y): self.X=X.astype(np.float32); self.y=y.astype(np.int64)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self,idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, chans, samples, classes=2, kern_len=64, F1=8, D=2, F2=16, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.first = nn.Sequential(\n",
    "            nn.Conv2d(1, F1, (1, kern_len), padding=(0, kern_len//2), bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            nn.Conv2d(F1, F1*D, (chans, 1), bias=False),\n",
    "            nn.BatchNorm2d(F1*D),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1,4)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.second = nn.Sequential(\n",
    "            nn.Conv2d(F1*D, F2, (1, 16), bias=False),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1,8)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy=torch.zeros(1,1,chans,samples)\n",
    "            feat=self.first(dummy); feat=self.second(feat)\n",
    "            hid_dim=feat.shape[1]\n",
    "        self.classify = nn.Linear(hid_dim, classes)\n",
    "    def forward(self,x):\n",
    "        x = x.unsqueeze(1); x = self.first(x); x = self.second(x); return self.classify(x)\n",
    "\n",
    "def train_epoch(model, loader, opt, loss_fn, device='cpu'):\n",
    "    model.train(); losses=[]\n",
    "    for xb,yb in loader:\n",
    "        xb,yb=xb.to(device), yb.to(device)\n",
    "        opt.zero_grad(); logits=model(xb); loss=loss_fn(logits,yb); loss.backward(); opt.step()\n",
    "        losses.append(loss.item())\n",
    "    return float(np.mean(losses))\n",
    "\n",
    "def eval_model_preds(model, loader, device='cpu'):\n",
    "    model.eval(); ys=[]; preds=[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in loader:\n",
    "            xb=xb.to(device); logits=model(xb)\n",
    "            p=logits.argmax(dim=1).cpu().numpy()\n",
    "            preds.extend(p.tolist()); ys.extend(yb.numpy().tolist())\n",
    "    return np.array(ys), np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd2afe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique labels (raw): [0 1]\n",
      "min, max: 0 1\n"
     ]
    }
   ],
   "source": [
    "y = d['y'].astype(int)\n",
    "print(\"unique labels (raw):\", np.unique(y))\n",
    "print(\"min, max:\", y.min(), y.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a742701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EEGNet] ep1: tr_loss=0.7282, test_acc=0.5000, f1=0.4857\n",
      "[EEGNet] ep5: tr_loss=0.6414, test_acc=0.5000, f1=0.4857\n",
      "[EEGNet] ep10: tr_loss=0.5989, test_acc=0.3333, f1=0.2500\n",
      "[EEGNet] ep15: tr_loss=0.5415, test_acc=0.6667, f1=0.6667\n",
      "[EEGNet] ep20: tr_loss=0.4926, test_acc=0.6667, f1=0.6667\n",
      "[EEGNet] ep25: tr_loss=0.4355, test_acc=0.6667, f1=0.6667\n",
      "EEGNet best acc: 0.6666666666666666 at epoch 14\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_epochs, n_chans, n_times = X.shape\n",
    "\n",
    "tr_idx, te_idx = train_test_split(\n",
    "    np.arange(n_epochs),\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    EEGDataset(X[tr_idx], y[tr_idx]),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    EEGDataset(X[te_idx], y[te_idx]),\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "model = EEGNet(n_chans, n_times, classes=len(np.unique(y))).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "MODEL_PATH = EEGNET_DIR / \"eegnet_best.pth\"\n",
    "\n",
    "EPOCHS = 25\n",
    "best_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    tr_loss = train_epoch(model, train_loader, opt, loss_fn, device)\n",
    "    ys, preds = eval_model_preds(model, test_loader, device)\n",
    "\n",
    "    acc = accuracy_score(ys, preds)\n",
    "    f1v = f1_score(ys, preds, average='weighted')\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_epoch = ep\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "    if ep == 1 or ep % 5 == 0:\n",
    "        print(f\"[EEGNet] ep{ep}: tr_loss={tr_loss:.4f}, test_acc={acc:.4f}, f1={f1v:.4f}\")\n",
    "\n",
    "print(\"EEGNet best acc:\", best_acc, \"at epoch\", best_epoch)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "705b4e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved EEGNet holdout results.\n"
     ]
    }
   ],
   "source": [
    "eeg_summary = {\n",
    "    \"best_accuracy\": float(best_acc),\n",
    "    \"best_epoch\": int(best_epoch)\n",
    "}\n",
    "\n",
    "pd.DataFrame([eeg_summary]).to_csv(\n",
    "    TABLES_DIR / \"eegnet_holdout_results.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Saved EEGNet holdout results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d55f0149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {np.int64(0): np.int64(14), np.int64(1): np.int64(16)}\n"
     ]
    }
   ],
   "source": [
    "y = d[\"y\"]\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"Labels:\", dict(zip(unique, counts)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
