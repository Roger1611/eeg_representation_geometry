{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f82a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BNCI: (640, 25, 561) labels: {np.int64(0): np.int64(160), np.int64(1): np.int64(160), np.int64(2): np.int64(160), np.int64(3): np.int64(160)}\n",
      "BNCI meta keys: ['sfreq']\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.9e+02 (2.2e-16 eps * 25 dim * 3.4e+16  max singular value)\n",
      "    Estimated rank (data): 25\n",
      "    data: rank 25 computed from 25 data channels with 0 projectors\n",
      "Reducing data rank from 25 -> 25\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=2 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=3 covariance using EMPIRICAL\n",
      "Done.\n",
      "CSP features: (640, 8)\n",
      "[CSP] LDA: acc 0.261 ± 0.030, f1 0.255\n",
      "[CSP] SVM-rbf: acc 0.211 ± 0.020, f1 0.196\n",
      "[CSP] RandomForest: acc 0.250 ± 0.034, f1 0.247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roger\\Desktop\\vs_code\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roger\\Desktop\\vs_code\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roger\\Desktop\\vs_code\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roger\\Desktop\\vs_code\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roger\\Desktop\\vs_code\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CSP] MLP: acc 0.223 ± 0.046, f1 0.222\n",
      "\n",
      "=== Fold 1/5 ===\n",
      " ep1: tr_loss=1.4002, val_acc=0.2422, f1=0.2082\n",
      " ep5: tr_loss=1.3142, val_acc=0.3359, f1=0.3338\n",
      " ep10: tr_loss=1.2207, val_acc=0.3516, f1=0.3438\n",
      " ep15: tr_loss=1.1609, val_acc=0.3750, f1=0.3654\n",
      " ep20: tr_loss=1.1095, val_acc=0.3984, f1=0.3900\n",
      " ep25: tr_loss=0.9879, val_acc=0.4219, f1=0.4216\n",
      " ep30: tr_loss=0.9052, val_acc=0.4531, f1=0.4545\n",
      "Fold 1 best acc: 0.4922\n",
      "\n",
      "=== Fold 2/5 ===\n",
      " ep1: tr_loss=1.3952, val_acc=0.3750, f1=0.3542\n",
      " ep5: tr_loss=1.2887, val_acc=0.3672, f1=0.3638\n",
      " ep10: tr_loss=1.2177, val_acc=0.3281, f1=0.3130\n",
      " ep15: tr_loss=1.1488, val_acc=0.4219, f1=0.4063\n",
      " ep20: tr_loss=1.0770, val_acc=0.4297, f1=0.4295\n",
      " ep25: tr_loss=1.0209, val_acc=0.4922, f1=0.4905\n",
      " ep30: tr_loss=0.9478, val_acc=0.4766, f1=0.4766\n",
      "Fold 2 best acc: 0.5078\n",
      "\n",
      "=== Fold 3/5 ===\n",
      " ep1: tr_loss=1.3818, val_acc=0.2422, f1=0.1990\n",
      " ep5: tr_loss=1.2818, val_acc=0.2656, f1=0.2624\n",
      " ep10: tr_loss=1.2132, val_acc=0.2578, f1=0.2431\n",
      " ep15: tr_loss=1.1451, val_acc=0.3750, f1=0.3543\n",
      " ep20: tr_loss=1.0540, val_acc=0.3906, f1=0.3798\n",
      " ep25: tr_loss=0.9761, val_acc=0.4609, f1=0.4602\n",
      " ep30: tr_loss=0.8385, val_acc=0.4844, f1=0.4812\n",
      "Fold 3 best acc: 0.5312\n",
      "\n",
      "=== Fold 4/5 ===\n",
      " ep1: tr_loss=1.3867, val_acc=0.2422, f1=0.1374\n",
      " ep5: tr_loss=1.3013, val_acc=0.2500, f1=0.2464\n",
      " ep10: tr_loss=1.2239, val_acc=0.3281, f1=0.3256\n",
      " ep15: tr_loss=1.1467, val_acc=0.3750, f1=0.3770\n",
      " ep20: tr_loss=1.0537, val_acc=0.4297, f1=0.4286\n",
      " ep25: tr_loss=0.9619, val_acc=0.4531, f1=0.4500\n",
      " ep30: tr_loss=0.8642, val_acc=0.4531, f1=0.4558\n",
      "Fold 4 best acc: 0.4922\n",
      "\n",
      "=== Fold 5/5 ===\n",
      " ep1: tr_loss=1.3934, val_acc=0.3203, f1=0.3223\n",
      " ep5: tr_loss=1.3023, val_acc=0.3984, f1=0.3740\n",
      " ep10: tr_loss=1.2429, val_acc=0.4141, f1=0.3933\n",
      " ep15: tr_loss=1.1523, val_acc=0.5000, f1=0.4977\n",
      " ep20: tr_loss=1.0918, val_acc=0.4766, f1=0.4676\n",
      " ep25: tr_loss=0.9744, val_acc=0.5312, f1=0.5204\n",
      " ep30: tr_loss=0.9034, val_acc=0.6016, f1=0.6004\n",
      "Fold 5 best acc: 0.6172\n",
      "\n",
      "=== EEGNet k-fold results ===\n",
      "      best_acc\n",
      "fold          \n",
      "1     0.492188\n",
      "2     0.507812\n",
      "3     0.531250\n",
      "4     0.492188\n",
      "5     0.617188\n",
      "\n",
      "=== Final comparison ===\n",
      "                 model  acc_mean   acc_std   f1_mean\n",
      "0     EEGNet (k-fold)  0.528125  0.046771       NaN\n",
      "1           LDA (CSP)  0.260937  0.030298  0.255045\n",
      "2  RandomForest (CSP)  0.250000  0.033874  0.246943\n",
      "3           MLP (CSP)  0.223438  0.045715  0.222303\n",
      "4       SVM-rbf (CSP)  0.210938  0.019764  0.195653\n",
      "Saved bnci_benchmark_summary.csv and fold models eegnet_fold*_best.pth\n"
     ]
    }
   ],
   "source": [
    "# ===== BNCI-only benchmark (fixed safe meta loading) =====\n",
    "# Requirements: numpy, torch, sklearn, mne, pandas, joblib\n",
    "import os, math, random, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import joblib, torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "seed_everything(42)\n",
    "\n",
    "# ---------- load BNCI preprocessed (safe meta extraction) ----------\n",
    "BNCI_PREPRO = \"preprocessed_BNCI.npz\"\n",
    "if not os.path.exists(BNCI_PREPRO):\n",
    "    raise FileNotFoundError(\"preprocessed_BNCI.npz not found. Run BNCI extraction first.\")\n",
    "d = np.load(BNCI_PREPRO, allow_pickle=True)\n",
    "X = d['X'].astype(np.float32)   # (N, n_ch, n_times)\n",
    "y = d['y'].astype(int)\n",
    "\n",
    "# safe meta load (handles 0-d numpy objects)\n",
    "meta = {}\n",
    "if 'meta' in d:\n",
    "    meta_raw = d['meta']\n",
    "    try:\n",
    "        if np.ndim(meta_raw) == 0:\n",
    "            # meta stored as object scalar (e.g. array({}, dtype=object))\n",
    "            meta = meta_raw.item() if hasattr(meta_raw, 'item') else {}\n",
    "            if meta is None: meta = {}\n",
    "        else:\n",
    "            meta = dict(meta_raw)\n",
    "    except Exception:\n",
    "        try:\n",
    "            meta = dict(meta_raw)\n",
    "        except Exception:\n",
    "            meta = {}\n",
    "else:\n",
    "    meta = {}\n",
    "\n",
    "print(\"Loaded BNCI:\", X.shape, \"labels:\", dict(zip(*np.unique(y, return_counts=True))))\n",
    "print(\"BNCI meta keys:\", list(meta.keys()))\n",
    "\n",
    "# ---------- small augmentation helper (on-the-fly) ----------\n",
    "def random_augment_numpy(epoch):\n",
    "    # epoch: n_ch x n_times\n",
    "    e = epoch.copy()\n",
    "    if np.random.rand() < 0.5:\n",
    "        e = e + np.random.normal(0, 0.01, e.shape)   # noise\n",
    "    if np.random.rand() < 0.4:\n",
    "        shift = np.random.randint(-10, 11)\n",
    "        e = np.roll(e, shift, axis=1)\n",
    "    if np.random.rand() < 0.3:\n",
    "        # channel dropout\n",
    "        ch = e.shape[0]\n",
    "        drop_mask = np.random.rand(ch) < 0.05\n",
    "        e[drop_mask,:] = 0\n",
    "    return e\n",
    "\n",
    "# ---------- CSP + classical baseline (k-fold) ----------\n",
    "use_csp = True\n",
    "if use_csp:\n",
    "    from mne.decoding import CSP\n",
    "    # choose small number of components (<= n_ch)\n",
    "    n_components = min(8, X.shape[1])\n",
    "    csp = CSP(n_components=n_components, log=True, norm_trace=False)\n",
    "    X_csp = csp.fit_transform(X, y)   # shape (N, n_components)\n",
    "    print(\"CSP features:\", X_csp.shape)\n",
    "else:\n",
    "    X_csp = None\n",
    "\n",
    "classifiers = {\n",
    "    'LDA': Pipeline([('sc', StandardScaler()), ('clf', LinearDiscriminantAnalysis())]),\n",
    "    'SVM-rbf': Pipeline([('sc', StandardScaler()), ('clf', SVC(kernel='rbf', C=1, probability=True))]),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    'MLP': Pipeline([('sc', StandardScaler()), ('clf', MLPClassifier(hidden_layer_sizes=(100,), max_iter=400))])\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results_cls = {}\n",
    "if X_csp is not None:\n",
    "    for name, clf in classifiers.items():\n",
    "        accs=[]; f1s=[]\n",
    "        for tr,te in kf.split(X_csp, y):\n",
    "            clf.fit(X_csp[tr], y[tr])\n",
    "            p = clf.predict(X_csp[te])\n",
    "            accs.append(accuracy_score(y[te], p)); f1s.append(f1_score(y[te], p, average='weighted'))\n",
    "        results_cls[name] = {'acc_mean': np.mean(accs), 'acc_std': np.std(accs), 'f1_mean': np.mean(f1s)}\n",
    "        print(f\"[CSP] {name}: acc {results_cls[name]['acc_mean']:.3f} ± {results_cls[name]['acc_std']:.3f}, f1 {results_cls[name]['f1_mean']:.3f}\")\n",
    "\n",
    "# save CSP artifact\n",
    "if X_csp is not None:\n",
    "    joblib.dump({'csp': csp, 'cls_results': results_cls}, 'bnci_classical_artifacts.pkl')\n",
    "\n",
    "# ---------- EEGNet implementation (small, robust) ----------\n",
    "class BNCI_Dataset(Dataset):\n",
    "    def __init__(self, X, y, augment=False):\n",
    "        self.X = X.astype(np.float32); self.y = y.astype(int)\n",
    "        self.augment = augment\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if self.augment:\n",
    "            x = random_augment_numpy(x)\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(int(self.y[idx]), dtype=torch.long)\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, chans, samples, classes=2, kern_len=64, F1=8, D=2, F2=16, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.first = nn.Sequential(\n",
    "            nn.Conv2d(1, F1, (1, kern_len), padding=(0, kern_len//2), bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            nn.Conv2d(F1, F1*D, (chans, 1), bias=False),\n",
    "            nn.BatchNorm2d(F1*D),\n",
    "            nn.ELU(), nn.AvgPool2d((1,4)), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.second = nn.Sequential(\n",
    "            nn.Conv2d(F1*D, F2, (1, 16), bias=False),\n",
    "            nn.BatchNorm2d(F2), nn.ELU(), nn.AvgPool2d((1,8)), nn.Flatten()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1,1,chans,samples)\n",
    "            feat = self.first(dummy); feat = self.second(feat)\n",
    "            hid_dim = feat.shape[1]\n",
    "        self.classify = nn.Linear(hid_dim, classes)\n",
    "    def forward(self,x):\n",
    "        x = x.unsqueeze(1); x = self.first(x); x = self.second(x); return self.classify(x)\n",
    "\n",
    "# ---------- k-fold EEGNet training ----------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "fold_idx = 0\n",
    "for tr_idx, te_idx in kf.split(X, y):\n",
    "    fold_idx += 1\n",
    "    print(f\"\\n=== Fold {fold_idx}/{n_splits} ===\")\n",
    "    Xtr, Xte = X[tr_idx], X[te_idx]\n",
    "    ytr, yte = y[tr_idx], y[te_idx]\n",
    "    ds_tr = BNCI_Dataset(Xtr, ytr, augment=True)\n",
    "    ds_te = BNCI_Dataset(Xte, yte, augment=False)\n",
    "    loader_tr = DataLoader(ds_tr, batch_size=32, shuffle=True)\n",
    "    loader_te = DataLoader(ds_te, batch_size=64, shuffle=False)\n",
    "    chans, samples = X.shape[1], X.shape[2]\n",
    "    num_classes = int(len(np.unique(y)))\n",
    "    model = EEGNet(chans, samples, classes=num_classes).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    best_acc = 0; best_state = None\n",
    "    for ep in range(1, 31):  # 30 epochs per fold\n",
    "        model.train(); losses=[]\n",
    "        for xb,yb in loader_tr:\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(); logits = model(xb); loss = loss_fn(logits,yb); loss.backward(); opt.step()\n",
    "            losses.append(loss.item())\n",
    "        # eval\n",
    "        model.eval()\n",
    "        ys=[]; preds=[]\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in loader_te:\n",
    "                xb = xb.to(device)\n",
    "                logits = model(xb)\n",
    "                preds.extend(logits.argmax(dim=1).cpu().numpy()); ys.extend(yb.numpy())\n",
    "        acc = accuracy_score(ys, preds); f1v = f1_score(ys, preds, average='weighted')\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc; best_state = model.state_dict()\n",
    "        if ep==1 or ep%5==0:\n",
    "            print(f\" ep{ep}: tr_loss={np.mean(losses):.4f}, val_acc={acc:.4f}, f1={f1v:.4f}\")\n",
    "    # save best for fold\n",
    "    if best_state is not None:\n",
    "        torch.save(best_state, f\"eegnet_fold{fold_idx}_best.pth\")\n",
    "    print(f\"Fold {fold_idx} best acc: {best_acc:.4f}\")\n",
    "    fold_results.append({'fold':fold_idx, 'best_acc':best_acc})\n",
    "# summary\n",
    "print(\"\\n=== EEGNet k-fold results ===\")\n",
    "print(pd.DataFrame(fold_results).set_index('fold'))\n",
    "\n",
    "# ---------- final summary table (combine CSP/classical + EEGNet mean) ----------\n",
    "rows=[]\n",
    "if X_csp is not None:\n",
    "    for k,v in results_cls.items():\n",
    "        rows.append({'model': k+' (CSP)', 'acc_mean': v['acc_mean'], 'acc_std': v['acc_std'], 'f1_mean': v['f1_mean']})\n",
    "# EEGNet fold mean\n",
    "eeg_mean = np.mean([r['best_acc'] for r in fold_results])\n",
    "rows.append({'model': 'EEGNet (k-fold)', 'acc_mean': float(eeg_mean), 'acc_std': float(np.std([r['best_acc'] for r in fold_results])), 'f1_mean': None})\n",
    "df = pd.DataFrame(rows).sort_values('acc_mean', ascending=False).reset_index(drop=True)\n",
    "print(\"\\n=== Final comparison ===\\n\", df)\n",
    "df.to_csv('bnci_benchmark_summary.csv', index=False)\n",
    "print(\"Saved bnci_benchmark_summary.csv and fold models eegnet_fold*_best.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ba7edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (640, 25, 561) labels: {np.int64(0): np.int64(160), np.int64(1): np.int64(160), np.int64(2): np.int64(160), np.int64(3): np.int64(160)}\n",
      "Device: cuda\n",
      "\n",
      "===== Training: EEGNet =====\n",
      "\n",
      "--- EEGNet fold 1/5 ---\n",
      " ep1: tr_loss=1.3971, val_acc=0.2891, val_f1=0.2722 (best 0.2891)\n",
      " ep2: tr_loss=1.3611, val_acc=0.3047, val_f1=0.2945 (best 0.3047)\n",
      " ep3: tr_loss=1.3407, val_acc=0.3281, val_f1=0.3117 (best 0.3281)\n",
      " ep5: tr_loss=1.3082, val_acc=0.3594, val_f1=0.3596 (best 0.3594)\n",
      " ep8: tr_loss=1.2570, val_acc=0.3672, val_f1=0.3674 (best 0.3672)\n",
      " ep10: tr_loss=1.2251, val_acc=0.3281, val_f1=0.3213 (best 0.3672)\n",
      " ep12: tr_loss=1.2149, val_acc=0.4141, val_f1=0.4073 (best 0.4141)\n",
      " ep13: tr_loss=1.1965, val_acc=0.4297, val_f1=0.4127 (best 0.4297)\n",
      " ep14: tr_loss=1.1822, val_acc=0.4531, val_f1=0.4443 (best 0.4531)\n",
      " ep15: tr_loss=1.1728, val_acc=0.4141, val_f1=0.3932 (best 0.4531)\n",
      " ep18: tr_loss=1.1036, val_acc=0.4766, val_f1=0.4606 (best 0.4766)\n",
      " ep20: tr_loss=1.0845, val_acc=0.3984, val_f1=0.3855 (best 0.4766)\n",
      " ep25: tr_loss=1.0092, val_acc=0.4297, val_f1=0.4282 (best 0.4766)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: EEGNet_fold1_best.pth\n",
      "\n",
      "--- EEGNet fold 2/5 ---\n",
      " ep1: tr_loss=1.3997, val_acc=0.2891, val_f1=0.2409 (best 0.2891)\n",
      " ep2: tr_loss=1.3584, val_acc=0.3125, val_f1=0.2945 (best 0.3125)\n",
      " ep4: tr_loss=1.3160, val_acc=0.3203, val_f1=0.3020 (best 0.3203)\n",
      " ep5: tr_loss=1.2949, val_acc=0.3281, val_f1=0.3201 (best 0.3281)\n",
      " ep7: tr_loss=1.2573, val_acc=0.3438, val_f1=0.3412 (best 0.3438)\n",
      " ep8: tr_loss=1.2556, val_acc=0.3594, val_f1=0.3556 (best 0.3594)\n",
      " ep10: tr_loss=1.2127, val_acc=0.3750, val_f1=0.3698 (best 0.3750)\n",
      " ep11: tr_loss=1.2125, val_acc=0.3828, val_f1=0.3790 (best 0.3828)\n",
      " ep12: tr_loss=1.1979, val_acc=0.3906, val_f1=0.3845 (best 0.3906)\n",
      " ep13: tr_loss=1.1670, val_acc=0.4062, val_f1=0.4012 (best 0.4062)\n",
      " ep15: tr_loss=1.1595, val_acc=0.4062, val_f1=0.4052 (best 0.4062)\n",
      " ep16: tr_loss=1.1424, val_acc=0.4297, val_f1=0.4313 (best 0.4297)\n",
      " ep20: tr_loss=1.0784, val_acc=0.4375, val_f1=0.4391 (best 0.4375)\n",
      " ep21: tr_loss=1.0745, val_acc=0.4453, val_f1=0.4453 (best 0.4453)\n",
      " ep23: tr_loss=1.0520, val_acc=0.4688, val_f1=0.4701 (best 0.4688)\n",
      " ep25: tr_loss=1.0010, val_acc=0.4609, val_f1=0.4605 (best 0.4688)\n",
      " ep26: tr_loss=0.9891, val_acc=0.5078, val_f1=0.5070 (best 0.5078)\n",
      " ep30: tr_loss=0.9180, val_acc=0.4844, val_f1=0.4858 (best 0.5078)\n",
      " ep32: tr_loss=0.8735, val_acc=0.5156, val_f1=0.5176 (best 0.5156)\n",
      " ep33: tr_loss=0.8707, val_acc=0.5312, val_f1=0.5325 (best 0.5312)\n",
      " ep34: tr_loss=0.8826, val_acc=0.5391, val_f1=0.5407 (best 0.5391)\n",
      " ep35: tr_loss=0.8481, val_acc=0.5547, val_f1=0.5563 (best 0.5547)\n",
      " ep36: tr_loss=0.8683, val_acc=0.5781, val_f1=0.5782 (best 0.5781)\n",
      " ep40: tr_loss=0.8270, val_acc=0.5469, val_f1=0.5491 (best 0.5781)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: EEGNet_fold2_best.pth\n",
      "\n",
      "--- EEGNet fold 3/5 ---\n",
      " ep1: tr_loss=1.4040, val_acc=0.2266, val_f1=0.1901 (best 0.2266)\n",
      " ep3: tr_loss=1.3347, val_acc=0.2344, val_f1=0.2220 (best 0.2344)\n",
      " ep4: tr_loss=1.3190, val_acc=0.2734, val_f1=0.2705 (best 0.2734)\n",
      " ep5: tr_loss=1.2840, val_acc=0.2266, val_f1=0.2129 (best 0.2734)\n",
      " ep8: tr_loss=1.2584, val_acc=0.2812, val_f1=0.2783 (best 0.2812)\n",
      " ep10: tr_loss=1.2286, val_acc=0.2734, val_f1=0.2539 (best 0.2812)\n",
      " ep11: tr_loss=1.1897, val_acc=0.2891, val_f1=0.2821 (best 0.2891)\n",
      " ep12: tr_loss=1.1994, val_acc=0.3203, val_f1=0.3198 (best 0.3203)\n",
      " ep13: tr_loss=1.1778, val_acc=0.3516, val_f1=0.3488 (best 0.3516)\n",
      " ep15: tr_loss=1.1522, val_acc=0.3125, val_f1=0.2994 (best 0.3516)\n",
      " ep20: tr_loss=1.0778, val_acc=0.3438, val_f1=0.3382 (best 0.3516)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: EEGNet_fold3_best.pth\n",
      "\n",
      "--- EEGNet fold 4/5 ---\n",
      " ep1: tr_loss=1.3879, val_acc=0.2734, val_f1=0.2156 (best 0.2734)\n",
      " ep2: tr_loss=1.3555, val_acc=0.3125, val_f1=0.3110 (best 0.3125)\n",
      " ep4: tr_loss=1.3154, val_acc=0.3359, val_f1=0.3319 (best 0.3359)\n",
      " ep5: tr_loss=1.2955, val_acc=0.2812, val_f1=0.2598 (best 0.3359)\n",
      " ep10: tr_loss=1.2282, val_acc=0.3438, val_f1=0.3408 (best 0.3438)\n",
      " ep11: tr_loss=1.2205, val_acc=0.3594, val_f1=0.3574 (best 0.3594)\n",
      " ep15: tr_loss=1.1767, val_acc=0.3594, val_f1=0.3601 (best 0.3594)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: EEGNet_fold4_best.pth\n",
      "\n",
      "--- EEGNet fold 5/5 ---\n",
      " ep1: tr_loss=1.3834, val_acc=0.2656, val_f1=0.2130 (best 0.2656)\n",
      " ep2: tr_loss=1.3573, val_acc=0.3281, val_f1=0.3312 (best 0.3281)\n",
      " ep4: tr_loss=1.3188, val_acc=0.3594, val_f1=0.3449 (best 0.3594)\n",
      " ep5: tr_loss=1.2990, val_acc=0.3438, val_f1=0.3267 (best 0.3594)\n",
      " ep10: tr_loss=1.2249, val_acc=0.3438, val_f1=0.3350 (best 0.3594)\n",
      " ep12: tr_loss=1.2158, val_acc=0.3750, val_f1=0.3647 (best 0.3750)\n",
      " ep13: tr_loss=1.2168, val_acc=0.3906, val_f1=0.3916 (best 0.3906)\n",
      " ep15: tr_loss=1.1889, val_acc=0.4141, val_f1=0.4090 (best 0.4141)\n",
      " ep17: tr_loss=1.1739, val_acc=0.4453, val_f1=0.4402 (best 0.4453)\n",
      " ep19: tr_loss=1.1593, val_acc=0.4688, val_f1=0.4631 (best 0.4688)\n",
      " ep20: tr_loss=1.1797, val_acc=0.4531, val_f1=0.4491 (best 0.4688)\n",
      " ep22: tr_loss=1.1342, val_acc=0.4922, val_f1=0.4921 (best 0.4922)\n",
      " ep25: tr_loss=1.1022, val_acc=0.4609, val_f1=0.4547 (best 0.4922)\n",
      " ep26: tr_loss=1.1222, val_acc=0.5000, val_f1=0.4966 (best 0.5000)\n",
      " ep30: tr_loss=1.0529, val_acc=0.4531, val_f1=0.4440 (best 0.5000)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: EEGNet_fold5_best.pth\n",
      "\n",
      ">>> EEGNet summary: mean_acc=0.4531 ± 0.0866\n",
      "\n",
      "===== Training: ShallowConvNet =====\n",
      "\n",
      "--- ShallowConvNet fold 1/5 ---\n",
      " ep1: tr_loss=1.8038, val_acc=0.2188, val_f1=0.1534 (best 0.2188)\n",
      " ep2: tr_loss=1.5928, val_acc=0.2656, val_f1=0.2173 (best 0.2656)\n",
      " ep3: tr_loss=1.4657, val_acc=0.3203, val_f1=0.3029 (best 0.3203)\n",
      " ep5: tr_loss=1.3856, val_acc=0.3203, val_f1=0.3110 (best 0.3203)\n",
      " ep8: tr_loss=1.3064, val_acc=0.3438, val_f1=0.3440 (best 0.3438)\n",
      " ep9: tr_loss=1.3094, val_acc=0.3672, val_f1=0.3401 (best 0.3672)\n",
      " ep10: tr_loss=1.2607, val_acc=0.3672, val_f1=0.3474 (best 0.3672)\n",
      " ep15: tr_loss=1.2330, val_acc=0.3750, val_f1=0.3601 (best 0.3750)\n",
      " ep20: tr_loss=1.1708, val_acc=0.3594, val_f1=0.3508 (best 0.3750)\n",
      " ep23: tr_loss=1.2088, val_acc=0.3828, val_f1=0.3773 (best 0.3828)\n",
      " ep25: tr_loss=1.1822, val_acc=0.3672, val_f1=0.3637 (best 0.3828)\n",
      " ep27: tr_loss=1.1797, val_acc=0.3984, val_f1=0.3875 (best 0.3984)\n",
      " ep30: tr_loss=1.2085, val_acc=0.3594, val_f1=0.3610 (best 0.3984)\n",
      " ep35: tr_loss=1.1737, val_acc=0.4062, val_f1=0.4020 (best 0.4062)\n",
      " ep40: tr_loss=1.1595, val_acc=0.3750, val_f1=0.3698 (best 0.4062)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: ShallowConvNet_fold1_best.pth\n",
      "\n",
      "--- ShallowConvNet fold 2/5 ---\n",
      " ep1: tr_loss=1.7362, val_acc=0.2812, val_f1=0.2425 (best 0.2812)\n",
      " ep3: tr_loss=1.4599, val_acc=0.2969, val_f1=0.2893 (best 0.2969)\n",
      " ep5: tr_loss=1.3936, val_acc=0.2969, val_f1=0.2758 (best 0.2969)\n",
      " ep6: tr_loss=1.3563, val_acc=0.3047, val_f1=0.2711 (best 0.3047)\n",
      " ep7: tr_loss=1.3090, val_acc=0.3516, val_f1=0.3435 (best 0.3516)\n",
      " ep10: tr_loss=1.2942, val_acc=0.2734, val_f1=0.2623 (best 0.3516)\n",
      " ep15: tr_loss=1.2088, val_acc=0.2969, val_f1=0.2905 (best 0.3516)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: ShallowConvNet_fold2_best.pth\n",
      "\n",
      "--- ShallowConvNet fold 3/5 ---\n",
      " ep1: tr_loss=1.8257, val_acc=0.2578, val_f1=0.2130 (best 0.2578)\n",
      " ep2: tr_loss=1.5925, val_acc=0.2812, val_f1=0.2425 (best 0.2812)\n",
      " ep4: tr_loss=1.4299, val_acc=0.3281, val_f1=0.3128 (best 0.3281)\n",
      " ep5: tr_loss=1.3475, val_acc=0.3125, val_f1=0.2758 (best 0.3281)\n",
      " ep10: tr_loss=1.2692, val_acc=0.3125, val_f1=0.3121 (best 0.3281)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: ShallowConvNet_fold3_best.pth\n",
      "\n",
      "--- ShallowConvNet fold 4/5 ---\n",
      " ep1: tr_loss=1.6632, val_acc=0.2422, val_f1=0.1851 (best 0.2422)\n",
      " ep2: tr_loss=1.5910, val_acc=0.2500, val_f1=0.1772 (best 0.2500)\n",
      " ep3: tr_loss=1.5455, val_acc=0.2891, val_f1=0.2572 (best 0.2891)\n",
      " ep4: tr_loss=1.4513, val_acc=0.3047, val_f1=0.2947 (best 0.3047)\n",
      " ep5: tr_loss=1.3397, val_acc=0.2109, val_f1=0.1668 (best 0.3047)\n",
      " ep10: tr_loss=1.2635, val_acc=0.2656, val_f1=0.2613 (best 0.3047)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: ShallowConvNet_fold4_best.pth\n",
      "\n",
      "--- ShallowConvNet fold 5/5 ---\n",
      " ep1: tr_loss=1.7528, val_acc=0.3125, val_f1=0.2687 (best 0.3125)\n",
      " ep4: tr_loss=1.4072, val_acc=0.3750, val_f1=0.3238 (best 0.3750)\n",
      " ep5: tr_loss=1.3731, val_acc=0.3359, val_f1=0.3141 (best 0.3750)\n",
      " ep9: tr_loss=1.2867, val_acc=0.3828, val_f1=0.3447 (best 0.3828)\n",
      " ep10: tr_loss=1.2721, val_acc=0.3047, val_f1=0.3015 (best 0.3828)\n",
      " ep15: tr_loss=1.2220, val_acc=0.2891, val_f1=0.2880 (best 0.3828)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: ShallowConvNet_fold5_best.pth\n",
      "\n",
      ">>> ShallowConvNet summary: mean_acc=0.3547 ± 0.0365\n",
      "\n",
      "===== Training: DeepConvNet =====\n",
      "\n",
      "--- DeepConvNet fold 1/5 ---\n",
      " ep1: tr_loss=1.7535, val_acc=0.4375, val_f1=0.4270 (best 0.4375)\n",
      " ep5: tr_loss=1.3479, val_acc=0.4609, val_f1=0.3886 (best 0.4609)\n",
      " ep8: tr_loss=1.3337, val_acc=0.5234, val_f1=0.4932 (best 0.5234)\n",
      " ep10: tr_loss=1.2329, val_acc=0.5000, val_f1=0.4558 (best 0.5234)\n",
      " ep13: tr_loss=1.2547, val_acc=0.5391, val_f1=0.5257 (best 0.5391)\n",
      " ep15: tr_loss=1.1003, val_acc=0.4844, val_f1=0.4692 (best 0.5391)\n",
      " ep19: tr_loss=1.0082, val_acc=0.5469, val_f1=0.5161 (best 0.5469)\n",
      " ep20: tr_loss=1.0187, val_acc=0.5312, val_f1=0.5211 (best 0.5469)\n",
      " ep21: tr_loss=1.0336, val_acc=0.5547, val_f1=0.5436 (best 0.5547)\n",
      " ep24: tr_loss=0.9891, val_acc=0.5781, val_f1=0.5616 (best 0.5781)\n",
      " ep25: tr_loss=0.9888, val_acc=0.5859, val_f1=0.5753 (best 0.5859)\n",
      " ep28: tr_loss=0.9781, val_acc=0.5938, val_f1=0.5953 (best 0.5938)\n",
      " ep30: tr_loss=0.9962, val_acc=0.5703, val_f1=0.5572 (best 0.5938)\n",
      " ep33: tr_loss=0.9129, val_acc=0.6094, val_f1=0.5888 (best 0.6094)\n",
      " ep35: tr_loss=0.9233, val_acc=0.5547, val_f1=0.5341 (best 0.6094)\n",
      " ep37: tr_loss=0.9356, val_acc=0.6328, val_f1=0.6247 (best 0.6328)\n",
      " ep40: tr_loss=0.9178, val_acc=0.5859, val_f1=0.5760 (best 0.6328)\n",
      " ep45: tr_loss=0.9403, val_acc=0.5938, val_f1=0.5726 (best 0.6328)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: DeepConvNet_fold1_best.pth\n",
      "\n",
      "--- DeepConvNet fold 2/5 ---\n",
      " ep1: tr_loss=1.8552, val_acc=0.3125, val_f1=0.2267 (best 0.3125)\n",
      " ep2: tr_loss=1.6065, val_acc=0.3516, val_f1=0.2893 (best 0.3516)\n",
      " ep3: tr_loss=1.6068, val_acc=0.3594, val_f1=0.2947 (best 0.3594)\n",
      " ep4: tr_loss=1.4072, val_acc=0.4922, val_f1=0.4490 (best 0.4922)\n",
      " ep5: tr_loss=1.4273, val_acc=0.3750, val_f1=0.2793 (best 0.4922)\n",
      " ep10: tr_loss=1.1537, val_acc=0.5469, val_f1=0.5301 (best 0.5469)\n",
      " ep11: tr_loss=1.0709, val_acc=0.5859, val_f1=0.5836 (best 0.5859)\n",
      " ep15: tr_loss=1.0811, val_acc=0.4922, val_f1=0.4916 (best 0.5859)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: DeepConvNet_fold2_best.pth\n",
      "\n",
      "--- DeepConvNet fold 3/5 ---\n",
      " ep1: tr_loss=1.8772, val_acc=0.3203, val_f1=0.2928 (best 0.3203)\n",
      " ep3: tr_loss=1.4562, val_acc=0.4219, val_f1=0.3652 (best 0.4219)\n",
      " ep4: tr_loss=1.2670, val_acc=0.4297, val_f1=0.3887 (best 0.4297)\n",
      " ep5: tr_loss=1.3783, val_acc=0.4609, val_f1=0.4536 (best 0.4609)\n",
      " ep10: tr_loss=1.1888, val_acc=0.5078, val_f1=0.4925 (best 0.5078)\n",
      " ep15: tr_loss=1.1393, val_acc=0.5156, val_f1=0.5134 (best 0.5156)\n",
      " ep19: tr_loss=1.0709, val_acc=0.5469, val_f1=0.5365 (best 0.5469)\n",
      " ep20: tr_loss=0.9530, val_acc=0.5000, val_f1=0.4890 (best 0.5469)\n",
      " ep24: tr_loss=1.0309, val_acc=0.5625, val_f1=0.5604 (best 0.5625)\n",
      " ep25: tr_loss=1.0142, val_acc=0.5703, val_f1=0.5634 (best 0.5703)\n",
      " ep30: tr_loss=0.9829, val_acc=0.5312, val_f1=0.5299 (best 0.5703)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: DeepConvNet_fold3_best.pth\n",
      "\n",
      "--- DeepConvNet fold 4/5 ---\n",
      " ep1: tr_loss=2.0603, val_acc=0.2812, val_f1=0.2447 (best 0.2812)\n",
      " ep2: tr_loss=1.6327, val_acc=0.3281, val_f1=0.3135 (best 0.3281)\n",
      " ep3: tr_loss=1.4429, val_acc=0.3438, val_f1=0.2946 (best 0.3438)\n",
      " ep4: tr_loss=1.4077, val_acc=0.3984, val_f1=0.3625 (best 0.3984)\n",
      " ep5: tr_loss=1.3986, val_acc=0.4688, val_f1=0.4656 (best 0.4688)\n",
      " ep8: tr_loss=1.2715, val_acc=0.5078, val_f1=0.5136 (best 0.5078)\n",
      " ep10: tr_loss=1.1853, val_acc=0.4375, val_f1=0.4027 (best 0.5078)\n",
      " ep15: tr_loss=1.0022, val_acc=0.5234, val_f1=0.4893 (best 0.5234)\n",
      " ep18: tr_loss=0.9964, val_acc=0.5312, val_f1=0.5178 (best 0.5312)\n",
      " ep20: tr_loss=1.0510, val_acc=0.5078, val_f1=0.4805 (best 0.5312)\n",
      " ep22: tr_loss=0.9125, val_acc=0.5547, val_f1=0.5439 (best 0.5547)\n",
      " ep24: tr_loss=0.9854, val_acc=0.5625, val_f1=0.5474 (best 0.5625)\n",
      " ep25: tr_loss=0.9859, val_acc=0.4922, val_f1=0.4884 (best 0.5625)\n",
      " ep30: tr_loss=0.9097, val_acc=0.5547, val_f1=0.5530 (best 0.5625)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: DeepConvNet_fold4_best.pth\n",
      "\n",
      "--- DeepConvNet fold 5/5 ---\n",
      " ep1: tr_loss=1.8229, val_acc=0.3125, val_f1=0.2877 (best 0.3125)\n",
      " ep2: tr_loss=1.6667, val_acc=0.3672, val_f1=0.3436 (best 0.3672)\n",
      " ep3: tr_loss=1.4995, val_acc=0.4375, val_f1=0.4057 (best 0.4375)\n",
      " ep4: tr_loss=1.3661, val_acc=0.4453, val_f1=0.4084 (best 0.4453)\n",
      " ep5: tr_loss=1.3805, val_acc=0.4609, val_f1=0.4198 (best 0.4609)\n",
      " ep9: tr_loss=1.3070, val_acc=0.5078, val_f1=0.4804 (best 0.5078)\n",
      " ep10: tr_loss=1.2219, val_acc=0.5078, val_f1=0.4851 (best 0.5078)\n",
      " ep14: tr_loss=1.0885, val_acc=0.5469, val_f1=0.5209 (best 0.5469)\n",
      " ep15: tr_loss=1.1563, val_acc=0.5859, val_f1=0.5799 (best 0.5859)\n",
      " ep17: tr_loss=1.0629, val_acc=0.6406, val_f1=0.6314 (best 0.6406)\n",
      " ep20: tr_loss=1.1700, val_acc=0.4688, val_f1=0.4472 (best 0.6406)\n",
      " ep25: tr_loss=0.9703, val_acc=0.6016, val_f1=0.5969 (best 0.6406)\n",
      " Early stopping (no val improvement).\n",
      " Saved best model: DeepConvNet_fold5_best.pth\n",
      "\n",
      ">>> DeepConvNet summary: mean_acc=0.5984 ± 0.0322\n",
      "\n",
      "=== Final Stage-2 comparison ===\n",
      "             model  acc_mean   acc_std   f1_mean\n",
      "0     DeepConvNet  0.598437  0.036056  0.554931\n",
      "1          EEGNet  0.453125  0.096793  0.436327\n",
      "2  ShallowConvNet  0.354687  0.040820  0.309042\n",
      "Saved stage2_benchmark_summary.csv and fold models (*.pth)\n"
     ]
    }
   ],
   "source": [
    "# ===== Stage 2: EEGNet / ShallowConvNet / DeepConvNet benchmark (k-fold) =====\n",
    "# Paste into BNCI_benchmark.ipynb. Requires: numpy, torch, sklearn, pandas, joblib\n",
    "import os, random, math, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "seed_everything(42)\n",
    "\n",
    "# ---------- Load BNCI preprocessed (safe meta load) ----------\n",
    "PRE = \"preprocessed_BNCI.npz\"\n",
    "assert os.path.exists(PRE), \"preprocessed_BNCI.npz not found. Run extraction first.\"\n",
    "d = np.load(PRE, allow_pickle=True)\n",
    "X = d['X'].astype(np.float32)   # shape: (N, n_ch, n_times)\n",
    "y = d['y'].astype(int)\n",
    "meta = {}\n",
    "if 'meta' in d:\n",
    "    mr = d['meta']\n",
    "    try:\n",
    "        meta = mr.item() if np.ndim(mr)==0 else dict(mr)\n",
    "    except Exception:\n",
    "        try:\n",
    "            meta = dict(mr)\n",
    "        except Exception:\n",
    "            meta = {}\n",
    "print(\"Loaded:\", X.shape, \"labels:\", dict(zip(*np.unique(y, return_counts=True))))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---------- Augmentation ----------\n",
    "def augment_numpy(epoch):\n",
    "    # epoch: n_ch x n_times\n",
    "    e = epoch.copy()\n",
    "    # gaussian noise\n",
    "    if np.random.rand() < 0.5:\n",
    "        e = e + np.random.normal(0, 0.01, e.shape)\n",
    "    # time shift\n",
    "    if np.random.rand() < 0.4:\n",
    "        shift = np.random.randint(-10, 11)\n",
    "        e = np.roll(e, shift, axis=1)\n",
    "    # channel dropout\n",
    "    if np.random.rand() < 0.3:\n",
    "        ch = e.shape[0]\n",
    "        drop = np.random.rand(ch) < 0.05\n",
    "        e[drop,:] = 0\n",
    "    return e\n",
    "\n",
    "class BNCI_Dataset(Dataset):\n",
    "    def __init__(self, X, y, augment=False):\n",
    "        self.X = X.astype(np.float32); self.y = y.astype(int); self.augment = augment\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if self.augment:\n",
    "            x = augment_numpy(x)\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(int(self.y[idx]), dtype=torch.long)\n",
    "\n",
    "# ---------- Model definitions ----------\n",
    "# EEGNet (small/robust)\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, chans, samples, classes=2, F1=8, D=2, F2=16, kern_len=64, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.first = nn.Sequential(\n",
    "            nn.Conv2d(1, F1, (1, kern_len), padding=(0, kern_len//2), bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            nn.Conv2d(F1, F1*D, (chans, 1), bias=False),\n",
    "            nn.BatchNorm2d(F1*D),\n",
    "            nn.ELU(), nn.AvgPool2d((1,4)), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.second = nn.Sequential(\n",
    "            nn.Conv2d(F1*D, F2, (1, 16), bias=False),\n",
    "            nn.BatchNorm2d(F2), nn.ELU(), nn.AvgPool2d((1,8)), nn.Flatten()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1,1,chans,samples)\n",
    "            h = self.first(dummy); h = self.second(h)\n",
    "            hid = h.shape[1]\n",
    "        self.classify = nn.Linear(hid, classes)\n",
    "    def forward(self,x):\n",
    "        x = x.unsqueeze(1); x = self.first(x); x = self.second(x); return self.classify(x)\n",
    "\n",
    "# ShallowConvNet (Bandpower-like)\n",
    "class ShallowConvNet(nn.Module):\n",
    "    def __init__(self, chans, samples, classes=2, F=40, kern_len=25, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.temporal = nn.Conv2d(1, F, (1, kern_len), padding=(0, kern_len//2), bias=False)\n",
    "        self.spatial = nn.Conv2d(F, F, (chans, 1), bias=False)\n",
    "        self.pool = nn.Sequential(nn.ELU(), nn.AvgPool2d((1, 75)), nn.Dropout(dropout))\n",
    "        # dynamic flatten\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1,1,chans,samples)\n",
    "            out = self.pool(self.spatial(self.temporal(dummy)))\n",
    "            hid = int(torch.prod(torch.tensor(out.shape[1:])))\n",
    "        self.classify = nn.Linear(hid, classes)\n",
    "    def forward(self,x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.temporal(x)\n",
    "        x = self.spatial(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.flatten(1)\n",
    "        return self.classify(x)\n",
    "\n",
    "# DeepConvNet (as in Schirrmeister et al.)\n",
    "class DeepConvNet(nn.Module):\n",
    "    def __init__(self, chans, samples, classes=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 25, (1,5), padding=(0,2), bias=False),\n",
    "            nn.Conv2d(25, 25, (chans,1), bias=False),\n",
    "            nn.BatchNorm2d(25),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1,2)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(25, 50, (1,5), padding=(0,2), bias=False),\n",
    "            nn.BatchNorm2d(50), nn.ELU(), nn.MaxPool2d((1,2)), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(50, 100, (1,5), padding=(0,2), bias=False),\n",
    "            nn.BatchNorm2d(100), nn.ELU(), nn.MaxPool2d((1,2)), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(100, 200, (1,5), padding=(0,2), bias=False),\n",
    "            nn.BatchNorm2d(200), nn.ELU(), nn.MaxPool2d((1,2)), nn.Dropout(dropout)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1,1,chans,samples)\n",
    "            out = self.block1(dummy); out = self.block2(out); out = self.block3(out); out = self.block4(out)\n",
    "            hid = int(torch.prod(torch.tensor(out.shape[1:])))\n",
    "        self.classify = nn.Linear(hid, classes)\n",
    "    def forward(self,x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.block1(x); x = self.block2(x); x = self.block3(x); x = self.block4(x)\n",
    "        x = x.flatten(1)\n",
    "        return self.classify(x)\n",
    "\n",
    "# ---------- Training utilities ----------\n",
    "def train_epoch(model, loader, opt, loss_fn, device):\n",
    "    model.train(); losses=[]\n",
    "    for xb,yb in loader:\n",
    "        xb = xb.to(device, dtype=torch.float32); yb = yb.to(device, dtype=torch.long)\n",
    "        opt.zero_grad(); logits = model(xb); loss = loss_fn(logits, yb)\n",
    "        loss.backward(); opt.step()\n",
    "        losses.append(loss.item())\n",
    "    return float(np.mean(losses)) if len(losses)>0 else 0.0\n",
    "\n",
    "def eval_model(model, loader, device):\n",
    "    model.eval(); ys=[]; preds=[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in loader:\n",
    "            xb = xb.to(device, dtype=torch.float32)\n",
    "            logits = model(xb)\n",
    "            preds.extend(logits.argmax(dim=1).cpu().numpy()); ys.extend(yb.numpy())\n",
    "    return np.array(ys), np.array(preds)\n",
    "\n",
    "# ---------- K-Fold training for each model ----------\n",
    "models_to_run = ['EEGNet', 'ShallowConvNet', 'DeepConvNet']\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "summary_rows = []\n",
    "\n",
    "for model_name in models_to_run:\n",
    "    print(\"\\n===== Training:\", model_name, \"=====\")\n",
    "    fold = 0\n",
    "    fold_best_accs = []\n",
    "    for tr_idx, te_idx in kf.split(X, y):\n",
    "        fold += 1\n",
    "        print(f\"\\n--- {model_name} fold {fold}/{n_splits} ---\")\n",
    "        Xtr, Xte = X[tr_idx], X[te_idx]; ytr, yte = y[tr_idx], y[te_idx]\n",
    "        ds_tr = BNCI_Dataset(Xtr, ytr, augment=True)\n",
    "        ds_te = BNCI_Dataset(Xte, yte, augment=False)\n",
    "        loader_tr = DataLoader(ds_tr, batch_size=32, shuffle=True)\n",
    "        loader_te = DataLoader(ds_te, batch_size=64, shuffle=False)\n",
    "\n",
    "        chans, samples = X.shape[1], X.shape[2]\n",
    "        num_classes = int(len(np.unique(y)))\n",
    "        # instantiate model\n",
    "        if model_name == 'EEGNet':\n",
    "            model = EEGNet(chans, samples, classes=num_classes).to(device)\n",
    "        elif model_name == 'ShallowConvNet':\n",
    "            model = ShallowConvNet(chans, samples, classes=num_classes).to(device)\n",
    "        else:\n",
    "            model = DeepConvNet(chans, samples, classes=num_classes).to(device)\n",
    "\n",
    "        opt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        # FIXED scheduler: removed verbose argument for compatibility\n",
    "        scheduler = ReduceLROnPlateau(opt, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "        best_acc = 0.0; best_state = None; stale = 0; patience = 8\n",
    "        for ep in range(1, 51):   # up to 50 epochs\n",
    "            tr_loss = train_epoch(model, loader_tr, opt, loss_fn, device)\n",
    "            ys_val, preds_val = eval_model(model, loader_te, device)\n",
    "            val_acc = accuracy_score(ys_val, preds_val); val_f1 = f1_score(ys_val, preds_val, average='weighted')\n",
    "            scheduler.step(val_acc)\n",
    "            if val_acc > best_acc + 1e-4:\n",
    "                best_acc = val_acc; best_state = model.state_dict(); stale = 0\n",
    "            else:\n",
    "                stale += 1\n",
    "            if ep==1 or ep%5==0 or stale==0:\n",
    "                print(f\" ep{ep}: tr_loss={tr_loss:.4f}, val_acc={val_acc:.4f}, val_f1={val_f1:.4f} (best {best_acc:.4f})\")\n",
    "            if stale >= patience:\n",
    "                print(\" Early stopping (no val improvement).\")\n",
    "                break\n",
    "\n",
    "        if best_state is not None:\n",
    "            fname = f\"{model_name}_fold{fold}_best.pth\"\n",
    "            torch.save(best_state, fname)\n",
    "            print(\" Saved best model:\", fname)\n",
    "        fold_best_accs.append(best_acc)\n",
    "        summary_rows.append({'model': model_name, 'fold': fold, 'best_acc': float(best_acc), 'best_f1': float(val_f1)})\n",
    "\n",
    "    mean_acc = float(np.mean(fold_best_accs)); std_acc = float(np.std(fold_best_accs))\n",
    "    print(f\"\\n>>> {model_name} summary: mean_acc={mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "\n",
    "# ---------- Final comparison ----------\n",
    "df = pd.DataFrame(summary_rows)\n",
    "summary_table = df.groupby('model').agg({'best_acc':['mean','std'], 'best_f1':'mean'})\n",
    "summary_table.columns = ['acc_mean','acc_std','f1_mean']\n",
    "summary_table = summary_table.reset_index().sort_values('acc_mean', ascending=False)\n",
    "print(\"\\n=== Final Stage-2 comparison ===\\n\", summary_table)\n",
    "summary_table.to_csv('stage2_benchmark_summary.csv', index=False)\n",
    "print(\"Saved stage2_benchmark_summary.csv and fold models (*.pth)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
