{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b87730f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 .mat files (showing up to 30):\n",
      " - A01E.mat\n",
      " - A01T.mat\n",
      " - A02E.mat\n",
      " - A02T.mat\n",
      " - A03E.mat\n",
      " - A03T.mat\n",
      " - A04E.mat\n",
      " - A04T.mat\n",
      " - A05E.mat\n",
      " - A05T.mat\n",
      " - A06E.mat\n",
      " - A06T.mat\n",
      " - A07E.mat\n",
      " - A07T.mat\n",
      " - A08E.mat\n",
      " - A08T.mat\n",
      " - A09E.mat\n",
      " - A09T.mat\n",
      "\n",
      "Inspecting a few files to show keys inside (first 6):\n",
      "\n",
      "FILE: A01E.mat\n",
      "  keys: ['data']\n",
      "----------------------------------------\n",
      "FILE: A01T.mat\n",
      "  keys: ['data']\n",
      "----------------------------------------\n",
      "FILE: A02E.mat\n",
      "  keys: ['data']\n",
      "----------------------------------------\n",
      "FILE: A02T.mat\n",
      "  keys: ['data']\n",
      "----------------------------------------\n",
      "FILE: A03E.mat\n",
      "  keys: ['data']\n",
      "----------------------------------------\n",
      "FILE: A03T.mat\n",
      "  keys: ['data']\n",
      "----------------------------------------\n",
      "\n",
      "Baseline epoch shape: (30, 64, 561) sfreq_target: 250.0 epoch_duration(s): 2.244\n",
      "\n",
      "=== Trying: A01E.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A01T.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A02E.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A02T.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A03E.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A03T.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A04E.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A04T.mat\n",
      "  Found EEG array with shape: (7,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A05E.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A05T.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A06E.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A06T.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A07E.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A07T.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A08E.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A08T.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A09E.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "=== Trying: A09T.mat\n",
      "  Found EEG array with shape: (9,)\n",
      "  Could not find marker positions in this file automatically.\n",
      "  Possible keys for markers: []\n",
      "\n",
      "\n",
      "=== Extraction summary ===\n",
      "Files with successful extraction: 0 total extracted epochs: 0\n",
      "\n",
      "No files were auto-extracted. Inspect the keys printed above for a representative .mat and tell me which key contains EEG data and which contains event positions/labels (I can then give a short mapping).\n"
     ]
    }
   ],
   "source": [
    "# BNCI .mat auto-inspect + epoch extractor + merge with existing preprocessed.npz\n",
    "# Paste into a NEW notebook. Requires: numpy, scipy, mne\n",
    "import os, glob, textwrap\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import resample\n",
    "import mne, pprint\n",
    "\n",
    "# ---------- USER: point to folder ----------\n",
    "BNCI_ROOT = r\"C:\\Users\\roger\\Desktop\\vs_code\\brain_intent_decoding\\BNCI_folder\"  # <<-- change to your BNCI folder path\n",
    "PREPRO_PATH = \"preprocessed.npz\"   # your existing EEGBCI preprocessed file\n",
    "OUT_PATH = \"preprocessed_combined.npz\"\n",
    "# epoch time window (seconds) relative to event onset -- we will adapt epoch length to baseline epoch length\n",
    "TMIN = 0.0\n",
    "TMAX = 3.0\n",
    "\n",
    "# ---------- helper utilities ----------\n",
    "def list_mat_keys(path):\n",
    "    try:\n",
    "        m = loadmat(path, squeeze_me=True, struct_as_record=False)\n",
    "        return sorted(list(k for k in m.keys() if not k.startswith(\"__\")))\n",
    "    except Exception as e:\n",
    "        return f\"ERR: {e}\"\n",
    "\n",
    "def safe_get(mat, keys):\n",
    "    \"\"\"Try a list of candidate keys and return first present, else None\"\"\"\n",
    "    for k in keys:\n",
    "        if k in mat:\n",
    "            return mat[k]\n",
    "    return None\n",
    "\n",
    "def struct_field(obj, field):\n",
    "    # helper to try to access MATLAB struct-like field robustly\n",
    "    try:\n",
    "        return getattr(obj, field)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return obj[field]\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def try_infer_eeg_and_markers(mat_dict):\n",
    "    \"\"\"\n",
    "    Attempts to infer EEG time-series array and marker structure from a loaded .mat dict.\n",
    "    Returns (data, sfreq, markers) where:\n",
    "      - data is numpy array shape (n_channels, n_samples)\n",
    "      - sfreq is float (Hz) or None\n",
    "      - markers is dict {code: list_of_sample_indices} or dict with keys 'pos' and 'y' as arrays\n",
    "    \"\"\"\n",
    "    # Common candidates\n",
    "    data_candidates = ['cnt', 'data', 'EEG', 'X', 'signal', 'cnt_x']  # try these in order\n",
    "    mrk_candidates  = ['mrk', 'marker', 'events', 'event', 'trial', 'pos', 'eventpos']\n",
    "    sfreq_candidates = ['fs', 'srate', 'sr', 'nfo', 'nfo_fs']\n",
    "\n",
    "    data = None\n",
    "    for k in data_candidates:\n",
    "        if k in mat_dict:\n",
    "            data = mat_dict[k]\n",
    "            break\n",
    "    # If data is a struct, try to extract field 'x' or 'cnt'\n",
    "    if data is not None and (not isinstance(data, np.ndarray)):\n",
    "        # sometimes data is MATLAB object with .x or .data\n",
    "        for fld in ('x', 'X', 'data', 'cnt'):\n",
    "            try:\n",
    "                val = getattr(data, fld)\n",
    "                if isinstance(val, np.ndarray):\n",
    "                    data = val; break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Now markers detection\n",
    "    markers = {}\n",
    "    # mrk struct with pos & y (common in BNCI)\n",
    "    if 'mrk' in mat_dict:\n",
    "        mrk = mat_dict['mrk']\n",
    "        pos = struct_field(mrk, 'pos') or struct_field(mrk, 'onset') or safe_get(mrk.__dict__ if hasattr(mrk,'__dict__') else mrk, ['pos','onset'])\n",
    "        y   = struct_field(mrk, 'y')   or struct_field(mrk, 'classlabel') or safe_get(mrk.__dict__ if hasattr(mrk,'__dict__') else mrk, ['y','classlabel'])\n",
    "        if pos is not None and y is not None:\n",
    "            # try to build dict of code->samples\n",
    "            try:\n",
    "                pos = np.array(pos).astype(int).ravel()\n",
    "                y_arr = np.array(y).ravel()\n",
    "                for p,lab in zip(pos, y_arr):\n",
    "                    markers.setdefault(int(lab), []).append(int(p))\n",
    "                return np.array(data), None, markers\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # try event arrays directly\n",
    "    for k in ('pos','eventpos','events','event'):\n",
    "        if k in mat_dict:\n",
    "            pos = mat_dict[k]\n",
    "            if isinstance(pos, np.ndarray) and pos.size>0:\n",
    "                # if there's also labels array\n",
    "                y_candidates = ['y','classlabel','label','codes']\n",
    "                y_val = None\n",
    "                for yc in y_candidates:\n",
    "                    if yc in mat_dict:\n",
    "                        y_val = mat_dict[yc]; break\n",
    "                if y_val is not None:\n",
    "                    pos = np.array(pos).astype(int).ravel()\n",
    "                    y_arr = np.array(y_val).ravel()\n",
    "                    for p,lab in zip(pos, y_arr):\n",
    "                        markers.setdefault(int(lab), []).append(int(p))\n",
    "                    return np.array(data), None, markers\n",
    "\n",
    "    # fallback: some BNCI .mat have 'trial' list where each trial contains data and class\n",
    "    if 'trial' in mat_dict:\n",
    "        trials = mat_dict['trial']\n",
    "        # try to parse trials (list-like)\n",
    "        try:\n",
    "            samples = []\n",
    "            labels = []\n",
    "            for t in trials:\n",
    "                if hasattr(t, 'data'):\n",
    "                    samples.append(np.array(t.data))\n",
    "                elif isinstance(t, np.ndarray) and t.ndim==2:\n",
    "                    samples.append(t)\n",
    "                # labels\n",
    "                lab = struct_field(t, 'class') or struct_field(t,'label')\n",
    "                if lab is not None:\n",
    "                    labels.append(int(lab))\n",
    "            if len(samples)>0 and len(labels)==len(samples):\n",
    "                # stitch together and compute marker positions\n",
    "                # produce continuous concatenation (not always correct) - return as fallback\n",
    "                data = np.concatenate(samples, axis=1)\n",
    "                pos = []\n",
    "                cur = 0\n",
    "                for s in samples:\n",
    "                    pos.append(cur); cur += s.shape[1]\n",
    "                for p,l in zip(pos, labels):\n",
    "                    markers.setdefault(int(l), []).append(int(p))\n",
    "                return np.array(data), None, markers\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # final fallback: can't parse\n",
    "    return (np.array(data) if data is not None else None), None, None\n",
    "\n",
    "# ---------- Inspect .mat files in BNCI_ROOT ----------\n",
    "mat_files = sorted(glob.glob(os.path.join(BNCI_ROOT, \"*.mat\")))\n",
    "if len(mat_files)==0:\n",
    "    # also look recursively\n",
    "    mat_files = sorted(glob.glob(os.path.join(BNCI_ROOT, \"**\", \"*.mat\"), recursive=True))\n",
    "\n",
    "print(f\"Found {len(mat_files)} .mat files (showing up to 30):\")\n",
    "for mf in mat_files[:30]:\n",
    "    print(\" -\", os.path.basename(mf))\n",
    "print(\"\\nInspecting a few files to show keys inside (first 6):\\n\")\n",
    "for mf in mat_files[:6]:\n",
    "    print(\"FILE:\", os.path.basename(mf))\n",
    "    try:\n",
    "        keys = list_mat_keys(mf)\n",
    "        print(\"  keys:\", keys)\n",
    "    except Exception as e:\n",
    "        print(\"  failed to read keys:\", e)\n",
    "    print(\"-\"*40)\n",
    "\n",
    "# ---------- Load baseline preprocessed to get shape & sfreq ----------\n",
    "if not os.path.exists(PREPRO_PATH):\n",
    "    raise FileNotFoundError(f\"Baseline preprocessed file '{PREPRO_PATH}' not found. Save it first.\")\n",
    "d = np.load(PREPRO_PATH, allow_pickle=True)\n",
    "X0 = d['X']; y0 = d['y'].astype(int)\n",
    "meta0 = {}\n",
    "if 'meta' in d:\n",
    "    meta_raw = d['meta']\n",
    "    try:\n",
    "        meta0 = meta_raw.item() if meta_raw.shape==() else dict(meta_raw)\n",
    "    except Exception:\n",
    "        meta0 = {}\n",
    "n_chans = X0.shape[1]; n_times_target = X0.shape[2]\n",
    "sfreq_target = float(meta0.get('sfreq', 250.0))\n",
    "epoch_duration = n_times_target / sfreq_target\n",
    "print(\"\\nBaseline epoch shape:\", X0.shape, \"sfreq_target:\", sfreq_target, \"epoch_duration(s):\", epoch_duration)\n",
    "\n",
    "# ---------- Try quick extraction on each .mat and report success metrics ----------\n",
    "extracted = []\n",
    "for mf in mat_files:\n",
    "    print(\"\\n=== Trying:\", os.path.basename(mf))\n",
    "    try:\n",
    "        mat = loadmat(mf, squeeze_me=True, struct_as_record=False)\n",
    "    except Exception as e:\n",
    "        print(\" loadmat failed:\", e); continue\n",
    "    data, sfreq_guess, markers = try_infer_eeg_and_markers(mat)\n",
    "    if data is None:\n",
    "        print(\"  Could not find continuous EEG array in this .mat (tried common keys). Keys:\", list(mat.keys())[:30])\n",
    "        continue\n",
    "    print(\"  Found EEG array with shape:\", np.shape(data))\n",
    "    if markers is None:\n",
    "        print(\"  Could not find marker positions in this file automatically.\")\n",
    "        # show keys that might contain markers to help you set EVENT_ID_MAP later\n",
    "        possible = [k for k in mat.keys() if any(sub in k.lower() for sub in ['mrk','evt','event','pos','trial','y','label'])]\n",
    "        print(\"  Possible keys for markers:\", possible)\n",
    "        continue\n",
    "\n",
    "    # validate channel count match or not\n",
    "    if data.ndim==2:\n",
    "        ch, samples = data.shape\n",
    "    elif data.ndim==1:\n",
    "        ch = 1; samples = data.shape[0]\n",
    "        data = data.reshape(1, -1)\n",
    "    else:\n",
    "        print(\"  Unexpected data ndim:\", data.ndim); continue\n",
    "\n",
    "    print(\"  marker codes found:\", sorted(markers.keys())[:20], \"counts:\", {k: len(v) for k,v in markers.items()})\n",
    "    if ch != n_chans:\n",
    "        print(f\"  Channel count mismatch: file has {ch} ch but baseline uses {n_chans} ch.\")\n",
    "        # attempt to find channel names in mat to align (best-effort)\n",
    "        ch_names = None\n",
    "        for cand in ('clab','chan','labels','label','chans','channel'):\n",
    "            if cand in mat:\n",
    "                ch_names = mat[cand]\n",
    "                break\n",
    "        if ch_names is not None:\n",
    "            print(\"   Found candidate channel names in file:\", getattr(ch_names, 'shape', 'ok'))\n",
    "        else:\n",
    "            print(\"   No channel name info found; we'll skip auto-extraction for this file.\")\n",
    "            continue\n",
    "\n",
    "    # if channels match: compute epochs from markers (resample if needed)\n",
    "    # convert data to float32 and ensure shape chan x samples\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    # determine file sampling freq if present\n",
    "    file_sfreq = None\n",
    "    for sfk in ('fs','srate','sr','sfreq'):\n",
    "        if sfk in mat:\n",
    "            file_sfreq = float(mat[sfk])\n",
    "            break\n",
    "    if file_sfreq is None and sfreq_guess is not None:\n",
    "        file_sfreq = sfreq_guess\n",
    "    if file_sfreq is None:\n",
    "        file_sfreq = sfreq_target  # fallback\n",
    "    # resample if file_sfreq != sfreq_target\n",
    "    if abs(file_sfreq - sfreq_target) > 1e-3:\n",
    "        # resample each channel to target\n",
    "        n_target = int(round(data.shape[1] * (sfreq_target / file_sfreq)))\n",
    "        data = resample(data, n_target, axis=1).astype(np.float32)\n",
    "        print(f\"  Resampled from {file_sfreq}Hz -> {sfreq_target}Hz; new samples {data.shape[1]}\")\n",
    "\n",
    "    # compute epoch sample window\n",
    "    tmin_samps = int(round(TMIN * sfreq_target))\n",
    "    tmax_samps = int(round((TMIN + epoch_duration) * sfreq_target))\n",
    "    n_times = tmax_samps - tmin_samps\n",
    "\n",
    "    # for each marker code, extract epochs\n",
    "    epochs_list = []\n",
    "    labels_list = []\n",
    "    for code, pos_list in markers.items():\n",
    "        for p in pos_list:\n",
    "            st = int(p + tmin_samps)\n",
    "            ed = int(p + tmax_samps)\n",
    "            if st < 0 or ed > data.shape[1]:\n",
    "                continue\n",
    "            seg = data[:, st:ed]\n",
    "            if seg.shape[1] != n_times:\n",
    "                continue\n",
    "            epochs_list.append(seg)\n",
    "            labels_list.append(int(code))\n",
    "    if len(epochs_list)==0:\n",
    "        print(\"  No usable epochs extracted (markers may be outside bounds).\")\n",
    "        continue\n",
    "    epochs_arr = np.stack(epochs_list).astype(np.float32)  # shape (n_epochs_file, ch, n_times)\n",
    "    print(f\"  Extracted {epochs_arr.shape[0]} epochs shape {epochs_arr.shape}\")\n",
    "    extracted.append((mf, epochs_arr, np.array(labels_list, dtype=int)))\n",
    "\n",
    "# ---------- Summary and next actions ----------\n",
    "print(\"\\n\\n=== Extraction summary ===\")\n",
    "total_ex = sum(e[1].shape[0] for e in extracted)\n",
    "print(\"Files with successful extraction:\", len(extracted), \"total extracted epochs:\", total_ex)\n",
    "for f, arr, labs in extracted:\n",
    "    print(\" -\", os.path.basename(f), \"->\", arr.shape, \"labels:\", dict(zip(*np.unique(labs, return_counts=True))))\n",
    "\n",
    "if len(extracted)==0:\n",
    "    print(\"\\nNo files were auto-extracted. Inspect the keys printed above for a representative .mat and tell me which key contains EEG data and which contains event positions/labels (I can then give a short mapping).\")\n",
    "else:\n",
    "    # OPTIONAL: combine extracted into a single X_BNCI and y_BNCI\n",
    "    X_bnci = np.concatenate([arr for (_,arr,_) in extracted], axis=0)\n",
    "    y_bnci = np.concatenate([labs for (_,_,labs) in extracted], axis=0)\n",
    "    print(\"\\nCombined BNCI extracted shape:\", X_bnci.shape, y_bnci.shape)\n",
    "    # NOTE: BNCI labels may be non-zero-indexed (e.g., 1/2) -> rebase to 0..C-1\n",
    "    y_bnci = y_bnci - y_bnci.min()\n",
    "    print(\"BNCI label distribution after rebase:\", dict(zip(*np.unique(y_bnci, return_counts=True))))\n",
    "    # Merge with baseline preprocessed\n",
    "    X_comb = np.concatenate([X0, X_bnci], axis=0)\n",
    "    y_comb = np.concatenate([y0 - y0.min(), y_bnci], axis=0)  # ensure baseline labels are zero-indexed\n",
    "    print(\"Merged shape:\", X_comb.shape, y_comb.shape, \"saving to:\", OUT_PATH)\n",
    "    meta_comb = {'sfreq': sfreq_target}\n",
    "    np.savez_compressed(OUT_PATH, X=X_comb.astype(np.float32), y=y_comb.astype(int), meta=meta_comb)\n",
    "    print(\"Saved merged dataset to\", OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf92c769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 mat files. Using first: A01E.mat\n",
      "Loading: C:\\Users\\roger\\Desktop\\vs_code\\brain_intent_decoding\\BNCI_folder\\A01E.mat\n",
      "\n",
      "Top-level keys in .mat:\n",
      "['data']\n",
      "\n",
      "mat['data'] type: <class 'numpy.ndarray'>\n",
      "ndim: 1 shape: (9,)\n",
      "\n",
      "Inspecting attributes/fields of mat['data'] (first-level):\n",
      "['T', 'all', 'any', 'argmax', 'argmin', 'argpartition', 'argsort', 'astype', 'base', 'byteswap', 'choose', 'clip', 'compress', 'conj', 'conjugate', 'copy', 'ctypes', 'cumprod', 'cumsum', 'data', 'device', 'diagonal', 'dot', 'dtype', 'dump', 'dumps', 'fill', 'flags', 'flat', 'flatten', 'getfield', 'imag', 'item', 'itemset', 'itemsize', 'mT', 'max', 'mean', 'min', 'nbytes', 'ndim', 'newbyteorder', 'nonzero', 'partition', 'prod', 'ptp', 'put', 'ravel', 'real', 'repeat', 'reshape', 'resize', 'round', 'searchsorted', 'setfield', 'setflags', 'shape', 'size', 'sort', 'squeeze', 'std', 'strides', 'sum', 'swapaxes', 'take', 'to_device', 'tobytes', 'tofile', 'tolist', 'tostring', 'trace', 'transpose', 'var', 'view']\n",
      "\n",
      "Candidate marker/label keys found at top-level (possibly):\n",
      "   data -> <class 'numpy.ndarray'> (9,)\n",
      "    contents: [<scipy.io.matlab._mio5_params.mat_struct object at 0x0000019E15ECE750>\n",
      " <scipy.io.matlab._mio5_params.mat_struct object at 0x0000019E15ECF350>\n",
      " <scipy.io.matlab._mio5_params.mat_struct object at 0x0000019E15ECFFD0>\n",
      " <scipy.io.matlab._mio5_params.mat_struct object at 0x0000019E53F8A190>\n",
      " <scipy.io.matlab._mio5_params.mat_struct object at 0x0000019E53F84290>\n",
      " <scipy.io.matlab._mio5_params.mat_struct object at 0x0000019E15D93AD0>\n",
      " <scipy.io.matlab._mio5_params.mat_struct object at 0x0000019E53F9D010>\n",
      " <scipy.io.matlab._mio5_params.mat_struct object at 0x0000019E15F3B810>\n",
      " <scipy.io.matlab._mio5_params.mat_struct object at 0x0000019E15F387D0>]\n",
      "\n",
      "If nothing obvious, paste the output here (the top-level keys list and the preview).\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: inspect structure inside first BNCI .mat file (A01E.mat)\n",
    "from scipy.io import loadmat\n",
    "import numpy as np, os, glob, pprint\n",
    "\n",
    "BNCI_ROOT = r\"C:\\Users\\roger\\Desktop\\vs_code\\brain_intent_decoding\\BNCI_folder\"  # adjust if needed\n",
    "mat_files = sorted(glob.glob(os.path.join(BNCI_ROOT, \"*.mat\")))\n",
    "if len(mat_files) == 0:\n",
    "    mat_files = sorted(glob.glob(os.path.join(BNCI_ROOT, \"**\", \"*.mat\"), recursive=True))\n",
    "print(\"Found\", len(mat_files), \"mat files. Using first:\", os.path.basename(mat_files[0]))\n",
    "\n",
    "mf = mat_files[0]\n",
    "print(\"Loading:\", mf)\n",
    "mat = loadmat(mf, squeeze_me=True, struct_as_record=False)\n",
    "\n",
    "print(\"\\nTop-level keys in .mat:\")\n",
    "top_keys = [k for k in mat.keys() if not k.startswith(\"__\")]\n",
    "print(top_keys)\n",
    "\n",
    "# Show mat['data'] type and a small preview\n",
    "if 'data' in mat:\n",
    "    d = mat['data']\n",
    "    print(\"\\nmat['data'] type:\", type(d))\n",
    "    try:\n",
    "        print(\"ndim:\", getattr(d, 'ndim', None), \"shape:\", getattr(d,'shape', None))\n",
    "    except Exception:\n",
    "        pass\n",
    "    # if numeric array, show min/max and dtype\n",
    "    if isinstance(d, np.ndarray) and np.issubdtype(d.dtype, np.number):\n",
    "        print(\"numeric array info: dtype=\", d.dtype, \"shape=\", d.shape,\n",
    "              \"min/max:\", np.min(d), np.max(d))\n",
    "    else:\n",
    "        # if struct-like object, print its attributes / fields\n",
    "        print(\"\\nInspecting attributes/fields of mat['data'] (first-level):\")\n",
    "        try:\n",
    "            # Try dict-like access\n",
    "            if hasattr(d, '__dict__'):\n",
    "                fields = list(d.__dict__.keys())\n",
    "            else:\n",
    "                # for numpy.object arrays or matlab structs, use dir() and filter\n",
    "                fields = [a for a in dir(d) if not a.startswith('_')][:200]\n",
    "            print(fields)\n",
    "        except Exception as e:\n",
    "            print(\"  (couldn't enumerate fields):\", e)\n",
    "\n",
    "# Try to find likely marker/label fields anywhere in the mat\n",
    "candidates = ['mrk','mrk_pos','mrk_y','y','pos','classlabel','trial','event','events','eventpos','label','labels','markers']\n",
    "found = {}\n",
    "for k in top_keys:\n",
    "    if k.lower() in candidates:\n",
    "        found[k] = mat[k]\n",
    "    # also inspect contents if struct-like\n",
    "    val = mat[k]\n",
    "    try:\n",
    "        # if struct with attributes\n",
    "        attrs = []\n",
    "        if hasattr(val, '__dict__'):\n",
    "            attrs = list(val.__dict__.keys())\n",
    "        elif isinstance(val, np.ndarray) and val.dtype == object:\n",
    "            attrs = ['object-array']\n",
    "        else:\n",
    "            # show shape/type\n",
    "            attrs = [f\"type={type(val)}, shape={getattr(val,'shape',None)}\"]\n",
    "        if any(c in ''.join(attrs).lower() for c in candidates):\n",
    "            found[k] = val\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"\\nCandidate marker/label keys found at top-level (possibly):\")\n",
    "if len(found)==0:\n",
    "    print(\"  none discovered automatically. Below is a compact dump of a few keys to help us map:\")\n",
    "    for k in top_keys[:30]:\n",
    "        v = mat[k]\n",
    "        try:\n",
    "            desc = f\"type={type(v)}, shape={getattr(v,'shape',None)}\"\n",
    "        except:\n",
    "            desc = str(type(v))\n",
    "        print(f\"  {k}: {desc}\")\n",
    "else:\n",
    "    for k,v in found.items():\n",
    "        print(\"  \", k, \"->\", type(v), getattr(v,'shape',None))\n",
    "        # if small numeric arrays, print contents\n",
    "        if isinstance(v, np.ndarray) and v.size < 50:\n",
    "            print(\"    contents:\", v)\n",
    "        elif hasattr(v, '__dict__'):\n",
    "            print(\"    fields:\", list(v.__dict__.keys()))\n",
    "\n",
    "print(\"\\nIf nothing obvious, paste the output here (the top-level keys list and the preview).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d29a1c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using first file: C:\\Users\\roger\\Desktop\\vs_code\\brain_intent_decoding\\BNCI_folder\\A01E.mat\n",
      "Top-level keys: ['data']\n",
      "\n",
      "Type of mat['data']: <class 'numpy.ndarray'> shape: (9,)\n",
      "\n",
      "First element type: <class 'scipy.io.matlab._mio5_params.mat_struct'>\n",
      "\n",
      "Fields found on first element (count=9):\n",
      "['_fieldnames',\n",
      " 'X',\n",
      " 'trial',\n",
      " 'y',\n",
      " 'fs',\n",
      " 'classes',\n",
      " 'artifacts',\n",
      " 'gender',\n",
      " 'age']\n",
      "\n",
      "Field summaries (first element):\n",
      " - _fieldnames: list/tuple len=8; sample types/distinct={<class 'str'>}\n",
      " - X: dtype=float64, shape=(34291, 25), min/max=-88.9/136, sample=[11.23046875, -27.24609375, 6.103515625, 4.8828125, 2.05078125, -2.63671875, 3.02734375, 3.857421875]\n",
      " - trial: dtype=uint8, shape=(0,), sample unavailable\n",
      " - y: dtype=uint8, shape=(0,), sample unavailable\n",
      " - fs: <class 'int'>\n",
      " - classes: (4,)\n",
      " - artifacts: dtype=uint8, shape=(0,), sample unavailable\n",
      " - gender: <class 'str'>\n",
      " - age: <class 'int'>\n",
      "\n",
      "\n",
      "Presence summary of these fields across all elements in mat['data']:\n",
      " - X: present in 9/9 elements; examples: [(<class 'numpy.ndarray'>, (34291, 25)), (<class 'numpy.ndarray'>, (34459, 25)), (<class 'numpy.ndarray'>, (37040, 25))]\n",
      " - _fieldnames: present in 9/9 elements; examples: [(<class 'list'>, None), (<class 'list'>, None), (<class 'list'>, None)]\n",
      " - age: present in 9/9 elements; examples: [(<class 'int'>, None), (<class 'int'>, None), (<class 'int'>, None)]\n",
      " - artifacts: present in 9/9 elements; examples: [(<class 'numpy.ndarray'>, (0,)), (<class 'numpy.ndarray'>, (0,)), (<class 'numpy.ndarray'>, (0,))]\n",
      " - classes: present in 9/9 elements; examples: [(<class 'numpy.ndarray'>, (4,)), (<class 'numpy.ndarray'>, (4,)), (<class 'numpy.ndarray'>, (4,))]\n",
      " - fs: present in 9/9 elements; examples: [(<class 'int'>, None), (<class 'int'>, None), (<class 'int'>, None)]\n",
      " - gender: present in 9/9 elements; examples: [(<class 'str'>, None), (<class 'str'>, None), (<class 'str'>, None)]\n",
      " - trial: present in 9/9 elements; examples: [(<class 'numpy.ndarray'>, (0,)), (<class 'numpy.ndarray'>, (0,)), (<class 'numpy.ndarray'>, (0,))]\n",
      " - y: present in 9/9 elements; examples: [(<class 'numpy.ndarray'>, (0,)), (<class 'numpy.ndarray'>, (0,)), (<class 'numpy.ndarray'>, (0,))]\n",
      "\n",
      "\n",
      "If you paste the output above, I'll map the exact fields to 'EEG' and 'markers' and give extraction code.\n"
     ]
    }
   ],
   "source": [
    "# Inspect internal fields of BNCI .mat 'data' entries (run this cell)\n",
    "from scipy.io import loadmat\n",
    "import numpy as np, glob, os, pprint\n",
    "\n",
    "BNCI_ROOT = r\"C:\\Users\\roger\\Desktop\\vs_code\\brain_intent_decoding\\BNCI_folder\"  # adjust if needed\n",
    "mat_files = sorted(glob.glob(os.path.join(BNCI_ROOT, \"*.mat\")))\n",
    "if len(mat_files) == 0:\n",
    "    mat_files = sorted(glob.glob(os.path.join(BNCI_ROOT, \"**\", \"*.mat\"), recursive=True))\n",
    "print(\"Using first file:\", mat_files[0])\n",
    "\n",
    "mf = mat_files[0]\n",
    "mat = loadmat(mf, squeeze_me=True, struct_as_record=False)\n",
    "print(\"Top-level keys:\", [k for k in mat.keys() if not k.startswith(\"__\")])\n",
    "\n",
    "data_arr = mat.get('data', None)\n",
    "if data_arr is None:\n",
    "    raise RuntimeError(\"No 'data' key found in .mat\")\n",
    "\n",
    "print(\"\\nType of mat['data']:\", type(data_arr), \"shape:\", getattr(data_arr, 'shape', None))\n",
    "# If it's a 1-D array of mat_struct, show info for first element\n",
    "if isinstance(data_arr, np.ndarray) and data_arr.size>0:\n",
    "    first = data_arr.flat[0]\n",
    "    print(\"\\nFirst element type:\", type(first))\n",
    "    # list possible field names\n",
    "    fields = []\n",
    "    # mat_struct from scipy typically exposes attributes via .__dict__ or use dir()\n",
    "    try:\n",
    "        if hasattr(first, '__dict__') and isinstance(first.__dict__, dict) and len(first.__dict__)>0:\n",
    "            fields = list(first.__dict__.keys())\n",
    "        else:\n",
    "            # fallback - use dir and filter\n",
    "            fields = [a for a in dir(first) if not a.startswith('_')][:200]\n",
    "    except Exception as e:\n",
    "        fields = [a for a in dir(first) if not a.startswith('_')][:200]\n",
    "    print(\"\\nFields found on first element (count={}):\".format(len(fields)))\n",
    "    pprint.pprint(fields)\n",
    "\n",
    "    # For each field, try to print small summary (type, shape/len, a small sample)\n",
    "    print(\"\\nField summaries (first element):\")\n",
    "    for f in fields:\n",
    "        try:\n",
    "            val = getattr(first, f)\n",
    "        except Exception:\n",
    "            try:\n",
    "                val = first[f]\n",
    "            except Exception:\n",
    "                val = None\n",
    "        if val is None:\n",
    "            print(f\" - {f}: <unreadable>\")\n",
    "            continue\n",
    "        t = type(val)\n",
    "        shape = getattr(val, 'shape', None)\n",
    "        # basic numeric arrays: print dtype/min/max/first elements\n",
    "        if isinstance(val, np.ndarray) and np.issubdtype(val.dtype, np.number):\n",
    "            s = f\"dtype={val.dtype}, shape={val.shape}\"\n",
    "            try:\n",
    "                mn, mx = float(np.min(val)), float(np.max(val))\n",
    "                sample = val.ravel()[:8].tolist()\n",
    "                print(f\" - {f}: {s}, min/max={mn:.3g}/{mx:.3g}, sample={sample}\")\n",
    "            except Exception:\n",
    "                print(f\" - {f}: {s}, sample unavailable\")\n",
    "        elif isinstance(val, (list, tuple)):\n",
    "            print(f\" - {f}: list/tuple len={len(val)}; sample types/distinct={set(type(x) for x in val) if len(val)<=10 else 'varies'}\")\n",
    "        else:\n",
    "            # some fields are nested mat_structs; print attributes if present\n",
    "            try:\n",
    "                subfields = list(val.__dict__.keys()) if hasattr(val, '__dict__') else None\n",
    "            except Exception:\n",
    "                subfields = None\n",
    "            if subfields:\n",
    "                print(f\" - {f}: matstruct with fields {subfields}\")\n",
    "            else:\n",
    "                # fallback summary\n",
    "                desc = getattr(val, 'shape', None) or str(type(val))\n",
    "                print(f\" - {f}: {desc}\")\n",
    "\n",
    "    # Now produce presence summary across all elements\n",
    "    print(\"\\n\\nPresence summary of these fields across all elements in mat['data']:\")\n",
    "    all_fields = set(fields)\n",
    "    for elem in data_arr.flat:\n",
    "        try:\n",
    "            if hasattr(elem, '__dict__') and isinstance(elem.__dict__, dict):\n",
    "                all_fields.update(elem.__dict__.keys())\n",
    "            else:\n",
    "                all_fields.update([a for a in dir(elem) if not a.startswith('_')][:200])\n",
    "        except Exception:\n",
    "            pass\n",
    "    all_fields = sorted(list(all_fields))\n",
    "    summary = {}\n",
    "    for f in all_fields:\n",
    "        cnt = 0\n",
    "        vals = []\n",
    "        for elem in data_arr.flat:\n",
    "            try:\n",
    "                v = getattr(elem, f, None)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    v = elem[f]\n",
    "                except Exception:\n",
    "                    v = None\n",
    "            if v is not None:\n",
    "                cnt += 1\n",
    "                # collect small shape/type info\n",
    "                try:\n",
    "                    shape = getattr(v, 'shape', None)\n",
    "                    t = type(v)\n",
    "                    vals.append((t, shape))\n",
    "                except Exception:\n",
    "                    vals.append((type(v), None))\n",
    "        summary[f] = (cnt, vals[:3])\n",
    "    # print compact\n",
    "    for f in all_fields:\n",
    "        c, sample_info = summary[f]\n",
    "        if c>0:\n",
    "            print(f\" - {f}: present in {c}/{data_arr.size} elements; examples: {sample_info}\")\n",
    "else:\n",
    "    print(\"mat['data'] is not an ndarray or empty; value:\", data_arr)\n",
    "\n",
    "print(\"\\n\\nIf you paste the output above, I'll map the exact fields to 'EEG' and 'markers' and give extraction code.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c64fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting: C:\\Users\\roger\\Desktop\\vs_code\\brain_intent_decoding\\BNCI_folder\\A01E.mat\n",
      "data array shape: (9,)\n",
      "\n",
      "--- FIRST ELEMENT DETAILS ---\n",
      "\n",
      "Field: _fieldnames\n",
      "  type: <class 'list'>\n",
      "  shape/type: None None\n",
      "  preview: ['X' 'trial' 'y' 'fs' 'classes' 'artifacts' 'gender' 'age']\n",
      "\n",
      "Field: X\n",
      "  type: <class 'numpy.ndarray'>\n",
      "  shape/type: (34291, 25) float64\n",
      "  preview (first 8): [ 11.23046875 -27.24609375   6.10351562   4.8828125    2.05078125\n",
      "  -2.63671875   3.02734375   3.85742188]\n",
      "\n",
      "Field: fs\n",
      "  type: <class 'int'>\n",
      "  shape/type: None None\n",
      "\n",
      "Field: trial\n",
      "  type: <class 'numpy.ndarray'>\n",
      "  shape/type: (0,) uint8\n",
      "  preview: []\n",
      "\n",
      "Field: y\n",
      "  type: <class 'numpy.ndarray'>\n",
      "  shape/type: (0,) uint8\n",
      "  preview: []\n",
      "\n",
      "Field: classes\n",
      "  type: <class 'numpy.ndarray'>\n",
      "  shape/type: (4,) object\n",
      "  preview: ['left hand' 'right hand' 'feet' 'tongue']\n",
      "\n",
      "Field: age\n",
      "  type: <class 'int'>\n",
      "  shape/type: None None\n",
      "\n",
      "Field: gender\n",
      "  type: <class 'str'>\n",
      "  shape/type: None None\n",
      "\n",
      "Field: artifacts\n",
      "  type: <class 'numpy.ndarray'>\n",
      "  shape/type: (0,) uint8\n",
      "  preview: []\n",
      "\n",
      "--- SUMMARY ACROSS ALL ELEMENTS ---\n",
      "X: present in 9/9 elements; sample shapes/types: [(34291, 25), (34459, 25), (37040, 25), (96735, 25), (96735, 25)]\n",
      "fs: present in 9/9 elements; sample shapes/types: [None, None, None, None, None]\n",
      "trial: present in 9/9 elements; sample shapes/types: [(0,), (0,), (0,), (48,), (48,)]\n",
      "y: present in 9/9 elements; sample shapes/types: [(0,), (0,), (0,), (48,), (48,)]\n",
      "classes: present in 9/9 elements; sample shapes/types: [(4,), (4,), (4,), (4,), (4,)]\n",
      "_fieldnames: present in 9/9 elements; sample shapes/types: [None, None, None, None, None]\n",
      "\n",
      "_channel names from _fieldnames (first element):\n",
      "['X' 'trial' 'y' 'fs' 'classes' 'artifacts' 'gender' 'age']\n",
      "\n",
      "Done. Paste the entire output here and I'll provide the exact extraction code (epochs + labels) to merge BNCI into your preprocessed dataset.\n"
     ]
    }
   ],
   "source": [
    "# Small diagnostic: print the useful fields inside each element of mat['data']\n",
    "from scipy.io import loadmat\n",
    "import glob, os, numpy as np, pprint\n",
    "\n",
    "BNCI_ROOT = r\"C:\\Users\\roger\\Desktop\\vs_code\\brain_intent_decoding\\BNCI_folder\"  # update if needed\n",
    "mat_files = sorted(glob.glob(os.path.join(BNCI_ROOT, \"*.mat\")))\n",
    "mf = mat_files[0]\n",
    "print(\"Inspecting:\", mf)\n",
    "mat = loadmat(mf, squeeze_me=True, struct_as_record=False)\n",
    "data_arr = mat['data']\n",
    "print(\"data array shape:\", getattr(data_arr,'shape',None))\n",
    "\n",
    "def safe_attr(obj, name):\n",
    "    try:\n",
    "        return getattr(obj, name)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return obj[name]\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "# Print full details for the first element\n",
    "first = data_arr.flat[0]\n",
    "print(\"\\n--- FIRST ELEMENT DETAILS ---\")\n",
    "for fld in ['_fieldnames', 'X', 'fs', 'trial', 'y', 'classes', 'age', 'gender', 'artifacts']:\n",
    "    v = safe_attr(first, fld)\n",
    "    print(f\"\\nField: {fld}\")\n",
    "    if v is None:\n",
    "        print(\"  <missing>\")\n",
    "        continue\n",
    "    print(\"  type:\", type(v))\n",
    "    try:\n",
    "        print(\"  shape/type:\", getattr(v,'shape', None), getattr(v,'dtype', None))\n",
    "    except Exception:\n",
    "        pass\n",
    "    # small preview\n",
    "    if isinstance(v, (list, tuple, np.ndarray)) and np.asarray(v).size<=20:\n",
    "        print(\"  preview:\", np.asarray(v))\n",
    "    elif isinstance(v, np.ndarray) and np.issubdtype(v.dtype, np.number):\n",
    "        arr = np.asarray(v)\n",
    "        print(\"  preview (first 8):\", arr.ravel()[:8])\n",
    "    else:\n",
    "        # mat_struct nested - show its attributes\n",
    "        try:\n",
    "            sub = v.__dict__ if hasattr(v,'__dict__') else None\n",
    "            if sub:\n",
    "                print(\"  subfields:\", list(sub.keys()))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Now summarize presence/len info across all elements\n",
    "print(\"\\n--- SUMMARY ACROSS ALL ELEMENTS ---\")\n",
    "summary_fields = ['X','fs','trial','y','classes','_fieldnames']\n",
    "for fld in summary_fields:\n",
    "    present = 0\n",
    "    shapes = []\n",
    "    for elem in data_arr.flat:\n",
    "        val = safe_attr(elem, fld)\n",
    "        if val is not None:\n",
    "            present += 1\n",
    "            try:\n",
    "                shapes.append(getattr(val,'shape', None))\n",
    "            except Exception:\n",
    "                shapes.append(type(val))\n",
    "    print(f\"{fld}: present in {present}/{data_arr.size} elements; sample shapes/types: {shapes[:5]}\")\n",
    "\n",
    "# Print channel names if available in _fieldnames of first element\n",
    "fn = safe_attr(first, '_fieldnames')\n",
    "if fn is not None:\n",
    "    print(\"\\n_channel names from _fieldnames (first element):\")\n",
    "    try:\n",
    "        print(np.asarray(fn))\n",
    "    except Exception:\n",
    "        print(fn)\n",
    "\n",
    "print(\"\\nDone. Paste the entire output here and I'll provide the exact extraction code (epochs + labels) to merge BNCI into your preprocessed dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d2f7e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline epoch shape: (30, 64, 561) target_sfreq: 250.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'left hand'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 142\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# iterate elements\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m data_arr.flat:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     epochs, labels = \u001b[43mextract_epochs_from_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_n_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sfreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m epochs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    144\u001b[39m         \u001b[38;5;66;03m# skip this element\u001b[39;00m\n\u001b[32m    145\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mextract_epochs_from_element\u001b[39m\u001b[34m(elem, target_n_times, target_sfreq)\u001b[39m\n\u001b[32m     87\u001b[39m     seg = seg.T.astype(np.float32)\n\u001b[32m     88\u001b[39m     epochs.append(seg)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     labels.append(\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     90\u001b[39m epochs = np.stack(epochs, axis=\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# (n_trials, n_ch, n_times_file)\u001b[39;00m\n\u001b[32m     91\u001b[39m labels = np.array(labels, dtype=\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: 'left hand'"
     ]
    }
   ],
   "source": [
    "# ===== BNCI extraction cell (handles trial-less 'classes' case) =====\n",
    "# Paste into your advanced notebook. Requires: numpy, scipy, scipy.io, mne (optional), tqdm\n",
    "import os, glob, numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import resample\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- USER SETTINGS ----------\n",
    "BNCI_ROOT = r\"C:\\Users\\roger\\Desktop\\vs_code\\brain_intent_decoding\\BNCI_folder\"  # update if needed\n",
    "BASELINE_PREPRO = \"preprocessed.npz\"     # your baseline (EEGBCI) preprocessed file\n",
    "OUT_BNCI = \"preprocessed_BNCI.npz\"       # will be written\n",
    "# If baseline epoch length differs from BNCI trial length, this controls how we trim/resample\n",
    "# We'll infer target epoch length from BASELINE_PREPRO automatically below.\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def safe_getattr(o, name):\n",
    "    try:\n",
    "        return getattr(o, name)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return o[name]\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def extract_epochs_from_element(elem, target_n_times, target_sfreq):\n",
    "    \"\"\"\n",
    "    elem: mat_struct element -> we expect elem.X (n_samples x n_ch), elem.fs (scalar), elem.classes or elem.trial/elem.y\n",
    "    Returns: epochs (n_trials, n_ch, target_n_times) and labels (n_trials,)\n",
    "    \"\"\"\n",
    "    X = safe_getattr(elem, \"X\")  # X shape (n_samples, n_ch) in this .mat format\n",
    "    if X is None:\n",
    "        raise ValueError(\"No X field found in element\")\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    # note: in file X is (n_samples, n_channels) -> we will convert to (n_epochs, n_ch, n_times) later\n",
    "    file_fs = safe_getattr(elem, \"fs\") or None\n",
    "    classes = safe_getattr(elem, \"classes\")\n",
    "    trial = safe_getattr(elem, \"trial\")\n",
    "    y = safe_getattr(elem, \"y\")\n",
    "\n",
    "    n_samples, n_ch = X.shape\n",
    "    # Case A: trial / y contain onsets or trial-wise arrays (rare here) - handle if present and non-empty\n",
    "    if isinstance(trial, (list, tuple, np.ndarray)) and np.asarray(trial).size > 0 and isinstance(y, (list, np.ndarray)) and np.asarray(y).size>0:\n",
    "        # attempt to interpret trial as start indices or list of arrays\n",
    "        try:\n",
    "            trial_arr = np.asarray(trial)\n",
    "            # if trial is list of arrays each trial data: stack them\n",
    "            if trial_arr.ndim == 1 and trial_arr.size > 0 and np.asarray(trial_arr[0]).ndim == 2:\n",
    "                epochs = [np.array(t, dtype=np.float32) for t in trial_arr]\n",
    "                labels = np.asarray(y).astype(int).ravel()\n",
    "            else:\n",
    "                # otherwise we don't have direct per-trial arrays here -> fallback to classes below\n",
    "                epochs = None; labels = None\n",
    "        except Exception:\n",
    "            epochs = None; labels = None\n",
    "    else:\n",
    "        epochs = None; labels = None\n",
    "\n",
    "    # Case B: use classes array and split X sequentially into n_trials segments.\n",
    "    if epochs is None:\n",
    "        if classes is None:\n",
    "            # try y as labels (possible)\n",
    "            if y is not None and np.asarray(y).size>0:\n",
    "                classes = np.asarray(y).ravel()\n",
    "        if classes is None:\n",
    "            # cannot determine trials\n",
    "            return None, None\n",
    "        classes = np.asarray(classes).ravel()\n",
    "        n_trials = len(classes)\n",
    "        if n_trials <= 0:\n",
    "            return None, None\n",
    "        # compute samples per trial (must be integer)\n",
    "        if n_samples % n_trials != 0:\n",
    "            # if not divisible, try rounding segments of approx equal size\n",
    "            samples_per_trial = int(np.floor(n_samples / n_trials))\n",
    "        else:\n",
    "            samples_per_trial = n_samples // n_trials\n",
    "        # extract sequential segments\n",
    "        epochs = []\n",
    "        labels = []\n",
    "        for i in range(n_trials):\n",
    "            st = i * samples_per_trial\n",
    "            ed = st + samples_per_trial\n",
    "            if ed > n_samples:\n",
    "                break\n",
    "            seg = X[st:ed, :]  # shape (samples_per_trial, n_ch)\n",
    "            # transpose to (n_ch, n_times)\n",
    "            seg = seg.T.astype(np.float32)\n",
    "            epochs.append(seg)\n",
    "            labels.append(int(classes[i]))\n",
    "        epochs = np.stack(epochs, axis=0)  # (n_trials, n_ch, n_times_file)\n",
    "        labels = np.array(labels, dtype=int)\n",
    "\n",
    "    # Now we have epochs shape (n_trials, n_ch, n_times_file). We need to resample in time to target_n_times\n",
    "    n_trials, n_ch, n_times_file = epochs.shape\n",
    "    if file_fs is None:\n",
    "        file_fs = float(safe_getattr(elem, \"fs\") or target_sfreq)\n",
    "    # If sample count differs from target_n_times, resample each epoch in time axis\n",
    "    if n_times_file != target_n_times:\n",
    "        epochs_res = np.zeros((n_trials, n_ch, target_n_times), dtype=np.float32)\n",
    "        for ti in range(n_trials):\n",
    "            for ch in range(n_ch):\n",
    "                epochs_res[ti, ch, :] = resample(epochs[ti, ch, :], target_n_times)\n",
    "        epochs = epochs_res\n",
    "    return epochs, labels\n",
    "\n",
    "# ---------- infer target epoch length & sfreq from baseline preprocessed ----------\n",
    "\n",
    "if not os.path.exists(BASELINE_PREPRO):\n",
    "    raise FileNotFoundError(f\"Baseline preprocessed file '{BASELINE_PREPRO}' not found. Save baseline first.\")\n",
    "\n",
    "d = np.load(BASELINE_PREPRO, allow_pickle=True)\n",
    "X0 = d['X']\n",
    "y0 = d['y'].astype(int)\n",
    "meta0 = {}\n",
    "if 'meta' in d:\n",
    "    meta_raw = d['meta']\n",
    "    try:\n",
    "        meta0 = meta_raw.item() if meta_raw.shape==() else dict(meta_raw)\n",
    "    except Exception:\n",
    "        meta0 = {}\n",
    "\n",
    "target_n_ch = X0.shape[1]\n",
    "target_n_times = X0.shape[2]\n",
    "target_sfreq = float(meta0.get('sfreq', 250.0))\n",
    "print(\"Baseline epoch shape:\", X0.shape, \"target_sfreq:\", target_sfreq)\n",
    "\n",
    "# ---------- iterate BNCI .mat files and extract by element ----------\n",
    "mat_files = sorted(glob.glob(os.path.join(BNCI_ROOT, \"*.mat\")))\n",
    "extracted_epochs = []\n",
    "extracted_labels = []\n",
    "file_info = []\n",
    "for mf in mat_files:\n",
    "    try:\n",
    "        mat = loadmat(mf, squeeze_me=True, struct_as_record=False)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load\", mf, \":\", e); continue\n",
    "    data_arr = mat.get('data', None)\n",
    "    if data_arr is None:\n",
    "        print(\"No data entry in\", mf); continue\n",
    "    # iterate elements\n",
    "    for elem in data_arr.flat:\n",
    "        epochs, labels = extract_epochs_from_element(elem, target_n_times, target_sfreq)\n",
    "        if epochs is None or labels is None:\n",
    "            # skip this element\n",
    "            continue\n",
    "        # epochs shape (n_trials, n_ch_file, target_n_times)\n",
    "        n_trials, n_ch_file, _ = epochs.shape\n",
    "        file_info.append((mf, n_trials, n_ch_file))\n",
    "        extracted_epochs.append(epochs)\n",
    "        extracted_labels.append(labels)\n",
    "\n",
    "# concatenate\n",
    "if len(extracted_epochs)==0:\n",
    "    print(\"No epochs extracted from BNCI files automatically. See file_info:\", file_info)\n",
    "else:\n",
    "    X_bnci = np.concatenate(extracted_epochs, axis=0)   # (N_epochs_bnci, n_ch_file, target_n_times)\n",
    "    y_bnci = np.concatenate(extracted_labels, axis=0)\n",
    "    print(\"Extracted BNCI epochs shape:\", X_bnci.shape, \"labels shape:\", y_bnci.shape)\n",
    "    # rebase bnci labels to 0..C-1\n",
    "    y_bnci = y_bnci - y_bnci.min()\n",
    "    print(\"BNCI label counts:\", dict(zip(*np.unique(y_bnci, return_counts=True))))\n",
    "\n",
    "    # Save BNCI-only preprocessed file\n",
    "    np.savez_compressed(OUT_BNCI, X=X_bnci.astype(np.float32), y=y_bnci.astype(int), meta={'sfreq': target_sfreq})\n",
    "    print(\"Saved BNCI preprocessed to\", OUT_BNCI)\n",
    "\n",
    "    # Show channel mismatch with baseline\n",
    "    print(\"\\nBaseline channels:\", target_n_ch, \"BNCI file channels:\", X_bnci.shape[1])\n",
    "    if X_bnci.shape[1] != target_n_ch:\n",
    "        print(\"Channel count mismatch. To merge with baseline you must either:\")\n",
    "        print(\"  1) align channel names and pick common subset (best, requires channel names), OR\")\n",
    "        print(\"  2) retrain on BNCI-only dataset (we saved preprocessed_BNCI.npz), OR\")\n",
    "        print(\"  3) as quick hack, truncate baseline channels to first N or BNCI channels to first N (not recommended).\")\n",
    "    else:\n",
    "        print(\"Channel counts match; you can concatenate with baseline X0 directly and save combined file.\")\n",
    "\n",
    "# =======================================================\n",
    "# If you want merging (automatic) despite mismatch, uncomment one of the two options below:\n",
    "# OPTION A (recommended if you have channel name lists): write an alignment routine (I can add it if you paste your baseline channel names)\n",
    "# OPTION B (quick hack, NOT recommended): truncate baseline or BNCI channels to min(target_n_ch, n_ch_file)\n",
    "# Example quick-hack merge (uncomment to use):\n",
    "#\n",
    "# if len(extracted_epochs)>0:\n",
    "#     min_ch = min(target_n_ch, X_bnci.shape[1])\n",
    "#     X0_trunc = X0[:, :min_ch, :]\n",
    "#     X_bnci_trunc = X_bnci[:, :min_ch, :]\n",
    "#     X_comb = np.concatenate([X0_trunc, X_bnci_trunc], axis=0)\n",
    "#     y_comb = np.concatenate([y0 - y0.min(), y_bnci], axis=0)\n",
    "#     np.savez_compressed(\"preprocessed_combined_quickhack.npz\", X=X_comb.astype(np.float32), y=y_comb.astype(int), meta={'sfreq':target_sfreq})\n",
    "#     print(\"Saved quick-hack merged file: preprocessed_combined_quickhack.npz\")\n",
    "#\n",
    "# =======================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c9cdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline epoch shape: (30, 64, 561) target_sfreq: 250.0\n",
      "Extracted BNCI epochs shape: (640, 25, 561) labels shape: (640,)\n",
      "Global label map (text->int): {'left hand': 0, 'right hand': 1, 'feet': 2, 'tongue': 3}\n",
      "Saved BNCI preprocessed to preprocessed_BNCI.npz\n",
      "Baseline channels: 64 BNCI file channels: 25\n",
      "Channel count mismatch detected. To merge safely we should align channel names or pick intersection.\n"
     ]
    }
   ],
   "source": [
    "# ===== Fixed BNCI extraction (handles string class names like \"left hand\") =====\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import resample\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- USER SETTINGS ----------\n",
    "BNCI_ROOT = r\"C:\\Users\\roger\\Desktop\\vs_code\\brain_intent_decoding\\BNCI_folder\"  # update if needed\n",
    "BASELINE_PREPRO = \"preprocessed.npz\"     # your baseline (EEGBCI) preprocessed file\n",
    "OUT_BNCI = \"preprocessed_BNCI.npz\"       # will be written\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def safe_getattr(o, name):\n",
    "    try:\n",
    "        return getattr(o, name)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return o[name]\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def normalize_label(x):\n",
    "    \"\"\"Turn a label (str or number) into a normalized string key for mapping.\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, bytes):\n",
    "        x = x.decode('utf-8', errors='ignore')\n",
    "    if isinstance(x, str):\n",
    "        return x.strip().lower()\n",
    "    # numeric -> return as string of int\n",
    "    try:\n",
    "        return str(int(x))\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "# Global label map across all files (keeps labels consistent)\n",
    "global_label_map = {}\n",
    "next_label_id = 0\n",
    "\n",
    "def map_label_to_int(raw_label):\n",
    "    global global_label_map, next_label_id\n",
    "    key = normalize_label(raw_label)\n",
    "    if key is None:\n",
    "        return None\n",
    "    if key not in global_label_map:\n",
    "        global_label_map[key] = next_label_id\n",
    "        next_label_id += 1\n",
    "    return global_label_map[key]\n",
    "\n",
    "def extract_epochs_from_element(elem, target_n_times, target_sfreq):\n",
    "    \"\"\"\n",
    "    elem: mat_struct element -> expect elem.X (n_samples x n_ch), elem.fs, elem.classes or elem.trial/elem.y\n",
    "    Returns: epochs (n_trials, n_ch, target_n_times) and labels (n_trials,)\n",
    "    \"\"\"\n",
    "    X = safe_getattr(elem, \"X\")\n",
    "    if X is None:\n",
    "        return None, None\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    file_fs = safe_getattr(elem, \"fs\") or None\n",
    "    classes = safe_getattr(elem, \"classes\")\n",
    "    trial = safe_getattr(elem, \"trial\")\n",
    "    y = safe_getattr(elem, \"y\")\n",
    "\n",
    "    n_samples, n_ch = X.shape\n",
    "    epochs = None; labels = None\n",
    "\n",
    "    # If trial/y are present and look like per-trial arrays, try to use them\n",
    "    if isinstance(trial, (list, tuple, np.ndarray)) and np.asarray(trial).size > 0 and isinstance(y, (list, np.ndarray)) and np.asarray(y).size>0:\n",
    "        try:\n",
    "            trial_arr = np.asarray(trial)\n",
    "            if trial_arr.ndim == 1 and trial_arr.size > 0 and np.asarray(trial_arr[0]).ndim == 2:\n",
    "                # each element is a trial array\n",
    "                epochs = [np.array(t, dtype=np.float32) for t in trial_arr]\n",
    "                labels = np.asarray(y).astype(int).ravel()\n",
    "        except Exception:\n",
    "            epochs = None; labels = None\n",
    "\n",
    "    # Fallback: use classes (may be strings)\n",
    "    if epochs is None:\n",
    "        if classes is None:\n",
    "            if y is not None and np.asarray(y).size>0:\n",
    "                classes = np.asarray(y).ravel()\n",
    "        if classes is None:\n",
    "            return None, None\n",
    "        classes_arr = np.asarray(classes).ravel()\n",
    "        # If classes are strings (dtype=object or str), map them via normalize->global map\n",
    "        # If classes are numeric, use them directly but still rebase to 0..C-1 afterwards\n",
    "        # Build label ints per trial\n",
    "        label_ints = []\n",
    "        for lab in classes_arr:\n",
    "            # if string-like, map via global map\n",
    "            if isinstance(lab, (str, bytes)) or (isinstance(lab, np.ndarray) and lab.dtype.type is np.str_):\n",
    "                li = map_label_to_int(lab)\n",
    "            else:\n",
    "                # numeric-like (e.g., uint8)\n",
    "                try:\n",
    "                    li = int(lab)\n",
    "                except Exception:\n",
    "                    li = map_label_to_int(lab)\n",
    "            label_ints.append(li)\n",
    "        label_ints = np.array(label_ints, dtype=int)\n",
    "        # If numeric labels look non-zero-indexed and small, we will rebase later globally.\n",
    "        n_trials = len(label_ints)\n",
    "        if n_trials <= 0:\n",
    "            return None, None\n",
    "        # split X sequentially into n_trials segments\n",
    "        if n_samples % n_trials == 0:\n",
    "            samples_per_trial = n_samples // n_trials\n",
    "        else:\n",
    "            samples_per_trial = int(np.floor(n_samples / n_trials))\n",
    "        trials_list = []\n",
    "        labels_list = []\n",
    "        for i in range(n_trials):\n",
    "            st = i * samples_per_trial\n",
    "            ed = st + samples_per_trial\n",
    "            if ed > n_samples: break\n",
    "            seg = X[st:ed, :].T.astype(np.float32)  # transpose -> (n_ch, n_times_file)\n",
    "            trials_list.append(seg)\n",
    "            labels_list.append(label_ints[i])\n",
    "        if len(trials_list) == 0:\n",
    "            return None, None\n",
    "        epochs = np.stack(trials_list, axis=0)  # (n_trials, n_ch, n_times_file)\n",
    "        labels = np.array(labels_list, dtype=int)\n",
    "\n",
    "    # Now epochs shape (n_trials, n_ch, n_times_file)\n",
    "    n_trials, n_ch_file, n_times_file = epochs.shape\n",
    "    # determine file sampling freq\n",
    "    file_fs = float(file_fs) if file_fs is not None else target_sfreq\n",
    "    # resample if needed (in time axis)\n",
    "    if n_times_file != target_n_times:\n",
    "        epochs_res = np.zeros((n_trials, n_ch_file, target_n_times), dtype=np.float32)\n",
    "        for ti in range(n_trials):\n",
    "            for ch in range(n_ch_file):\n",
    "                epochs_res[ti, ch, :] = resample(epochs[ti, ch, :], target_n_times)\n",
    "        epochs = epochs_res\n",
    "\n",
    "    return epochs, labels\n",
    "\n",
    "# ---------- infer baseline epoch params ----------\n",
    "if not os.path.exists(BASELINE_PREPRO):\n",
    "    raise FileNotFoundError(f\"Baseline preprocessed file '{BASELINE_PREPRO}' not found. Save baseline first.\")\n",
    "\n",
    "d = np.load(BASELINE_PREPRO, allow_pickle=True)\n",
    "X0 = d['X']; y0 = d['y'].astype(int)\n",
    "meta0 = {}\n",
    "if 'meta' in d:\n",
    "    meta_raw = d['meta']\n",
    "    try:\n",
    "        meta0 = meta_raw.item() if meta_raw.shape==() else dict(meta_raw)\n",
    "    except Exception:\n",
    "        meta0 = {}\n",
    "target_n_ch = X0.shape[1]\n",
    "target_n_times = X0.shape[2]\n",
    "target_sfreq = float(meta0.get('sfreq', 250.0))\n",
    "print(\"Baseline epoch shape:\", X0.shape, \"target_sfreq:\", target_sfreq)\n",
    "\n",
    "# ---------- iterate BNCI .mat files ----------\n",
    "mat_files = sorted(glob.glob(os.path.join(BNCI_ROOT, \"*.mat\")))\n",
    "extracted_epochs = []\n",
    "extracted_labels = []\n",
    "file_info = []\n",
    "for mf in mat_files:\n",
    "    try:\n",
    "        mat = loadmat(mf, squeeze_me=True, struct_as_record=False)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load\", mf, \":\", e); continue\n",
    "    data_arr = mat.get('data', None)\n",
    "    if data_arr is None:\n",
    "        print(\"No data entry in\", mf); continue\n",
    "    for elem in data_arr.flat:\n",
    "        epochs, labels = extract_epochs_from_element(elem, target_n_times, target_sfreq)\n",
    "        if epochs is None or labels is None:\n",
    "            continue\n",
    "        extracted_epochs.append(epochs)\n",
    "        extracted_labels.append(labels)\n",
    "        file_info.append((mf, epochs.shape[0], epochs.shape[1]))\n",
    "\n",
    "# concatenate\n",
    "if len(extracted_epochs) == 0:\n",
    "    print(\"No epochs extracted. file_info:\", file_info)\n",
    "else:\n",
    "    X_bnci = np.concatenate(extracted_epochs, axis=0)   # (N_epochs_bnci, n_ch_file, target_n_times)\n",
    "    y_bnci = np.concatenate(extracted_labels, axis=0)\n",
    "    print(\"Extracted BNCI epochs shape:\", X_bnci.shape, \"labels shape:\", y_bnci.shape)\n",
    "    # print and save the global mapping used for textual labels\n",
    "    print(\"Global label map (text->int):\", global_label_map)\n",
    "    # Save BNCI-only preprocessed\n",
    "    np.savez_compressed(OUT_BNCI, X=X_bnci.astype(np.float32), y=y_bnci.astype(int), meta={'sfreq': target_sfreq})\n",
    "    print(\"Saved BNCI preprocessed to\", OUT_BNCI)\n",
    "    # channel mismatch info\n",
    "    print(\"Baseline channels:\", target_n_ch, \"BNCI file channels:\", X_bnci.shape[1])\n",
    "    if X_bnci.shape[1] != target_n_ch:\n",
    "        print(\"Channel count mismatch detected. To merge safely we should align channel names or pick intersection.\")\n",
    "    else:\n",
    "        print(\"Channel counts match; you can concatenate directly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5308cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline shape: (30, 64, 561) BNCI shape: (640, 25, 561)\n",
      "Baseline meta keys: [] BNCI meta keys: ['sfreq']\n",
      "Discovered BNCI ch names: True Baseline ch names in meta: False\n",
      "Example BNCI names (first 20): ['X', 'trial', 'y', 'fs', 'classes', 'artifacts', 'gender', 'age']\n",
      "\n",
      "No automatic channel-name alignment possible. Options:\n",
      "\n",
      "OPTION 1 (RECOMMENDED): Do experiments separately:\n",
      " - run benchmarks on BNCI-only (preprocessed_BNCI.npz) and baseline-only (preprocessed.npz).\n",
      "   BNCI has 640 epochs (good for deep models); baseline is tiny (30 epochs) - you'll usually train on BNCI and evaluate there.\n",
      "   I will provide a BNCI-only benchmark cell next if you want.\n",
      "\n",
      "OPTION 2 (QUICK-HACK, NOT RECOMMENDED): Truncate to min channels and merge anyway.\n",
      "  This throws away channels from the larger dataset and may hurt performance. Use only to prototype quickly.\n",
      "\n",
      "To create quick-hack merged file (uncomment and run the small block below), it will keep the first min_ch channels from each dataset:\n",
      "  quick-hack min_ch = 25 (baseline has 64, BNCI has 25)\n",
      "\n",
      "Quick-hack merge code (copy-paste to run if you accept the tradeoff):\n",
      "\n",
      "\n",
      "# QUICK-HACK MERGE - USE IF YOU ACCEPT CHANNEL TRUNCATION\n",
      "import numpy as np\n",
      "d0 = np.load(\"preprocessed.npz\", allow_pickle=True); d1 = np.load(\"preprocessed_BNCI.npz\", allow_pickle=True)\n",
      "X0 = d0['X']; y0 = d0['y'].astype(int); X1 = d1['X']; y1 = d1['y'].astype(int)\n",
      "min_ch = 25\n",
      "X0_trunc = X0[:, :min_ch, :]; X1_trunc = X1[:, :min_ch, :]\n",
      "X_comb = np.concatenate([X0_trunc, X1_trunc], axis=0)\n",
      "y_comb = np.concatenate([y0 - y0.min(), y1], axis=0)\n",
      "np.savez_compressed(\"preprocessed_combined_quickhack.npz\", X=X_comb.astype(np.float32), y=y_comb.astype(int), meta={'sfreq': 250.0})\n",
      "print(\"Saved quick-hack merged to preprocessed_combined_quickhack.npz with shape\", X_comb.shape)\n",
      "\n",
      "\n",
      "Tell me which option you choose:\n",
      " - 'separate'  -> I will give BNCI-only benchmark cell next (recommended)\n",
      " - 'quickhack' -> I will run the quick-hack merge code for you (you accept channel truncation)\n",
      " - OR if you have baseline channel names available, paste them here and I will create a proper channel-alignment merge.\n"
     ]
    }
   ],
   "source": [
    "# ===== Channel alignment + merge helper =====\n",
    "# 1) Tries to extract channel name lists (BNCI .mat and baseline meta)\n",
    "# 2) If both exist: align by intersection and save preprocessed_combined.npz\n",
    "# 3) If not: prints options and offers quick-hack truncation (unrecommended)\n",
    "\n",
    "import os, glob, numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "BASE_PREPRO = \"preprocessed.npz\"\n",
    "BNCI_PREPRO = \"preprocessed_BNCI.npz\"\n",
    "OUT_COMBINED = \"preprocessed_combined.npz\"\n",
    "OUT_QUICK = \"preprocessed_combined_quickhack.npz\"\n",
    "BNCI_ROOT = r\"C:\\Users\\roger\\Desktop\\vs_code\\brain_intent_decoding\\BNCI_folder\"  # update if different\n",
    "\n",
    "# ---------- load preprocessed files ----------\n",
    "if not os.path.exists(BASE_PREPRO) or not os.path.exists(BNCI_PREPRO):\n",
    "    raise FileNotFoundError(\"Both baseline and BNCI preprocessed files must exist in cwd.\")\n",
    "\n",
    "d0 = np.load(BASE_PREPRO, allow_pickle=True)\n",
    "X0 = d0['X']; y0 = d0['y'].astype(int); meta0 = {}\n",
    "if 'meta' in d0:\n",
    "    mr = d0['meta']; meta0 = mr.item() if hasattr(mr, 'shape') and mr.shape==() else dict(mr)\n",
    "\n",
    "d1 = np.load(BNCI_PREPRO, allow_pickle=True)\n",
    "X1 = d1['X']; y1 = d1['y'].astype(int); meta1 = {}\n",
    "if 'meta' in d1:\n",
    "    mr = d1['meta']; meta1 = mr.item() if hasattr(mr, 'shape') and mr.shape==() else dict(mr)\n",
    "\n",
    "print(\"Baseline shape:\", X0.shape, \"BNCI shape:\", X1.shape)\n",
    "print(\"Baseline meta keys:\", list(meta0.keys()), \"BNCI meta keys:\", list(meta1.keys()))\n",
    "\n",
    "# ---------- attempt to discover BNCI channel names from .mat files ----------\n",
    "def try_get_bnci_ch_names(mat_folder):\n",
    "    names = None\n",
    "    files = sorted(glob.glob(os.path.join(mat_folder, \"*.mat\")))\n",
    "    for f in files:\n",
    "        try:\n",
    "            m = loadmat(f, squeeze_me=True, struct_as_record=False)\n",
    "            if 'data' in m:\n",
    "                data = m['data'].flat[0]  # first element likely representative\n",
    "                # common fields that might contain channel names: 'clab','chan','labels','label','_fieldnames','clab'\n",
    "                for cand in ('clab','chan','chanlocs','labels','label','_fieldnames','ch_names'):\n",
    "                    val = getattr(data, cand, None) if hasattr(data, cand) else (m['data'].flat[0].__dict__.get(cand) if hasattr(m['data'].flat[0], '__dict__') and cand in m['data'].flat[0].__dict__ else None)\n",
    "                    if val is not None:\n",
    "                        try:\n",
    "                            arr = np.array(val)\n",
    "                            # only accept plausible channel lists (len between 10 and 128)\n",
    "                            if arr.size > 5 and arr.size < 200:\n",
    "                                names = [str(x).strip() for x in arr.ravel().tolist()]\n",
    "                                return names\n",
    "                        except Exception:\n",
    "                            pass\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "bnci_ch_names = try_get_bnci_ch_names(BNCI_ROOT)\n",
    "baseline_ch_names = None\n",
    "# try common meta keys\n",
    "for k in ('ch_names','channels','channel_names','chan_names'):\n",
    "    if k in meta0:\n",
    "        baseline_ch_names = list(meta0[k])\n",
    "        break\n",
    "\n",
    "print(\"Discovered BNCI ch names:\", bool(bnci_ch_names), \"Baseline ch names in meta:\", bool(baseline_ch_names))\n",
    "if bnci_ch_names:\n",
    "    print(\"Example BNCI names (first 20):\", bnci_ch_names[:20])\n",
    "if baseline_ch_names:\n",
    "    print(\"Example baseline names (first 20):\", baseline_ch_names[:20])\n",
    "\n",
    "# ---------- If both have names, align by intersection ----------\n",
    "if bnci_ch_names and baseline_ch_names:\n",
    "    set0 = [n.lower() for n in baseline_ch_names]\n",
    "    set1 = [n.lower() for n in bnci_ch_names]\n",
    "    common = [c for c in baseline_ch_names if c.lower() in set1]\n",
    "    if len(common) == 0:\n",
    "        print(\"No overlapping channel names found between datasets despite names existing.\")\n",
    "    else:\n",
    "        print(\"Found\", len(common), \"common channel names. Aligning and saving merged dataset.\")\n",
    "        # reorder baseline and bnci to common ordering\n",
    "        idx0 = [i for i,n in enumerate(baseline_ch_names) if n in common]\n",
    "        idx1 = [bnci_ch_names.index(n) for n in common]\n",
    "        X0_al = X0[:, idx0, :]; X1_al = X1[:, idx1, :]\n",
    "        y0_adj = y0 - y0.min()\n",
    "        # concat\n",
    "        X_comb = np.concatenate([X0_al, X1_al], axis=0)\n",
    "        y_comb = np.concatenate([y0_adj, y1], axis=0)\n",
    "        np.savez_compressed(OUT_COMBINED, X=X_comb.astype(np.float32), y=y_comb.astype(int), meta={'sfreq': float(meta0.get('sfreq', meta1.get('sfreq',250.0))), 'ch_names': common})\n",
    "        print(\"Saved aligned combined dataset to\", OUT_COMBINED, \"with shape\", X_comb.shape)\n",
    "        raise SystemExit(\"Done - merged with aligned channel names. Use OUT_COMBINED for experiments.\")\n",
    "\n",
    "# ---------- If we reach here: no usable channel-name alignment found ----------\n",
    "print(\"\\nNo automatic channel-name alignment possible. Options:\")\n",
    "\n",
    "print(\"\\nOPTION 1 (RECOMMENDED): Do experiments separately:\\n - run benchmarks on BNCI-only (preprocessed_BNCI.npz) and baseline-only (preprocessed.npz).\\n   BNCI has 640 epochs (good for deep models); baseline is tiny (30 epochs) - you'll usually train on BNCI and evaluate there.\\n   I will provide a BNCI-only benchmark cell next if you want.\\n\")\n",
    "\n",
    "print(\"OPTION 2 (QUICK-HACK, NOT RECOMMENDED): Truncate to min channels and merge anyway.\")\n",
    "print(\"  This throws away channels from the larger dataset and may hurt performance. Use only to prototype quickly.\\n\")\n",
    "print(\"To create quick-hack merged file (uncomment and run the small block below), it will keep the first min_ch channels from each dataset:\")\n",
    "\n",
    "min_ch = min(X0.shape[1], X1.shape[1])\n",
    "print(f\"  quick-hack min_ch = {min_ch} (baseline has {X0.shape[1]}, BNCI has {X1.shape[1]})\")\n",
    "print(\"\\nQuick-hack merge code (copy-paste to run if you accept the tradeoff):\\n\")\n",
    "print(f\"\"\"\n",
    "# QUICK-HACK MERGE - USE IF YOU ACCEPT CHANNEL TRUNCATION\n",
    "import numpy as np\n",
    "d0 = np.load(\"{BASE_PREPRO}\", allow_pickle=True); d1 = np.load(\"{BNCI_PREPRO}\", allow_pickle=True)\n",
    "X0 = d0['X']; y0 = d0['y'].astype(int); X1 = d1['X']; y1 = d1['y'].astype(int)\n",
    "min_ch = {min_ch}\n",
    "X0_trunc = X0[:, :min_ch, :]; X1_trunc = X1[:, :min_ch, :]\n",
    "X_comb = np.concatenate([X0_trunc, X1_trunc], axis=0)\n",
    "y_comb = np.concatenate([y0 - y0.min(), y1], axis=0)\n",
    "np.savez_compressed(\"{OUT_QUICK}\", X=X_comb.astype(np.float32), y=y_comb.astype(int), meta={{'sfreq': {float(meta0.get('sfreq', meta1.get('sfreq',250.0)))}}})\n",
    "print(\"Saved quick-hack merged to {OUT_QUICK} with shape\", X_comb.shape)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nTell me which option you choose:\\n - 'separate'  -> I will give BNCI-only benchmark cell next (recommended)\\n - 'quickhack' -> I will run the quick-hack merge code for you (you accept channel truncation)\\n - OR if you have baseline channel names available, paste them here and I will create a proper channel-alignment merge.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f18025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
