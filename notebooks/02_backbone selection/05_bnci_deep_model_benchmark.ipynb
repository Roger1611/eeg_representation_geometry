{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import random\n",
    "\n",
    "# ---- Project Root ----\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[2]\n",
    "\n",
    "DATASETS_DIR = PROJECT_ROOT / \"datasets\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "\n",
    "FIGURES_DIR = RESULTS_DIR / \"figures\"\n",
    "TABLES_DIR = RESULTS_DIR / \"tables\" / \"backbone_benchmarking\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\" / \"backbone_benchmark_models\"\n",
    "\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BNCI_PATH = DATASETS_DIR / \"bnci_dataset\" / \"processed\" / \"preprocessed_BNCI.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6a9ab91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BNCI: (640, 25, 561)\n",
      "Label distribution: {np.int64(0): np.int64(160), np.int64(1): np.int64(160), np.int64(2): np.int64(160), np.int64(3): np.int64(160)}\n"
     ]
    }
   ],
   "source": [
    "assert BNCI_PATH.exists(), f\"BNCI file not found: {BNCI_PATH}\"\n",
    "\n",
    "d = np.load(BNCI_PATH, allow_pickle=True)\n",
    "X = d[\"X\"].astype(np.float32)\n",
    "y = d[\"y\"].astype(int)\n",
    "\n",
    "print(\"Loaded BNCI:\", X.shape)\n",
    "print(\"Label distribution:\", dict(zip(*np.unique(y, return_counts=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62193273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNCI_Dataset(Dataset):\n",
    "    def __init__(self, X, y, augment=False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "766d7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        chans: int,\n",
    "        samples: int,\n",
    "        classes: int,\n",
    "        F1: int = 8,\n",
    "        D: int = 2,\n",
    "        F2: int = 16,\n",
    "        kern_len: int = 64,\n",
    "        dropout: float = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.first = nn.Sequential(\n",
    "            nn.Conv2d(1, F1, (1, kern_len), padding=(0, kern_len // 2), bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "\n",
    "            nn.Conv2d(F1, F1 * D, (chans, 1), bias=False),\n",
    "            nn.BatchNorm2d(F1 * D),\n",
    "            nn.ELU(),\n",
    "\n",
    "            nn.AvgPool2d((1, 4)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.second = nn.Sequential(\n",
    "            nn.Conv2d(F1 * D, F2, (1, 16), bias=False),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8))\n",
    "        )\n",
    "\n",
    "        # Dynamically compute flatten size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, chans, samples)\n",
    "            h = self.first(dummy)\n",
    "            h = self.second(h)\n",
    "            flatten_dim = h.view(1, -1).shape[1]\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flatten_dim, classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)              # (B, 1, C, T)\n",
    "        x = self.first(x)\n",
    "        x = self.second(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "377cbcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowConvNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        chans: int,\n",
    "        samples: int,\n",
    "        classes: int,\n",
    "        F: int = 40,\n",
    "        kern_len: int = 25,\n",
    "        dropout: float = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.temporal = nn.Conv2d(\n",
    "            1, F,\n",
    "            (1, kern_len),\n",
    "            padding=(0, kern_len // 2),\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.spatial = nn.Conv2d(\n",
    "            F, F,\n",
    "            (chans, 1),\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.pool = nn.Sequential(\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 75)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Dynamic flatten size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, chans, samples)\n",
    "            h = self.temporal(dummy)\n",
    "            h = self.spatial(h)\n",
    "            h = self.pool(h)\n",
    "            flatten_dim = h.view(1, -1).shape[1]\n",
    "\n",
    "        self.classifier = nn.Linear(flatten_dim, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.temporal(x)\n",
    "        x = self.spatial(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.flatten(1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0000d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        chans: int,\n",
    "        samples: int,\n",
    "        classes: int,\n",
    "        dropout: float = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 25, (1, 5), padding=(0, 2), bias=False),\n",
    "            nn.Conv2d(25, 25, (chans, 1), bias=False),\n",
    "            nn.BatchNorm2d(25),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(25, 50, (1, 5), padding=(0, 2), bias=False),\n",
    "            nn.BatchNorm2d(50),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(50, 100, (1, 5), padding=(0, 2), bias=False),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(100, 200, (1, 5), padding=(0, 2), bias=False),\n",
    "            nn.BatchNorm2d(200),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Dynamic flatten\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, chans, samples)\n",
    "            h = self.block1(dummy)\n",
    "            h = self.block2(h)\n",
    "            h = self.block3(h)\n",
    "            h = self.block4(h)\n",
    "            flatten_dim = h.view(1, -1).shape[1]\n",
    "\n",
    "        self.classifier = nn.Linear(flatten_dim, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = x.flatten(1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cdbee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_kfold(model_class, model_name, X, y):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"\\n===== Training {model_name} on {device} =====\")\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    model_save_dir = MODELS_DIR / model_name\n",
    "    model_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (tr_idx, te_idx) in enumerate(kf.split(X, y), 1):\n",
    "\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "        Xtr, Xte = X[tr_idx], X[te_idx]\n",
    "        ytr, yte = y[tr_idx], y[te_idx]\n",
    "\n",
    "        ds_tr = BNCI_Dataset(Xtr, ytr)\n",
    "        ds_te = BNCI_Dataset(Xte, yte)\n",
    "\n",
    "        loader_tr = DataLoader(ds_tr, batch_size=32, shuffle=True)\n",
    "        loader_te = DataLoader(ds_te, batch_size=64, shuffle=False)\n",
    "\n",
    "        chans, samples = X.shape[1], X.shape[2]\n",
    "        num_classes = len(np.unique(y))\n",
    "\n",
    "        model = model_class(chans, samples, classes=num_classes).to(device)\n",
    "\n",
    "        opt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        scheduler = ReduceLROnPlateau(opt, mode='max', factor=0.5, patience=3)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        best_acc = 0.0\n",
    "        best_state = None\n",
    "        patience = 8\n",
    "        stale = 0\n",
    "\n",
    "        for ep in range(1, 51):  \n",
    "\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "\n",
    "            for xb, yb in loader_tr:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = loss_fn(logits, yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss /= len(loader_tr)\n",
    "\n",
    "            model.eval()\n",
    "            ys, preds = [], []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in loader_te:\n",
    "                    xb = xb.to(device)\n",
    "                    logits = model(xb)\n",
    "\n",
    "                    preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "                    ys.extend(yb.numpy())\n",
    "\n",
    "            acc = accuracy_score(ys, preds)\n",
    "            f1 = f1_score(ys, preds, average='weighted')\n",
    "\n",
    "            scheduler.step(acc)\n",
    "\n",
    "            if acc > best_acc + 1e-4:\n",
    "                best_acc = acc\n",
    "                best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "                stale = 0\n",
    "            else:\n",
    "                stale += 1\n",
    "\n",
    "            print(\n",
    "                f\"[{model_name} | Fold {fold} | Epoch {ep:02d}] \"\n",
    "                f\"train_loss={train_loss:.4f} \"\n",
    "                f\"val_acc={acc:.4f} \"\n",
    "                f\"val_f1={f1:.4f} \"\n",
    "                f\"(best={best_acc:.4f})\"\n",
    "            )\n",
    "\n",
    "            if stale >= patience:\n",
    "                print(f\"Early stopping at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "        torch.save(\n",
    "            best_state,\n",
    "            model_save_dir / f\"{model_name}_fold{fold}_best.pth\"\n",
    "        )\n",
    "\n",
    "        print(f\"Fold {fold} Best Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "        fold_results.append(best_acc)\n",
    "\n",
    "    print(f\"\\n===== {model_name} Mean Accuracy: {np.mean(fold_results):.4f} =====\")\n",
    "\n",
    "    return fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46323856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training EEGNet on cuda =====\n",
      "\n",
      "--- Fold 1 ---\n",
      "[EEGNet | Fold 1 | Epoch 01] train_loss=1.3843 val_acc=0.2578 val_f1=0.2135 (best=0.2578)\n",
      "[EEGNet | Fold 1 | Epoch 02] train_loss=1.3284 val_acc=0.2969 val_f1=0.2531 (best=0.2969)\n",
      "[EEGNet | Fold 1 | Epoch 03] train_loss=1.3051 val_acc=0.3438 val_f1=0.3426 (best=0.3438)\n",
      "[EEGNet | Fold 1 | Epoch 04] train_loss=1.2734 val_acc=0.3203 val_f1=0.2981 (best=0.3438)\n",
      "[EEGNet | Fold 1 | Epoch 05] train_loss=1.2643 val_acc=0.3438 val_f1=0.3154 (best=0.3438)\n",
      "[EEGNet | Fold 1 | Epoch 06] train_loss=1.2304 val_acc=0.4141 val_f1=0.4115 (best=0.4141)\n",
      "[EEGNet | Fold 1 | Epoch 07] train_loss=1.2245 val_acc=0.4219 val_f1=0.4167 (best=0.4219)\n",
      "[EEGNet | Fold 1 | Epoch 08] train_loss=1.1838 val_acc=0.4141 val_f1=0.4114 (best=0.4219)\n",
      "[EEGNet | Fold 1 | Epoch 09] train_loss=1.1565 val_acc=0.4141 val_f1=0.4107 (best=0.4219)\n",
      "[EEGNet | Fold 1 | Epoch 10] train_loss=1.1223 val_acc=0.4141 val_f1=0.4078 (best=0.4219)\n",
      "[EEGNet | Fold 1 | Epoch 11] train_loss=1.1109 val_acc=0.4531 val_f1=0.4480 (best=0.4531)\n",
      "[EEGNet | Fold 1 | Epoch 12] train_loss=1.0727 val_acc=0.4375 val_f1=0.4347 (best=0.4531)\n",
      "[EEGNet | Fold 1 | Epoch 13] train_loss=1.0320 val_acc=0.4844 val_f1=0.4734 (best=0.4844)\n",
      "[EEGNet | Fold 1 | Epoch 14] train_loss=0.9933 val_acc=0.5156 val_f1=0.5099 (best=0.5156)\n",
      "[EEGNet | Fold 1 | Epoch 15] train_loss=0.9539 val_acc=0.5312 val_f1=0.5291 (best=0.5312)\n",
      "[EEGNet | Fold 1 | Epoch 16] train_loss=0.9347 val_acc=0.6016 val_f1=0.6015 (best=0.6016)\n",
      "[EEGNet | Fold 1 | Epoch 17] train_loss=0.8686 val_acc=0.5703 val_f1=0.5708 (best=0.6016)\n",
      "[EEGNet | Fold 1 | Epoch 18] train_loss=0.8285 val_acc=0.5938 val_f1=0.5932 (best=0.6016)\n",
      "[EEGNet | Fold 1 | Epoch 19] train_loss=0.8053 val_acc=0.6250 val_f1=0.6248 (best=0.6250)\n",
      "[EEGNet | Fold 1 | Epoch 20] train_loss=0.7579 val_acc=0.6328 val_f1=0.6331 (best=0.6328)\n",
      "[EEGNet | Fold 1 | Epoch 21] train_loss=0.7479 val_acc=0.6328 val_f1=0.6329 (best=0.6328)\n",
      "[EEGNet | Fold 1 | Epoch 22] train_loss=0.6728 val_acc=0.6719 val_f1=0.6727 (best=0.6719)\n",
      "[EEGNet | Fold 1 | Epoch 23] train_loss=0.6519 val_acc=0.6562 val_f1=0.6550 (best=0.6719)\n",
      "[EEGNet | Fold 1 | Epoch 24] train_loss=0.6084 val_acc=0.6797 val_f1=0.6799 (best=0.6797)\n",
      "[EEGNet | Fold 1 | Epoch 25] train_loss=0.5757 val_acc=0.6875 val_f1=0.6873 (best=0.6875)\n",
      "[EEGNet | Fold 1 | Epoch 26] train_loss=0.5521 val_acc=0.6797 val_f1=0.6789 (best=0.6875)\n",
      "[EEGNet | Fold 1 | Epoch 27] train_loss=0.5368 val_acc=0.6875 val_f1=0.6869 (best=0.6875)\n",
      "[EEGNet | Fold 1 | Epoch 28] train_loss=0.4985 val_acc=0.7031 val_f1=0.7032 (best=0.7031)\n",
      "[EEGNet | Fold 1 | Epoch 29] train_loss=0.4750 val_acc=0.7266 val_f1=0.7261 (best=0.7266)\n",
      "[EEGNet | Fold 1 | Epoch 30] train_loss=0.4632 val_acc=0.7109 val_f1=0.7109 (best=0.7266)\n",
      "[EEGNet | Fold 1 | Epoch 31] train_loss=0.4192 val_acc=0.6797 val_f1=0.6793 (best=0.7266)\n",
      "[EEGNet | Fold 1 | Epoch 32] train_loss=0.3994 val_acc=0.6719 val_f1=0.6721 (best=0.7266)\n",
      "[EEGNet | Fold 1 | Epoch 33] train_loss=0.4018 val_acc=0.6797 val_f1=0.6784 (best=0.7266)\n",
      "[EEGNet | Fold 1 | Epoch 34] train_loss=0.3814 val_acc=0.6719 val_f1=0.6713 (best=0.7266)\n",
      "[EEGNet | Fold 1 | Epoch 35] train_loss=0.3504 val_acc=0.6797 val_f1=0.6788 (best=0.7266)\n",
      "[EEGNet | Fold 1 | Epoch 36] train_loss=0.3528 val_acc=0.6875 val_f1=0.6864 (best=0.7266)\n",
      "[EEGNet | Fold 1 | Epoch 37] train_loss=0.3442 val_acc=0.7031 val_f1=0.7025 (best=0.7266)\n",
      "Early stopping at epoch 37\n",
      "Fold 1 Best Accuracy: 0.7266\n",
      "\n",
      "--- Fold 2 ---\n",
      "[EEGNet | Fold 2 | Epoch 01] train_loss=1.4052 val_acc=0.3047 val_f1=0.2557 (best=0.3047)\n",
      "[EEGNet | Fold 2 | Epoch 02] train_loss=1.3523 val_acc=0.3750 val_f1=0.3364 (best=0.3750)\n",
      "[EEGNet | Fold 2 | Epoch 03] train_loss=1.3233 val_acc=0.3906 val_f1=0.3521 (best=0.3906)\n",
      "[EEGNet | Fold 2 | Epoch 04] train_loss=1.2907 val_acc=0.3594 val_f1=0.3163 (best=0.3906)\n",
      "[EEGNet | Fold 2 | Epoch 05] train_loss=1.2645 val_acc=0.3828 val_f1=0.3689 (best=0.3906)\n",
      "[EEGNet | Fold 2 | Epoch 06] train_loss=1.2464 val_acc=0.3906 val_f1=0.3761 (best=0.3906)\n",
      "[EEGNet | Fold 2 | Epoch 07] train_loss=1.2082 val_acc=0.4609 val_f1=0.4571 (best=0.4609)\n",
      "[EEGNet | Fold 2 | Epoch 08] train_loss=1.1762 val_acc=0.3984 val_f1=0.3890 (best=0.4609)\n",
      "[EEGNet | Fold 2 | Epoch 09] train_loss=1.1530 val_acc=0.4297 val_f1=0.4133 (best=0.4609)\n",
      "[EEGNet | Fold 2 | Epoch 10] train_loss=1.1344 val_acc=0.4297 val_f1=0.4155 (best=0.4609)\n",
      "[EEGNet | Fold 2 | Epoch 11] train_loss=1.1061 val_acc=0.4922 val_f1=0.4906 (best=0.4922)\n",
      "[EEGNet | Fold 2 | Epoch 12] train_loss=1.0923 val_acc=0.5000 val_f1=0.4981 (best=0.5000)\n",
      "[EEGNet | Fold 2 | Epoch 13] train_loss=1.0455 val_acc=0.5078 val_f1=0.5064 (best=0.5078)\n",
      "[EEGNet | Fold 2 | Epoch 14] train_loss=1.0135 val_acc=0.5312 val_f1=0.5294 (best=0.5312)\n",
      "[EEGNet | Fold 2 | Epoch 15] train_loss=0.9717 val_acc=0.5938 val_f1=0.5900 (best=0.5938)\n",
      "[EEGNet | Fold 2 | Epoch 16] train_loss=0.9299 val_acc=0.5859 val_f1=0.5848 (best=0.5938)\n",
      "[EEGNet | Fold 2 | Epoch 17] train_loss=0.8864 val_acc=0.6641 val_f1=0.6645 (best=0.6641)\n",
      "[EEGNet | Fold 2 | Epoch 18] train_loss=0.8678 val_acc=0.6719 val_f1=0.6740 (best=0.6719)\n",
      "[EEGNet | Fold 2 | Epoch 19] train_loss=0.8301 val_acc=0.6719 val_f1=0.6734 (best=0.6719)\n",
      "[EEGNet | Fold 2 | Epoch 20] train_loss=0.7687 val_acc=0.6953 val_f1=0.6953 (best=0.6953)\n",
      "[EEGNet | Fold 2 | Epoch 21] train_loss=0.7438 val_acc=0.6797 val_f1=0.6815 (best=0.6953)\n",
      "[EEGNet | Fold 2 | Epoch 22] train_loss=0.7320 val_acc=0.6953 val_f1=0.6970 (best=0.6953)\n",
      "[EEGNet | Fold 2 | Epoch 23] train_loss=0.7017 val_acc=0.6953 val_f1=0.6963 (best=0.6953)\n",
      "[EEGNet | Fold 2 | Epoch 24] train_loss=0.6590 val_acc=0.7422 val_f1=0.7420 (best=0.7422)\n",
      "[EEGNet | Fold 2 | Epoch 25] train_loss=0.6154 val_acc=0.7266 val_f1=0.7272 (best=0.7422)\n",
      "[EEGNet | Fold 2 | Epoch 26] train_loss=0.5730 val_acc=0.7344 val_f1=0.7349 (best=0.7422)\n",
      "[EEGNet | Fold 2 | Epoch 27] train_loss=0.5573 val_acc=0.7188 val_f1=0.7191 (best=0.7422)\n",
      "[EEGNet | Fold 2 | Epoch 28] train_loss=0.5156 val_acc=0.7578 val_f1=0.7568 (best=0.7578)\n",
      "[EEGNet | Fold 2 | Epoch 29] train_loss=0.5055 val_acc=0.7578 val_f1=0.7571 (best=0.7578)\n",
      "[EEGNet | Fold 2 | Epoch 30] train_loss=0.4848 val_acc=0.7188 val_f1=0.7190 (best=0.7578)\n",
      "[EEGNet | Fold 2 | Epoch 31] train_loss=0.4800 val_acc=0.7266 val_f1=0.7273 (best=0.7578)\n",
      "[EEGNet | Fold 2 | Epoch 32] train_loss=0.4377 val_acc=0.7422 val_f1=0.7427 (best=0.7578)\n",
      "[EEGNet | Fold 2 | Epoch 33] train_loss=0.3879 val_acc=0.7344 val_f1=0.7346 (best=0.7578)\n",
      "[EEGNet | Fold 2 | Epoch 34] train_loss=0.3894 val_acc=0.7422 val_f1=0.7428 (best=0.7578)\n",
      "[EEGNet | Fold 2 | Epoch 35] train_loss=0.3780 val_acc=0.7578 val_f1=0.7578 (best=0.7578)\n",
      "[EEGNet | Fold 2 | Epoch 36] train_loss=0.3671 val_acc=0.7500 val_f1=0.7506 (best=0.7578)\n",
      "Early stopping at epoch 36\n",
      "Fold 2 Best Accuracy: 0.7578\n",
      "\n",
      "--- Fold 3 ---\n",
      "[EEGNet | Fold 3 | Epoch 01] train_loss=1.3944 val_acc=0.2500 val_f1=0.2326 (best=0.2500)\n",
      "[EEGNet | Fold 3 | Epoch 02] train_loss=1.3390 val_acc=0.2344 val_f1=0.2191 (best=0.2500)\n",
      "[EEGNet | Fold 3 | Epoch 03] train_loss=1.3090 val_acc=0.2656 val_f1=0.2477 (best=0.2656)\n",
      "[EEGNet | Fold 3 | Epoch 04] train_loss=1.2777 val_acc=0.2578 val_f1=0.2432 (best=0.2656)\n",
      "[EEGNet | Fold 3 | Epoch 05] train_loss=1.2410 val_acc=0.2812 val_f1=0.2697 (best=0.2812)\n",
      "[EEGNet | Fold 3 | Epoch 06] train_loss=1.2319 val_acc=0.2578 val_f1=0.2482 (best=0.2812)\n",
      "[EEGNet | Fold 3 | Epoch 07] train_loss=1.2226 val_acc=0.3281 val_f1=0.3157 (best=0.3281)\n",
      "[EEGNet | Fold 3 | Epoch 08] train_loss=1.1864 val_acc=0.3516 val_f1=0.3469 (best=0.3516)\n",
      "[EEGNet | Fold 3 | Epoch 09] train_loss=1.1537 val_acc=0.3672 val_f1=0.3611 (best=0.3672)\n",
      "[EEGNet | Fold 3 | Epoch 10] train_loss=1.1330 val_acc=0.3906 val_f1=0.3893 (best=0.3906)\n",
      "[EEGNet | Fold 3 | Epoch 11] train_loss=1.0945 val_acc=0.4297 val_f1=0.4281 (best=0.4297)\n",
      "[EEGNet | Fold 3 | Epoch 12] train_loss=1.0715 val_acc=0.4375 val_f1=0.4366 (best=0.4375)\n",
      "[EEGNet | Fold 3 | Epoch 13] train_loss=1.0135 val_acc=0.4375 val_f1=0.4383 (best=0.4375)\n",
      "[EEGNet | Fold 3 | Epoch 14] train_loss=0.9817 val_acc=0.4922 val_f1=0.4948 (best=0.4922)\n",
      "[EEGNet | Fold 3 | Epoch 15] train_loss=0.9461 val_acc=0.5781 val_f1=0.5781 (best=0.5781)\n",
      "[EEGNet | Fold 3 | Epoch 16] train_loss=0.8919 val_acc=0.5859 val_f1=0.5853 (best=0.5859)\n",
      "[EEGNet | Fold 3 | Epoch 17] train_loss=0.8584 val_acc=0.6094 val_f1=0.6079 (best=0.6094)\n",
      "[EEGNet | Fold 3 | Epoch 18] train_loss=0.8349 val_acc=0.6250 val_f1=0.6231 (best=0.6250)\n",
      "[EEGNet | Fold 3 | Epoch 19] train_loss=0.7900 val_acc=0.6406 val_f1=0.6389 (best=0.6406)\n",
      "[EEGNet | Fold 3 | Epoch 20] train_loss=0.7546 val_acc=0.6562 val_f1=0.6562 (best=0.6562)\n",
      "[EEGNet | Fold 3 | Epoch 21] train_loss=0.7267 val_acc=0.6328 val_f1=0.6327 (best=0.6562)\n",
      "[EEGNet | Fold 3 | Epoch 22] train_loss=0.6902 val_acc=0.6641 val_f1=0.6631 (best=0.6641)\n",
      "[EEGNet | Fold 3 | Epoch 23] train_loss=0.6714 val_acc=0.6797 val_f1=0.6778 (best=0.6797)\n",
      "[EEGNet | Fold 3 | Epoch 24] train_loss=0.6248 val_acc=0.6562 val_f1=0.6553 (best=0.6797)\n",
      "[EEGNet | Fold 3 | Epoch 25] train_loss=0.5962 val_acc=0.6953 val_f1=0.6952 (best=0.6953)\n",
      "[EEGNet | Fold 3 | Epoch 26] train_loss=0.5626 val_acc=0.6953 val_f1=0.6955 (best=0.6953)\n",
      "[EEGNet | Fold 3 | Epoch 27] train_loss=0.5452 val_acc=0.7422 val_f1=0.7410 (best=0.7422)\n",
      "[EEGNet | Fold 3 | Epoch 28] train_loss=0.5070 val_acc=0.6953 val_f1=0.6925 (best=0.7422)\n",
      "[EEGNet | Fold 3 | Epoch 29] train_loss=0.4551 val_acc=0.7344 val_f1=0.7323 (best=0.7422)\n",
      "[EEGNet | Fold 3 | Epoch 30] train_loss=0.4494 val_acc=0.7188 val_f1=0.7193 (best=0.7422)\n",
      "[EEGNet | Fold 3 | Epoch 31] train_loss=0.4273 val_acc=0.7500 val_f1=0.7487 (best=0.7500)\n",
      "[EEGNet | Fold 3 | Epoch 32] train_loss=0.4225 val_acc=0.7578 val_f1=0.7556 (best=0.7578)\n",
      "[EEGNet | Fold 3 | Epoch 33] train_loss=0.4093 val_acc=0.7422 val_f1=0.7423 (best=0.7578)\n",
      "[EEGNet | Fold 3 | Epoch 34] train_loss=0.3919 val_acc=0.7344 val_f1=0.7331 (best=0.7578)\n",
      "[EEGNet | Fold 3 | Epoch 35] train_loss=0.3544 val_acc=0.7500 val_f1=0.7486 (best=0.7578)\n",
      "[EEGNet | Fold 3 | Epoch 36] train_loss=0.3613 val_acc=0.7344 val_f1=0.7352 (best=0.7578)\n",
      "[EEGNet | Fold 3 | Epoch 37] train_loss=0.3381 val_acc=0.7500 val_f1=0.7515 (best=0.7578)\n",
      "[EEGNet | Fold 3 | Epoch 38] train_loss=0.2944 val_acc=0.7500 val_f1=0.7509 (best=0.7578)\n",
      "[EEGNet | Fold 3 | Epoch 39] train_loss=0.2850 val_acc=0.7578 val_f1=0.7570 (best=0.7578)\n",
      "[EEGNet | Fold 3 | Epoch 40] train_loss=0.2791 val_acc=0.7578 val_f1=0.7563 (best=0.7578)\n",
      "Early stopping at epoch 40\n",
      "Fold 3 Best Accuracy: 0.7578\n",
      "\n",
      "--- Fold 4 ---\n",
      "[EEGNet | Fold 4 | Epoch 01] train_loss=1.3904 val_acc=0.2656 val_f1=0.2059 (best=0.2656)\n",
      "[EEGNet | Fold 4 | Epoch 02] train_loss=1.3470 val_acc=0.2734 val_f1=0.2311 (best=0.2734)\n",
      "[EEGNet | Fold 4 | Epoch 03] train_loss=1.3109 val_acc=0.2500 val_f1=0.2211 (best=0.2734)\n",
      "[EEGNet | Fold 4 | Epoch 04] train_loss=1.2743 val_acc=0.2734 val_f1=0.2431 (best=0.2734)\n",
      "[EEGNet | Fold 4 | Epoch 05] train_loss=1.2569 val_acc=0.2578 val_f1=0.2454 (best=0.2734)\n",
      "[EEGNet | Fold 4 | Epoch 06] train_loss=1.2418 val_acc=0.3047 val_f1=0.2966 (best=0.3047)\n",
      "[EEGNet | Fold 4 | Epoch 07] train_loss=1.2125 val_acc=0.3047 val_f1=0.2941 (best=0.3047)\n",
      "[EEGNet | Fold 4 | Epoch 08] train_loss=1.1973 val_acc=0.3359 val_f1=0.3321 (best=0.3359)\n",
      "[EEGNet | Fold 4 | Epoch 09] train_loss=1.1644 val_acc=0.3750 val_f1=0.3617 (best=0.3750)\n",
      "[EEGNet | Fold 4 | Epoch 10] train_loss=1.1465 val_acc=0.4297 val_f1=0.4283 (best=0.4297)\n",
      "[EEGNet | Fold 4 | Epoch 11] train_loss=1.1161 val_acc=0.4297 val_f1=0.4279 (best=0.4297)\n",
      "[EEGNet | Fold 4 | Epoch 12] train_loss=1.0845 val_acc=0.4922 val_f1=0.4921 (best=0.4922)\n",
      "[EEGNet | Fold 4 | Epoch 13] train_loss=1.0464 val_acc=0.4922 val_f1=0.4850 (best=0.4922)\n",
      "[EEGNet | Fold 4 | Epoch 14] train_loss=1.0039 val_acc=0.5703 val_f1=0.5677 (best=0.5703)\n",
      "[EEGNet | Fold 4 | Epoch 15] train_loss=0.9765 val_acc=0.6094 val_f1=0.6109 (best=0.6094)\n",
      "[EEGNet | Fold 4 | Epoch 16] train_loss=0.9315 val_acc=0.6172 val_f1=0.6172 (best=0.6172)\n",
      "[EEGNet | Fold 4 | Epoch 17] train_loss=0.8859 val_acc=0.6797 val_f1=0.6796 (best=0.6797)\n",
      "[EEGNet | Fold 4 | Epoch 18] train_loss=0.8151 val_acc=0.6719 val_f1=0.6740 (best=0.6797)\n",
      "[EEGNet | Fold 4 | Epoch 19] train_loss=0.7940 val_acc=0.6641 val_f1=0.6651 (best=0.6797)\n",
      "[EEGNet | Fold 4 | Epoch 20] train_loss=0.7532 val_acc=0.7031 val_f1=0.7049 (best=0.7031)\n",
      "[EEGNet | Fold 4 | Epoch 21] train_loss=0.7011 val_acc=0.7188 val_f1=0.7224 (best=0.7188)\n",
      "[EEGNet | Fold 4 | Epoch 22] train_loss=0.6799 val_acc=0.7188 val_f1=0.7198 (best=0.7188)\n",
      "[EEGNet | Fold 4 | Epoch 23] train_loss=0.6153 val_acc=0.7188 val_f1=0.7206 (best=0.7188)\n",
      "[EEGNet | Fold 4 | Epoch 24] train_loss=0.5842 val_acc=0.7578 val_f1=0.7582 (best=0.7578)\n",
      "[EEGNet | Fold 4 | Epoch 25] train_loss=0.5939 val_acc=0.7422 val_f1=0.7444 (best=0.7578)\n",
      "[EEGNet | Fold 4 | Epoch 26] train_loss=0.5428 val_acc=0.7422 val_f1=0.7429 (best=0.7578)\n",
      "[EEGNet | Fold 4 | Epoch 27] train_loss=0.5088 val_acc=0.7422 val_f1=0.7434 (best=0.7578)\n",
      "[EEGNet | Fold 4 | Epoch 28] train_loss=0.4979 val_acc=0.7656 val_f1=0.7647 (best=0.7656)\n",
      "[EEGNet | Fold 4 | Epoch 29] train_loss=0.4597 val_acc=0.7656 val_f1=0.7657 (best=0.7656)\n",
      "[EEGNet | Fold 4 | Epoch 30] train_loss=0.4611 val_acc=0.7734 val_f1=0.7735 (best=0.7734)\n",
      "[EEGNet | Fold 4 | Epoch 31] train_loss=0.4172 val_acc=0.7891 val_f1=0.7882 (best=0.7891)\n",
      "[EEGNet | Fold 4 | Epoch 32] train_loss=0.3841 val_acc=0.7578 val_f1=0.7605 (best=0.7891)\n",
      "[EEGNet | Fold 4 | Epoch 33] train_loss=0.3625 val_acc=0.7734 val_f1=0.7743 (best=0.7891)\n",
      "[EEGNet | Fold 4 | Epoch 34] train_loss=0.3173 val_acc=0.7656 val_f1=0.7660 (best=0.7891)\n",
      "[EEGNet | Fold 4 | Epoch 35] train_loss=0.3445 val_acc=0.7891 val_f1=0.7893 (best=0.7891)\n",
      "[EEGNet | Fold 4 | Epoch 36] train_loss=0.3115 val_acc=0.7891 val_f1=0.7883 (best=0.7891)\n",
      "[EEGNet | Fold 4 | Epoch 37] train_loss=0.2821 val_acc=0.7969 val_f1=0.7962 (best=0.7969)\n",
      "[EEGNet | Fold 4 | Epoch 38] train_loss=0.2896 val_acc=0.8047 val_f1=0.8045 (best=0.8047)\n",
      "[EEGNet | Fold 4 | Epoch 39] train_loss=0.2630 val_acc=0.7969 val_f1=0.7964 (best=0.8047)\n",
      "[EEGNet | Fold 4 | Epoch 40] train_loss=0.2856 val_acc=0.7812 val_f1=0.7812 (best=0.8047)\n",
      "[EEGNet | Fold 4 | Epoch 41] train_loss=0.2545 val_acc=0.7891 val_f1=0.7883 (best=0.8047)\n",
      "[EEGNet | Fold 4 | Epoch 42] train_loss=0.2384 val_acc=0.7891 val_f1=0.7884 (best=0.8047)\n",
      "[EEGNet | Fold 4 | Epoch 43] train_loss=0.2405 val_acc=0.7969 val_f1=0.7964 (best=0.8047)\n",
      "[EEGNet | Fold 4 | Epoch 44] train_loss=0.2447 val_acc=0.7891 val_f1=0.7885 (best=0.8047)\n",
      "[EEGNet | Fold 4 | Epoch 45] train_loss=0.2328 val_acc=0.7812 val_f1=0.7809 (best=0.8047)\n",
      "[EEGNet | Fold 4 | Epoch 46] train_loss=0.2133 val_acc=0.7969 val_f1=0.7968 (best=0.8047)\n",
      "Early stopping at epoch 46\n",
      "Fold 4 Best Accuracy: 0.8047\n",
      "\n",
      "--- Fold 5 ---\n",
      "[EEGNet | Fold 5 | Epoch 01] train_loss=1.3899 val_acc=0.2969 val_f1=0.2460 (best=0.2969)\n",
      "[EEGNet | Fold 5 | Epoch 02] train_loss=1.3544 val_acc=0.3516 val_f1=0.3316 (best=0.3516)\n",
      "[EEGNet | Fold 5 | Epoch 03] train_loss=1.3264 val_acc=0.3750 val_f1=0.3601 (best=0.3750)\n",
      "[EEGNet | Fold 5 | Epoch 04] train_loss=1.2929 val_acc=0.4141 val_f1=0.3920 (best=0.4141)\n",
      "[EEGNet | Fold 5 | Epoch 05] train_loss=1.2722 val_acc=0.3750 val_f1=0.3511 (best=0.4141)\n",
      "[EEGNet | Fold 5 | Epoch 06] train_loss=1.2580 val_acc=0.3672 val_f1=0.3363 (best=0.4141)\n",
      "[EEGNet | Fold 5 | Epoch 07] train_loss=1.2211 val_acc=0.3906 val_f1=0.3778 (best=0.4141)\n",
      "[EEGNet | Fold 5 | Epoch 08] train_loss=1.1914 val_acc=0.4453 val_f1=0.4258 (best=0.4453)\n",
      "[EEGNet | Fold 5 | Epoch 09] train_loss=1.1624 val_acc=0.4766 val_f1=0.4719 (best=0.4766)\n",
      "[EEGNet | Fold 5 | Epoch 10] train_loss=1.1363 val_acc=0.4922 val_f1=0.4845 (best=0.4922)\n",
      "[EEGNet | Fold 5 | Epoch 11] train_loss=1.1124 val_acc=0.5234 val_f1=0.5169 (best=0.5234)\n",
      "[EEGNet | Fold 5 | Epoch 12] train_loss=1.0885 val_acc=0.5938 val_f1=0.5895 (best=0.5938)\n",
      "[EEGNet | Fold 5 | Epoch 13] train_loss=1.0468 val_acc=0.5312 val_f1=0.5253 (best=0.5938)\n",
      "[EEGNet | Fold 5 | Epoch 14] train_loss=1.0206 val_acc=0.5703 val_f1=0.5662 (best=0.5938)\n",
      "[EEGNet | Fold 5 | Epoch 15] train_loss=0.9733 val_acc=0.6172 val_f1=0.6148 (best=0.6172)\n",
      "[EEGNet | Fold 5 | Epoch 16] train_loss=0.9449 val_acc=0.5938 val_f1=0.5892 (best=0.6172)\n",
      "[EEGNet | Fold 5 | Epoch 17] train_loss=0.9111 val_acc=0.6094 val_f1=0.6025 (best=0.6172)\n",
      "[EEGNet | Fold 5 | Epoch 18] train_loss=0.8472 val_acc=0.6328 val_f1=0.6289 (best=0.6328)\n",
      "[EEGNet | Fold 5 | Epoch 19] train_loss=0.8136 val_acc=0.6719 val_f1=0.6689 (best=0.6719)\n",
      "[EEGNet | Fold 5 | Epoch 20] train_loss=0.7815 val_acc=0.6641 val_f1=0.6626 (best=0.6719)\n",
      "[EEGNet | Fold 5 | Epoch 21] train_loss=0.7501 val_acc=0.6719 val_f1=0.6703 (best=0.6719)\n",
      "[EEGNet | Fold 5 | Epoch 22] train_loss=0.7170 val_acc=0.6797 val_f1=0.6794 (best=0.6797)\n",
      "[EEGNet | Fold 5 | Epoch 23] train_loss=0.6953 val_acc=0.6797 val_f1=0.6784 (best=0.6797)\n",
      "[EEGNet | Fold 5 | Epoch 24] train_loss=0.6664 val_acc=0.7188 val_f1=0.7194 (best=0.7188)\n",
      "[EEGNet | Fold 5 | Epoch 25] train_loss=0.6025 val_acc=0.6953 val_f1=0.6962 (best=0.7188)\n",
      "[EEGNet | Fold 5 | Epoch 26] train_loss=0.5823 val_acc=0.7109 val_f1=0.7094 (best=0.7188)\n",
      "[EEGNet | Fold 5 | Epoch 27] train_loss=0.5811 val_acc=0.7188 val_f1=0.7194 (best=0.7188)\n",
      "[EEGNet | Fold 5 | Epoch 28] train_loss=0.5382 val_acc=0.7109 val_f1=0.7094 (best=0.7188)\n",
      "[EEGNet | Fold 5 | Epoch 29] train_loss=0.4915 val_acc=0.7109 val_f1=0.7100 (best=0.7188)\n",
      "[EEGNet | Fold 5 | Epoch 30] train_loss=0.4934 val_acc=0.7266 val_f1=0.7272 (best=0.7266)\n",
      "[EEGNet | Fold 5 | Epoch 31] train_loss=0.4719 val_acc=0.7188 val_f1=0.7184 (best=0.7266)\n",
      "[EEGNet | Fold 5 | Epoch 32] train_loss=0.4671 val_acc=0.7188 val_f1=0.7189 (best=0.7266)\n",
      "[EEGNet | Fold 5 | Epoch 33] train_loss=0.4319 val_acc=0.7500 val_f1=0.7506 (best=0.7500)\n",
      "[EEGNet | Fold 5 | Epoch 34] train_loss=0.4416 val_acc=0.7266 val_f1=0.7264 (best=0.7500)\n",
      "[EEGNet | Fold 5 | Epoch 35] train_loss=0.4374 val_acc=0.7266 val_f1=0.7258 (best=0.7500)\n",
      "[EEGNet | Fold 5 | Epoch 36] train_loss=0.4073 val_acc=0.7109 val_f1=0.7104 (best=0.7500)\n",
      "[EEGNet | Fold 5 | Epoch 37] train_loss=0.3994 val_acc=0.7266 val_f1=0.7265 (best=0.7500)\n",
      "[EEGNet | Fold 5 | Epoch 38] train_loss=0.3985 val_acc=0.7188 val_f1=0.7184 (best=0.7500)\n",
      "[EEGNet | Fold 5 | Epoch 39] train_loss=0.3955 val_acc=0.7344 val_f1=0.7346 (best=0.7500)\n",
      "[EEGNet | Fold 5 | Epoch 40] train_loss=0.3739 val_acc=0.7266 val_f1=0.7266 (best=0.7500)\n",
      "[EEGNet | Fold 5 | Epoch 41] train_loss=0.3689 val_acc=0.7344 val_f1=0.7341 (best=0.7500)\n",
      "Early stopping at epoch 41\n",
      "Fold 5 Best Accuracy: 0.7500\n",
      "\n",
      "===== EEGNet Mean Accuracy: 0.7594 =====\n",
      "\n",
      "===== Training ShallowConvNet on cuda =====\n",
      "\n",
      "--- Fold 1 ---\n",
      "[ShallowConvNet | Fold 1 | Epoch 01] train_loss=1.8136 val_acc=0.2891 val_f1=0.2269 (best=0.2891)\n",
      "[ShallowConvNet | Fold 1 | Epoch 02] train_loss=1.5821 val_acc=0.2734 val_f1=0.1956 (best=0.2891)\n",
      "[ShallowConvNet | Fold 1 | Epoch 03] train_loss=1.5034 val_acc=0.2969 val_f1=0.2769 (best=0.2969)\n",
      "[ShallowConvNet | Fold 1 | Epoch 04] train_loss=1.3623 val_acc=0.3203 val_f1=0.2959 (best=0.3203)\n",
      "[ShallowConvNet | Fold 1 | Epoch 05] train_loss=1.4207 val_acc=0.3438 val_f1=0.3282 (best=0.3438)\n",
      "[ShallowConvNet | Fold 1 | Epoch 06] train_loss=1.3638 val_acc=0.3750 val_f1=0.3395 (best=0.3750)\n",
      "[ShallowConvNet | Fold 1 | Epoch 07] train_loss=1.3583 val_acc=0.3438 val_f1=0.2966 (best=0.3750)\n",
      "[ShallowConvNet | Fold 1 | Epoch 08] train_loss=1.3104 val_acc=0.3516 val_f1=0.3216 (best=0.3750)\n",
      "[ShallowConvNet | Fold 1 | Epoch 09] train_loss=1.3137 val_acc=0.2656 val_f1=0.2413 (best=0.3750)\n",
      "[ShallowConvNet | Fold 1 | Epoch 10] train_loss=1.2703 val_acc=0.3281 val_f1=0.2599 (best=0.3750)\n",
      "[ShallowConvNet | Fold 1 | Epoch 11] train_loss=1.2603 val_acc=0.3516 val_f1=0.3443 (best=0.3750)\n",
      "[ShallowConvNet | Fold 1 | Epoch 12] train_loss=1.2242 val_acc=0.4219 val_f1=0.4119 (best=0.4219)\n",
      "[ShallowConvNet | Fold 1 | Epoch 13] train_loss=1.2501 val_acc=0.3906 val_f1=0.3842 (best=0.4219)\n",
      "[ShallowConvNet | Fold 1 | Epoch 14] train_loss=1.2235 val_acc=0.3750 val_f1=0.3659 (best=0.4219)\n",
      "[ShallowConvNet | Fold 1 | Epoch 15] train_loss=1.2082 val_acc=0.3594 val_f1=0.3561 (best=0.4219)\n",
      "[ShallowConvNet | Fold 1 | Epoch 16] train_loss=1.2061 val_acc=0.3359 val_f1=0.3235 (best=0.4219)\n",
      "[ShallowConvNet | Fold 1 | Epoch 17] train_loss=1.2087 val_acc=0.3594 val_f1=0.3575 (best=0.4219)\n",
      "[ShallowConvNet | Fold 1 | Epoch 18] train_loss=1.1802 val_acc=0.3516 val_f1=0.3451 (best=0.4219)\n",
      "[ShallowConvNet | Fold 1 | Epoch 19] train_loss=1.1492 val_acc=0.3672 val_f1=0.3624 (best=0.4219)\n",
      "[ShallowConvNet | Fold 1 | Epoch 20] train_loss=1.2010 val_acc=0.3438 val_f1=0.3437 (best=0.4219)\n",
      "Early stopping at epoch 20\n",
      "Fold 1 Best Accuracy: 0.4219\n",
      "\n",
      "--- Fold 2 ---\n",
      "[ShallowConvNet | Fold 2 | Epoch 01] train_loss=1.8068 val_acc=0.2344 val_f1=0.2004 (best=0.2344)\n",
      "[ShallowConvNet | Fold 2 | Epoch 02] train_loss=1.5961 val_acc=0.2734 val_f1=0.2428 (best=0.2734)\n",
      "[ShallowConvNet | Fold 2 | Epoch 03] train_loss=1.5016 val_acc=0.3125 val_f1=0.2709 (best=0.3125)\n",
      "[ShallowConvNet | Fold 2 | Epoch 04] train_loss=1.3813 val_acc=0.3672 val_f1=0.3423 (best=0.3672)\n",
      "[ShallowConvNet | Fold 2 | Epoch 05] train_loss=1.3424 val_acc=0.2891 val_f1=0.2725 (best=0.3672)\n",
      "[ShallowConvNet | Fold 2 | Epoch 06] train_loss=1.3051 val_acc=0.3594 val_f1=0.3274 (best=0.3672)\n",
      "[ShallowConvNet | Fold 2 | Epoch 07] train_loss=1.3086 val_acc=0.3203 val_f1=0.2948 (best=0.3672)\n",
      "[ShallowConvNet | Fold 2 | Epoch 08] train_loss=1.3075 val_acc=0.3594 val_f1=0.3191 (best=0.3672)\n",
      "[ShallowConvNet | Fold 2 | Epoch 09] train_loss=1.2813 val_acc=0.3438 val_f1=0.3478 (best=0.3672)\n",
      "[ShallowConvNet | Fold 2 | Epoch 10] train_loss=1.2638 val_acc=0.3750 val_f1=0.3750 (best=0.3750)\n",
      "[ShallowConvNet | Fold 2 | Epoch 11] train_loss=1.2264 val_acc=0.3281 val_f1=0.2876 (best=0.3750)\n",
      "[ShallowConvNet | Fold 2 | Epoch 12] train_loss=1.2290 val_acc=0.3438 val_f1=0.3306 (best=0.3750)\n",
      "[ShallowConvNet | Fold 2 | Epoch 13] train_loss=1.2286 val_acc=0.3359 val_f1=0.3379 (best=0.3750)\n",
      "[ShallowConvNet | Fold 2 | Epoch 14] train_loss=1.1775 val_acc=0.3438 val_f1=0.3326 (best=0.3750)\n",
      "[ShallowConvNet | Fold 2 | Epoch 15] train_loss=1.1699 val_acc=0.3516 val_f1=0.3455 (best=0.3750)\n",
      "[ShallowConvNet | Fold 2 | Epoch 16] train_loss=1.1848 val_acc=0.3359 val_f1=0.3289 (best=0.3750)\n",
      "[ShallowConvNet | Fold 2 | Epoch 17] train_loss=1.1979 val_acc=0.3359 val_f1=0.3280 (best=0.3750)\n",
      "[ShallowConvNet | Fold 2 | Epoch 18] train_loss=1.1784 val_acc=0.3594 val_f1=0.3540 (best=0.3750)\n",
      "Early stopping at epoch 18\n",
      "Fold 2 Best Accuracy: 0.3750\n",
      "\n",
      "--- Fold 3 ---\n",
      "[ShallowConvNet | Fold 3 | Epoch 01] train_loss=1.8165 val_acc=0.2422 val_f1=0.1706 (best=0.2422)\n",
      "[ShallowConvNet | Fold 3 | Epoch 02] train_loss=1.6297 val_acc=0.2422 val_f1=0.2113 (best=0.2422)\n",
      "[ShallowConvNet | Fold 3 | Epoch 03] train_loss=1.4626 val_acc=0.3203 val_f1=0.2967 (best=0.3203)\n",
      "[ShallowConvNet | Fold 3 | Epoch 04] train_loss=1.4258 val_acc=0.2422 val_f1=0.2308 (best=0.3203)\n",
      "[ShallowConvNet | Fold 3 | Epoch 05] train_loss=1.3419 val_acc=0.2344 val_f1=0.2272 (best=0.3203)\n",
      "[ShallowConvNet | Fold 3 | Epoch 06] train_loss=1.3081 val_acc=0.2812 val_f1=0.2818 (best=0.3203)\n",
      "[ShallowConvNet | Fold 3 | Epoch 07] train_loss=1.3244 val_acc=0.2578 val_f1=0.2370 (best=0.3203)\n",
      "[ShallowConvNet | Fold 3 | Epoch 08] train_loss=1.2697 val_acc=0.2656 val_f1=0.2612 (best=0.3203)\n",
      "[ShallowConvNet | Fold 3 | Epoch 09] train_loss=1.2500 val_acc=0.2812 val_f1=0.2752 (best=0.3203)\n",
      "[ShallowConvNet | Fold 3 | Epoch 10] train_loss=1.2627 val_acc=0.3438 val_f1=0.3352 (best=0.3438)\n",
      "[ShallowConvNet | Fold 3 | Epoch 11] train_loss=1.2471 val_acc=0.2188 val_f1=0.1835 (best=0.3438)\n",
      "[ShallowConvNet | Fold 3 | Epoch 12] train_loss=1.2386 val_acc=0.2969 val_f1=0.2953 (best=0.3438)\n",
      "[ShallowConvNet | Fold 3 | Epoch 13] train_loss=1.2155 val_acc=0.2734 val_f1=0.2624 (best=0.3438)\n",
      "[ShallowConvNet | Fold 3 | Epoch 14] train_loss=1.2011 val_acc=0.2500 val_f1=0.2455 (best=0.3438)\n",
      "[ShallowConvNet | Fold 3 | Epoch 15] train_loss=1.1810 val_acc=0.3047 val_f1=0.3039 (best=0.3438)\n",
      "[ShallowConvNet | Fold 3 | Epoch 16] train_loss=1.1836 val_acc=0.2891 val_f1=0.2910 (best=0.3438)\n",
      "[ShallowConvNet | Fold 3 | Epoch 17] train_loss=1.1798 val_acc=0.2578 val_f1=0.2516 (best=0.3438)\n",
      "[ShallowConvNet | Fold 3 | Epoch 18] train_loss=1.1715 val_acc=0.2656 val_f1=0.2661 (best=0.3438)\n",
      "Early stopping at epoch 18\n",
      "Fold 3 Best Accuracy: 0.3438\n",
      "\n",
      "--- Fold 4 ---\n",
      "[ShallowConvNet | Fold 4 | Epoch 01] train_loss=1.7885 val_acc=0.2500 val_f1=0.2093 (best=0.2500)\n",
      "[ShallowConvNet | Fold 4 | Epoch 02] train_loss=1.5185 val_acc=0.3125 val_f1=0.3062 (best=0.3125)\n",
      "[ShallowConvNet | Fold 4 | Epoch 03] train_loss=1.4823 val_acc=0.2812 val_f1=0.2506 (best=0.3125)\n",
      "[ShallowConvNet | Fold 4 | Epoch 04] train_loss=1.3954 val_acc=0.3047 val_f1=0.2800 (best=0.3125)\n",
      "[ShallowConvNet | Fold 4 | Epoch 05] train_loss=1.3578 val_acc=0.3516 val_f1=0.3379 (best=0.3516)\n",
      "[ShallowConvNet | Fold 4 | Epoch 06] train_loss=1.3892 val_acc=0.3125 val_f1=0.2914 (best=0.3516)\n",
      "[ShallowConvNet | Fold 4 | Epoch 07] train_loss=1.2871 val_acc=0.2656 val_f1=0.2586 (best=0.3516)\n",
      "[ShallowConvNet | Fold 4 | Epoch 08] train_loss=1.3054 val_acc=0.3281 val_f1=0.3182 (best=0.3516)\n",
      "[ShallowConvNet | Fold 4 | Epoch 09] train_loss=1.2465 val_acc=0.2969 val_f1=0.2549 (best=0.3516)\n",
      "[ShallowConvNet | Fold 4 | Epoch 10] train_loss=1.2408 val_acc=0.3047 val_f1=0.2912 (best=0.3516)\n",
      "[ShallowConvNet | Fold 4 | Epoch 11] train_loss=1.2152 val_acc=0.3203 val_f1=0.3182 (best=0.3516)\n",
      "[ShallowConvNet | Fold 4 | Epoch 12] train_loss=1.2133 val_acc=0.3125 val_f1=0.3121 (best=0.3516)\n",
      "[ShallowConvNet | Fold 4 | Epoch 13] train_loss=1.2161 val_acc=0.2969 val_f1=0.2887 (best=0.3516)\n",
      "Early stopping at epoch 13\n",
      "Fold 4 Best Accuracy: 0.3516\n",
      "\n",
      "--- Fold 5 ---\n",
      "[ShallowConvNet | Fold 5 | Epoch 01] train_loss=1.8369 val_acc=0.2734 val_f1=0.2345 (best=0.2734)\n",
      "[ShallowConvNet | Fold 5 | Epoch 02] train_loss=1.5820 val_acc=0.2891 val_f1=0.2471 (best=0.2891)\n",
      "[ShallowConvNet | Fold 5 | Epoch 03] train_loss=1.4425 val_acc=0.2891 val_f1=0.2251 (best=0.2891)\n",
      "[ShallowConvNet | Fold 5 | Epoch 04] train_loss=1.4365 val_acc=0.3828 val_f1=0.3842 (best=0.3828)\n",
      "[ShallowConvNet | Fold 5 | Epoch 05] train_loss=1.4095 val_acc=0.2969 val_f1=0.2311 (best=0.3828)\n",
      "[ShallowConvNet | Fold 5 | Epoch 06] train_loss=1.3683 val_acc=0.3125 val_f1=0.2991 (best=0.3828)\n",
      "[ShallowConvNet | Fold 5 | Epoch 07] train_loss=1.3359 val_acc=0.3203 val_f1=0.2995 (best=0.3828)\n",
      "[ShallowConvNet | Fold 5 | Epoch 08] train_loss=1.3558 val_acc=0.2969 val_f1=0.2954 (best=0.3828)\n",
      "[ShallowConvNet | Fold 5 | Epoch 09] train_loss=1.2908 val_acc=0.2969 val_f1=0.2605 (best=0.3828)\n",
      "[ShallowConvNet | Fold 5 | Epoch 10] train_loss=1.2912 val_acc=0.3125 val_f1=0.3004 (best=0.3828)\n",
      "[ShallowConvNet | Fold 5 | Epoch 11] train_loss=1.2435 val_acc=0.3594 val_f1=0.3279 (best=0.3828)\n",
      "[ShallowConvNet | Fold 5 | Epoch 12] train_loss=1.2337 val_acc=0.3438 val_f1=0.3362 (best=0.3828)\n",
      "Early stopping at epoch 12\n",
      "Fold 5 Best Accuracy: 0.3828\n",
      "\n",
      "===== ShallowConvNet Mean Accuracy: 0.3750 =====\n",
      "\n",
      "===== Training DeepConvNet on cuda =====\n",
      "\n",
      "--- Fold 1 ---\n",
      "[DeepConvNet | Fold 1 | Epoch 01] train_loss=1.8585 val_acc=0.3125 val_f1=0.2356 (best=0.3125)\n",
      "[DeepConvNet | Fold 1 | Epoch 02] train_loss=1.4648 val_acc=0.3594 val_f1=0.3540 (best=0.3594)\n",
      "[DeepConvNet | Fold 1 | Epoch 03] train_loss=1.3229 val_acc=0.4297 val_f1=0.4069 (best=0.4297)\n",
      "[DeepConvNet | Fold 1 | Epoch 04] train_loss=1.2642 val_acc=0.3984 val_f1=0.3732 (best=0.4297)\n",
      "[DeepConvNet | Fold 1 | Epoch 05] train_loss=1.2056 val_acc=0.5000 val_f1=0.4074 (best=0.5000)\n",
      "[DeepConvNet | Fold 1 | Epoch 06] train_loss=1.1672 val_acc=0.3906 val_f1=0.3333 (best=0.5000)\n",
      "[DeepConvNet | Fold 1 | Epoch 07] train_loss=1.1500 val_acc=0.4844 val_f1=0.4554 (best=0.5000)\n",
      "[DeepConvNet | Fold 1 | Epoch 08] train_loss=1.0420 val_acc=0.4453 val_f1=0.4527 (best=0.5000)\n",
      "[DeepConvNet | Fold 1 | Epoch 09] train_loss=1.0498 val_acc=0.5469 val_f1=0.5387 (best=0.5469)\n",
      "[DeepConvNet | Fold 1 | Epoch 10] train_loss=1.0335 val_acc=0.5781 val_f1=0.5761 (best=0.5781)\n",
      "[DeepConvNet | Fold 1 | Epoch 11] train_loss=0.9751 val_acc=0.6016 val_f1=0.5914 (best=0.6016)\n",
      "[DeepConvNet | Fold 1 | Epoch 12] train_loss=1.0023 val_acc=0.5000 val_f1=0.4530 (best=0.6016)\n",
      "[DeepConvNet | Fold 1 | Epoch 13] train_loss=0.9435 val_acc=0.5703 val_f1=0.5314 (best=0.6016)\n",
      "[DeepConvNet | Fold 1 | Epoch 14] train_loss=0.8274 val_acc=0.5234 val_f1=0.4821 (best=0.6016)\n",
      "[DeepConvNet | Fold 1 | Epoch 15] train_loss=0.8473 val_acc=0.5781 val_f1=0.5505 (best=0.6016)\n",
      "[DeepConvNet | Fold 1 | Epoch 16] train_loss=0.7400 val_acc=0.5703 val_f1=0.5522 (best=0.6016)\n",
      "[DeepConvNet | Fold 1 | Epoch 17] train_loss=0.7999 val_acc=0.5938 val_f1=0.5800 (best=0.6016)\n",
      "[DeepConvNet | Fold 1 | Epoch 18] train_loss=0.7090 val_acc=0.6250 val_f1=0.6186 (best=0.6250)\n",
      "[DeepConvNet | Fold 1 | Epoch 19] train_loss=0.7501 val_acc=0.6094 val_f1=0.6055 (best=0.6250)\n",
      "[DeepConvNet | Fold 1 | Epoch 20] train_loss=0.7073 val_acc=0.5938 val_f1=0.5779 (best=0.6250)\n",
      "[DeepConvNet | Fold 1 | Epoch 21] train_loss=0.6999 val_acc=0.5938 val_f1=0.5762 (best=0.6250)\n",
      "[DeepConvNet | Fold 1 | Epoch 22] train_loss=0.6893 val_acc=0.6172 val_f1=0.6068 (best=0.6250)\n",
      "[DeepConvNet | Fold 1 | Epoch 23] train_loss=0.6488 val_acc=0.6094 val_f1=0.5940 (best=0.6250)\n",
      "[DeepConvNet | Fold 1 | Epoch 24] train_loss=0.5896 val_acc=0.6094 val_f1=0.5941 (best=0.6250)\n",
      "[DeepConvNet | Fold 1 | Epoch 25] train_loss=0.5833 val_acc=0.6172 val_f1=0.6071 (best=0.6250)\n",
      "[DeepConvNet | Fold 1 | Epoch 26] train_loss=0.6469 val_acc=0.6406 val_f1=0.6319 (best=0.6406)\n",
      "[DeepConvNet | Fold 1 | Epoch 27] train_loss=0.6174 val_acc=0.6406 val_f1=0.6330 (best=0.6406)\n",
      "[DeepConvNet | Fold 1 | Epoch 28] train_loss=0.6207 val_acc=0.6406 val_f1=0.6330 (best=0.6406)\n",
      "[DeepConvNet | Fold 1 | Epoch 29] train_loss=0.6296 val_acc=0.6172 val_f1=0.6079 (best=0.6406)\n",
      "[DeepConvNet | Fold 1 | Epoch 30] train_loss=0.5912 val_acc=0.6328 val_f1=0.6197 (best=0.6406)\n",
      "[DeepConvNet | Fold 1 | Epoch 31] train_loss=0.5944 val_acc=0.6250 val_f1=0.6136 (best=0.6406)\n",
      "[DeepConvNet | Fold 1 | Epoch 32] train_loss=0.5989 val_acc=0.6406 val_f1=0.6310 (best=0.6406)\n",
      "[DeepConvNet | Fold 1 | Epoch 33] train_loss=0.5872 val_acc=0.6562 val_f1=0.6475 (best=0.6562)\n",
      "[DeepConvNet | Fold 1 | Epoch 34] train_loss=0.5687 val_acc=0.6641 val_f1=0.6563 (best=0.6641)\n",
      "[DeepConvNet | Fold 1 | Epoch 35] train_loss=0.5929 val_acc=0.6484 val_f1=0.6401 (best=0.6641)\n",
      "[DeepConvNet | Fold 1 | Epoch 36] train_loss=0.5372 val_acc=0.6484 val_f1=0.6382 (best=0.6641)\n",
      "[DeepConvNet | Fold 1 | Epoch 37] train_loss=0.5956 val_acc=0.6328 val_f1=0.6274 (best=0.6641)\n",
      "[DeepConvNet | Fold 1 | Epoch 38] train_loss=0.5660 val_acc=0.6406 val_f1=0.6310 (best=0.6641)\n",
      "[DeepConvNet | Fold 1 | Epoch 39] train_loss=0.5414 val_acc=0.6406 val_f1=0.6310 (best=0.6641)\n",
      "[DeepConvNet | Fold 1 | Epoch 40] train_loss=0.5771 val_acc=0.6406 val_f1=0.6305 (best=0.6641)\n",
      "[DeepConvNet | Fold 1 | Epoch 41] train_loss=0.5551 val_acc=0.6484 val_f1=0.6376 (best=0.6641)\n",
      "[DeepConvNet | Fold 1 | Epoch 42] train_loss=0.5479 val_acc=0.6328 val_f1=0.6212 (best=0.6641)\n",
      "Early stopping at epoch 42\n",
      "Fold 1 Best Accuracy: 0.6641\n",
      "\n",
      "--- Fold 2 ---\n",
      "[DeepConvNet | Fold 2 | Epoch 01] train_loss=1.8777 val_acc=0.3906 val_f1=0.3599 (best=0.3906)\n",
      "[DeepConvNet | Fold 2 | Epoch 02] train_loss=1.5903 val_acc=0.4609 val_f1=0.4262 (best=0.4609)\n",
      "[DeepConvNet | Fold 2 | Epoch 03] train_loss=1.4203 val_acc=0.4688 val_f1=0.4102 (best=0.4688)\n",
      "[DeepConvNet | Fold 2 | Epoch 04] train_loss=1.3730 val_acc=0.3906 val_f1=0.3369 (best=0.4688)\n",
      "[DeepConvNet | Fold 2 | Epoch 05] train_loss=1.2035 val_acc=0.5000 val_f1=0.4732 (best=0.5000)\n",
      "[DeepConvNet | Fold 2 | Epoch 06] train_loss=1.1494 val_acc=0.5312 val_f1=0.5210 (best=0.5312)\n",
      "[DeepConvNet | Fold 2 | Epoch 07] train_loss=1.0356 val_acc=0.5703 val_f1=0.5445 (best=0.5703)\n",
      "[DeepConvNet | Fold 2 | Epoch 08] train_loss=1.0571 val_acc=0.5625 val_f1=0.5535 (best=0.5703)\n",
      "[DeepConvNet | Fold 2 | Epoch 09] train_loss=1.0168 val_acc=0.5000 val_f1=0.5038 (best=0.5703)\n",
      "[DeepConvNet | Fold 2 | Epoch 10] train_loss=0.9129 val_acc=0.5625 val_f1=0.5470 (best=0.5703)\n",
      "[DeepConvNet | Fold 2 | Epoch 11] train_loss=0.9235 val_acc=0.5156 val_f1=0.4894 (best=0.5703)\n",
      "[DeepConvNet | Fold 2 | Epoch 12] train_loss=0.8838 val_acc=0.6016 val_f1=0.5959 (best=0.6016)\n",
      "[DeepConvNet | Fold 2 | Epoch 13] train_loss=0.8483 val_acc=0.6328 val_f1=0.6261 (best=0.6328)\n",
      "[DeepConvNet | Fold 2 | Epoch 14] train_loss=0.8142 val_acc=0.6484 val_f1=0.6474 (best=0.6484)\n",
      "[DeepConvNet | Fold 2 | Epoch 15] train_loss=0.7595 val_acc=0.6016 val_f1=0.6006 (best=0.6484)\n",
      "[DeepConvNet | Fold 2 | Epoch 16] train_loss=0.8010 val_acc=0.6250 val_f1=0.6190 (best=0.6484)\n",
      "[DeepConvNet | Fold 2 | Epoch 17] train_loss=0.7416 val_acc=0.6094 val_f1=0.6080 (best=0.6484)\n",
      "[DeepConvNet | Fold 2 | Epoch 18] train_loss=0.7458 val_acc=0.6016 val_f1=0.6001 (best=0.6484)\n",
      "[DeepConvNet | Fold 2 | Epoch 19] train_loss=0.7560 val_acc=0.6016 val_f1=0.5981 (best=0.6484)\n",
      "[DeepConvNet | Fold 2 | Epoch 20] train_loss=0.6823 val_acc=0.6094 val_f1=0.6073 (best=0.6484)\n",
      "[DeepConvNet | Fold 2 | Epoch 21] train_loss=0.6944 val_acc=0.6250 val_f1=0.6221 (best=0.6484)\n",
      "[DeepConvNet | Fold 2 | Epoch 22] train_loss=0.6144 val_acc=0.6016 val_f1=0.5988 (best=0.6484)\n",
      "Early stopping at epoch 22\n",
      "Fold 2 Best Accuracy: 0.6484\n",
      "\n",
      "--- Fold 3 ---\n",
      "[DeepConvNet | Fold 3 | Epoch 01] train_loss=1.8503 val_acc=0.3750 val_f1=0.3032 (best=0.3750)\n",
      "[DeepConvNet | Fold 3 | Epoch 02] train_loss=1.5157 val_acc=0.3281 val_f1=0.3127 (best=0.3750)\n",
      "[DeepConvNet | Fold 3 | Epoch 03] train_loss=1.3149 val_acc=0.4922 val_f1=0.4579 (best=0.4922)\n",
      "[DeepConvNet | Fold 3 | Epoch 04] train_loss=1.2520 val_acc=0.3984 val_f1=0.3493 (best=0.4922)\n",
      "[DeepConvNet | Fold 3 | Epoch 05] train_loss=1.1977 val_acc=0.4141 val_f1=0.3814 (best=0.4922)\n",
      "[DeepConvNet | Fold 3 | Epoch 06] train_loss=1.1045 val_acc=0.4922 val_f1=0.4725 (best=0.4922)\n",
      "[DeepConvNet | Fold 3 | Epoch 07] train_loss=1.0754 val_acc=0.5469 val_f1=0.5312 (best=0.5469)\n",
      "[DeepConvNet | Fold 3 | Epoch 08] train_loss=1.0434 val_acc=0.5078 val_f1=0.4809 (best=0.5469)\n",
      "[DeepConvNet | Fold 3 | Epoch 09] train_loss=1.1214 val_acc=0.5156 val_f1=0.4978 (best=0.5469)\n",
      "[DeepConvNet | Fold 3 | Epoch 10] train_loss=1.0086 val_acc=0.5234 val_f1=0.5208 (best=0.5469)\n",
      "[DeepConvNet | Fold 3 | Epoch 11] train_loss=0.9009 val_acc=0.4844 val_f1=0.4760 (best=0.5469)\n",
      "[DeepConvNet | Fold 3 | Epoch 12] train_loss=0.8232 val_acc=0.5078 val_f1=0.5017 (best=0.5469)\n",
      "[DeepConvNet | Fold 3 | Epoch 13] train_loss=0.8364 val_acc=0.5625 val_f1=0.5589 (best=0.5625)\n",
      "[DeepConvNet | Fold 3 | Epoch 14] train_loss=0.8090 val_acc=0.5312 val_f1=0.5221 (best=0.5625)\n",
      "[DeepConvNet | Fold 3 | Epoch 15] train_loss=0.7766 val_acc=0.5469 val_f1=0.5317 (best=0.5625)\n",
      "[DeepConvNet | Fold 3 | Epoch 16] train_loss=0.7736 val_acc=0.5312 val_f1=0.5332 (best=0.5625)\n",
      "[DeepConvNet | Fold 3 | Epoch 17] train_loss=0.7468 val_acc=0.5391 val_f1=0.5383 (best=0.5625)\n",
      "[DeepConvNet | Fold 3 | Epoch 18] train_loss=0.7083 val_acc=0.5391 val_f1=0.5389 (best=0.5625)\n",
      "[DeepConvNet | Fold 3 | Epoch 19] train_loss=0.7126 val_acc=0.5391 val_f1=0.5375 (best=0.5625)\n",
      "[DeepConvNet | Fold 3 | Epoch 20] train_loss=0.6724 val_acc=0.5312 val_f1=0.5267 (best=0.5625)\n",
      "[DeepConvNet | Fold 3 | Epoch 21] train_loss=0.6806 val_acc=0.5312 val_f1=0.5338 (best=0.5625)\n",
      "Early stopping at epoch 21\n",
      "Fold 3 Best Accuracy: 0.5625\n",
      "\n",
      "--- Fold 4 ---\n",
      "[DeepConvNet | Fold 4 | Epoch 01] train_loss=1.8150 val_acc=0.3438 val_f1=0.2682 (best=0.3438)\n",
      "[DeepConvNet | Fold 4 | Epoch 02] train_loss=1.5417 val_acc=0.3984 val_f1=0.3898 (best=0.3984)\n",
      "[DeepConvNet | Fold 4 | Epoch 03] train_loss=1.3227 val_acc=0.4297 val_f1=0.3993 (best=0.4297)\n",
      "[DeepConvNet | Fold 4 | Epoch 04] train_loss=1.2598 val_acc=0.4766 val_f1=0.4660 (best=0.4766)\n",
      "[DeepConvNet | Fold 4 | Epoch 05] train_loss=1.1873 val_acc=0.3828 val_f1=0.3414 (best=0.4766)\n",
      "[DeepConvNet | Fold 4 | Epoch 06] train_loss=1.1115 val_acc=0.5469 val_f1=0.5373 (best=0.5469)\n",
      "[DeepConvNet | Fold 4 | Epoch 07] train_loss=0.9892 val_acc=0.5078 val_f1=0.4849 (best=0.5469)\n",
      "[DeepConvNet | Fold 4 | Epoch 08] train_loss=0.9896 val_acc=0.4844 val_f1=0.4424 (best=0.5469)\n",
      "[DeepConvNet | Fold 4 | Epoch 09] train_loss=0.9792 val_acc=0.5078 val_f1=0.4702 (best=0.5469)\n",
      "[DeepConvNet | Fold 4 | Epoch 10] train_loss=0.9860 val_acc=0.5078 val_f1=0.4804 (best=0.5469)\n",
      "[DeepConvNet | Fold 4 | Epoch 11] train_loss=0.9342 val_acc=0.5625 val_f1=0.5659 (best=0.5625)\n",
      "[DeepConvNet | Fold 4 | Epoch 12] train_loss=0.8082 val_acc=0.5469 val_f1=0.5511 (best=0.5625)\n",
      "[DeepConvNet | Fold 4 | Epoch 13] train_loss=0.7480 val_acc=0.5547 val_f1=0.5329 (best=0.5625)\n",
      "[DeepConvNet | Fold 4 | Epoch 14] train_loss=0.8209 val_acc=0.5703 val_f1=0.5786 (best=0.5703)\n",
      "[DeepConvNet | Fold 4 | Epoch 15] train_loss=0.7593 val_acc=0.5703 val_f1=0.5718 (best=0.5703)\n",
      "[DeepConvNet | Fold 4 | Epoch 16] train_loss=0.7668 val_acc=0.5391 val_f1=0.5358 (best=0.5703)\n",
      "[DeepConvNet | Fold 4 | Epoch 17] train_loss=0.7748 val_acc=0.5859 val_f1=0.5810 (best=0.5859)\n",
      "[DeepConvNet | Fold 4 | Epoch 18] train_loss=0.7334 val_acc=0.5625 val_f1=0.5600 (best=0.5859)\n",
      "[DeepConvNet | Fold 4 | Epoch 19] train_loss=0.7325 val_acc=0.5625 val_f1=0.5677 (best=0.5859)\n",
      "[DeepConvNet | Fold 4 | Epoch 20] train_loss=0.7525 val_acc=0.5703 val_f1=0.5667 (best=0.5859)\n",
      "[DeepConvNet | Fold 4 | Epoch 21] train_loss=0.6822 val_acc=0.5859 val_f1=0.5899 (best=0.5859)\n",
      "[DeepConvNet | Fold 4 | Epoch 22] train_loss=0.6932 val_acc=0.5703 val_f1=0.5670 (best=0.5859)\n",
      "[DeepConvNet | Fold 4 | Epoch 23] train_loss=0.6816 val_acc=0.5703 val_f1=0.5699 (best=0.5859)\n",
      "[DeepConvNet | Fold 4 | Epoch 24] train_loss=0.6803 val_acc=0.5703 val_f1=0.5694 (best=0.5859)\n",
      "[DeepConvNet | Fold 4 | Epoch 25] train_loss=0.6804 val_acc=0.5547 val_f1=0.5577 (best=0.5859)\n",
      "Early stopping at epoch 25\n",
      "Fold 4 Best Accuracy: 0.5859\n",
      "\n",
      "--- Fold 5 ---\n",
      "[DeepConvNet | Fold 5 | Epoch 01] train_loss=1.6531 val_acc=0.3750 val_f1=0.3040 (best=0.3750)\n",
      "[DeepConvNet | Fold 5 | Epoch 02] train_loss=1.6974 val_acc=0.3984 val_f1=0.3235 (best=0.3984)\n",
      "[DeepConvNet | Fold 5 | Epoch 03] train_loss=1.4236 val_acc=0.4141 val_f1=0.3905 (best=0.4141)\n",
      "[DeepConvNet | Fold 5 | Epoch 04] train_loss=1.4151 val_acc=0.4297 val_f1=0.3765 (best=0.4297)\n",
      "[DeepConvNet | Fold 5 | Epoch 05] train_loss=1.2274 val_acc=0.4453 val_f1=0.4083 (best=0.4453)\n",
      "[DeepConvNet | Fold 5 | Epoch 06] train_loss=1.2342 val_acc=0.4375 val_f1=0.4217 (best=0.4453)\n",
      "[DeepConvNet | Fold 5 | Epoch 07] train_loss=1.1971 val_acc=0.4922 val_f1=0.4680 (best=0.4922)\n",
      "[DeepConvNet | Fold 5 | Epoch 08] train_loss=1.0250 val_acc=0.4062 val_f1=0.3722 (best=0.4922)\n",
      "[DeepConvNet | Fold 5 | Epoch 09] train_loss=1.1135 val_acc=0.5156 val_f1=0.4983 (best=0.5156)\n",
      "[DeepConvNet | Fold 5 | Epoch 10] train_loss=0.9888 val_acc=0.4844 val_f1=0.4716 (best=0.5156)\n",
      "[DeepConvNet | Fold 5 | Epoch 11] train_loss=0.9868 val_acc=0.4844 val_f1=0.4676 (best=0.5156)\n",
      "[DeepConvNet | Fold 5 | Epoch 12] train_loss=0.8826 val_acc=0.5625 val_f1=0.5572 (best=0.5625)\n",
      "[DeepConvNet | Fold 5 | Epoch 13] train_loss=0.8173 val_acc=0.5391 val_f1=0.5274 (best=0.5625)\n",
      "[DeepConvNet | Fold 5 | Epoch 14] train_loss=0.8967 val_acc=0.5234 val_f1=0.5118 (best=0.5625)\n",
      "[DeepConvNet | Fold 5 | Epoch 15] train_loss=0.8423 val_acc=0.5938 val_f1=0.5724 (best=0.5938)\n",
      "[DeepConvNet | Fold 5 | Epoch 16] train_loss=0.7928 val_acc=0.4844 val_f1=0.4716 (best=0.5938)\n",
      "[DeepConvNet | Fold 5 | Epoch 17] train_loss=0.7064 val_acc=0.5000 val_f1=0.4843 (best=0.5938)\n",
      "[DeepConvNet | Fold 5 | Epoch 18] train_loss=0.7507 val_acc=0.5078 val_f1=0.4921 (best=0.5938)\n",
      "[DeepConvNet | Fold 5 | Epoch 19] train_loss=0.6828 val_acc=0.5938 val_f1=0.5888 (best=0.5938)\n",
      "[DeepConvNet | Fold 5 | Epoch 20] train_loss=0.6978 val_acc=0.5469 val_f1=0.5348 (best=0.5938)\n",
      "[DeepConvNet | Fold 5 | Epoch 21] train_loss=0.6540 val_acc=0.5156 val_f1=0.4869 (best=0.5938)\n",
      "[DeepConvNet | Fold 5 | Epoch 22] train_loss=0.6949 val_acc=0.5625 val_f1=0.5451 (best=0.5938)\n",
      "[DeepConvNet | Fold 5 | Epoch 23] train_loss=0.5883 val_acc=0.5625 val_f1=0.5533 (best=0.5938)\n",
      "Early stopping at epoch 23\n",
      "Fold 5 Best Accuracy: 0.5938\n",
      "\n",
      "===== DeepConvNet Mean Accuracy: 0.6109 =====\n",
      "            model  acc_mean   acc_std\n",
      "0          EEGNet  0.759375  0.025388\n",
      "2     DeepConvNet  0.610938  0.038717\n",
      "1  ShallowConvNet  0.375000  0.027511\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"EEGNet\": EEGNet,\n",
    "    \"ShallowConvNet\": ShallowConvNet,\n",
    "    \"DeepConvNet\": DeepConvNet\n",
    "}\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for name, model_class in models.items():\n",
    "\n",
    "    fold_accs = train_kfold(model_class, name, X, y)\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"model\": name,\n",
    "        \"acc_mean\": float(np.mean(fold_accs)),\n",
    "        \"acc_std\": float(np.std(fold_accs))\n",
    "    })\n",
    "\n",
    "summary_table = pd.DataFrame(summary_rows).sort_values(\"acc_mean\", ascending=False)\n",
    "summary_table.to_csv(TABLES_DIR / \"bnci_deep_models_benchmark.csv\", index=False)\n",
    "\n",
    "print(summary_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
